id,url,title,item_type,conference,year,abstract,paper_url,openreview_url,poster_url,slides_url,threat_pred,context_pred,techniques_pred,industries_pred,context,threat,context_alt,threat_alt,threat_disagreement_assessment,context_disagreement_assessment
18,https://neurips.cc/virtual/2022/poster/55654,PROSPECT: Labeled Tandem Mass Spectrometry Dataset for Machine Learning in Proteomics,Poster,NeurIPS,2022,"Proteomics is the interdisciplinary field focusing on the large-scale study of proteins. Proteins essentially organize and execute all functions within organisms. Today, the bottom-up analysis approach is the most commonly used workflow, where proteins are digested into peptides and subsequently analyzed using Tandem Mass Spectrometry (MS/MS). MS-based proteomics has transformed various fields in life sciences, such as drug discovery and biomarker identification. Today, proteomics is entering a phase where it is helpful for clinical decision-making. Computational methods are vital in turning large amounts of acquired raw MS data into information and, ultimately, knowledge. Deep learning has proved its success in multiple domains as a robust framework for supervised and unsupervised machine learning problems. In proteomics, scientists are increasingly leveraging the potential of deep learning to predict the properties of peptides based on their sequence to improve their confident identification. However, a reference dataset is missing, covering several proteomics tasks, enabling performance comparison, and evaluating reproducibility and generalization. Here, we present a large labeled proteomics dataset spanning several tasks in the domain to address this challenge. We focus on two common applications: peptide retention time and MS/MS spectrum prediction. We review existing methods and task formulations from a machine learning perspective and recommend suitable evaluation metrics and visualizations. With an accessible dataset, we aim to lower the entry barrier and enable faster development in machine learning for proteomics.",,https://openreview.net/forum?id=4nAe0PS7D-l,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55654.png?t=1669597063.410175,https://neurips.cc/virtual/2022/poster/55654,Not relevant,Defence,"['Proteomics', 'Tandem Mass Spectrometry', 'Machine Learning', 'Deep Learning', 'Proteins', 'Peptides', 'MS/MS Spectrum Prediction', 'Retention Time']","['Proteomics', 'Drug Discovery', 'Biomarker Identification', 'Clinical Decision-making']",,,,,,
22,https://neurips.cc/virtual/2022/poster/55643,"Turning the Tables: Biased, Imbalanced, Dynamic Tabular Datasets for ML Evaluation",Poster,NeurIPS,2022,"Evaluating new techniques on realistic datasets plays a crucial role in the development of ML research and its broader adoption by practitioners. In recent years, there has been a significant increase of publicly available unstructured data resources for computer vision and NLP tasks. However, tabular data — which is prevalent in many high-stakes domains — has been lagging behind. To bridge this gap, we present Bank Account Fraud (BAF), the first publicly available 1 privacy-preserving, large-scale, realistic suite of tabular datasets. The suite was generated by applying state-of-the-art tabular data generation techniques on an anonymized,real-world bank account opening fraud detection dataset. This setting carries a set of challenges that are commonplace in real-world applications, including temporal dynamics and significant class imbalance. Additionally, to allow practitioners to stress test both performance and fairness of ML methods, each dataset variant of BAF contains specific types of data bias. With this resource, we aim to provide the research community with a more realistic, complete, and robust test bed to evaluate novel and existing methods.",,https://openreview.net/forum?id=UrAYT2QwOX8,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55643.png?t=1669704348.187316,https://neurips.cc/virtual/2022/poster/55643,Not relevant,Not relevant,"['Tabular Data', 'Machine Learning', 'Computer Vision', 'NLP tasks', 'Bank Account Fraud', 'Privacy-preserving', 'Large-scale', 'Realistic', 'Real-world', 'Class Imbalance', 'Data Bias']","['Bank Account Fraud Detection', 'Real-world applications']",,,,,,
28,https://neurips.cc/virtual/2022/poster/55771,AMOS: A Large-Scale Abdominal Multi-Organ Benchmark for Versatile Medical Image Segmentation,Poster,NeurIPS,2022,"Despite the considerable progress in automatic abdominal multi-organ segmentation from CT/MRI scans in recent years, a comprehensive evaluation of the models' capabilities is hampered by the lack of a large-scale benchmark from diverse clinical scenarios. Constraint by the high cost of collecting and labeling 3D medical data, most of the deep learning models to date are driven by datasets with a limited number of organs of interest or samples, which still limits the power of modern deep models and makes it difficult to provide a fully comprehensive and fair estimate of various methods. To mitigate the limitations, we present AMOS, a large-scale, diverse, clinical dataset for abdominal organ segmentation. AMOS provides 500 CT and 100 MRI scans collected from multi-center, multi-vendor, multi-modality, multi-phase, multi-disease patients, each with voxel-level annotations of 15 abdominal organs, providing challenging examples and test-bed for studying robust segmentation algorithms under diverse targets and scenarios. We further benchmark several state-of-the-art medical segmentation models to evaluate the status of the existing methods on this new challenging dataset. We have made our datasets, benchmark servers, and baselines publicly available, and hope to inspire future research. Information can be found at https://amos22.grand-challenge.org.",,https://openreview.net/forum?id=Vk4-HUnkEak,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55771.png?t=1670294521.1317978,https://neurips.cc/virtual/2022/poster/55771,Not relevant,Not relevant,"['Abdominal Multi-Organ Benchmark', 'Medical Image Segmentation', 'CT/MRI Scans', 'Deep Learning', 'Large-scale', 'Clinical dataset', 'Multi-center', 'Multi-vendor', 'Multi-modality', 'Multi-phase', 'Multi-disease', 'Abdominal organs', 'Segmentation Algorithms']","['Medical Image Segmentation', 'Abdominal organs']",,,,,,
32,https://neurips.cc/virtual/2022/tutorial/55815,Algorithmic fairness: at the intersections,Tutorial,NeurIPS,2022,"As machine learning models permeate every aspect of decision making systems in consequential areas such as healthcare, banking, hiring and education, it has become critical for these models to satisfy trustworthiness desiderata such as fairness, privacy, robustness and interpretability. Initially studied in isolation, recent work has emerged at the intersection of these different fields of research, leading to interesting questions on how fairness can be achieved under privacy, interpretability and robustness constraints. Given the interesting questions that emerge at the intersection of these different fields, this tutorial aims to investigate how these different topics relate, and how they can augment each other to provide better or more suited definitions and mitigation strategies for algorithmic fairness. We are particularly interested in addressing open questions in the field, such as: how algorithmic fairness is compatible with privacy constraints?  What are the trade-offs when we consider algorithmic fairness at the intersection of robustness? Can we develop fair and explainable models? We will also articulate some limitations of technical approaches to algorithmic fairness, and discuss critiques that are coming from outside of computer science.",,,https://neurips.cc/virtual/2022/tutorial/55815,https://neurips.cc/virtual/2022/tutorial/55815,Robustness,Other aspects,"['Algorithmic fairness', 'Machine Learning', 'Decision making systems', 'Healthcare', 'Banking', 'Hiring', 'Education', 'Fairness', 'Privacy', 'Robustness', 'Interpretability', 'Mitigation strategies', 'Open questions', 'Trade-offs', 'Explainable models', 'Limitations', 'Critiques']","['Healthcare', 'Banking', 'Hiring', 'Education']",,,,,,
36,https://neurips.cc/virtual/2022/poster/55765,Robustness Disparities in Face Detection,Poster,NeurIPS,2022,"Facial analysis systems have been deployed by large companies and critiqued by scholars and activists for the past decade. Many existing algorithmic audits examine the performance of these systems on later stage elements of facial analysis systems like facial recognition and age, emotion, or perceived gender prediction; however, a core component to these systems has been vastly understudied from a fairness perspective: face detection, sometimes called face localization. Since face detection is a pre-requisite step in facial analysis systems, the bias we observe in face detection will flow downstream to the other components like facial recognition and emotion prediction. Additionally, no prior work has focused on the robustness of these systems under various perturbations and corruptions, which leaves open the question of how various people are impacted by these phenomena. We present the first of its kind detailed benchmark of face detection systems, specifically examining the robustness to noise of commercial and academic models. We use both standard and recently released academic facial datasets to quantitatively analyze trends in face detection robustness. Across all the datasets and systems, we generally find that photos of individuals who are masculine presenting, older, of darker skin type, or have dim lighting are more susceptible to errors than their counterparts in other identities.",,https://openreview.net/forum?id=UXPXs-OYbks,https://neurips.cc/virtual/2022/poster/55765,https://neurips.cc/virtual/2022/poster/55765,Robustness,Other aspects,"['Face detection', 'Facial analysis systems', 'Facial recognition', 'Emotion prediction', 'Robustness', 'Bias', 'Perturbations', 'Corruptions']",['Facial analysis systems'],,,,,,
37,https://neurips.cc/virtual/2022/tutorial/55809,"Probabilistic Circuits: Representations, Inference, Learning and Applications",Tutorial,NeurIPS,2022,"In several real-world scenarios, decision making involves complex reasoning, i.e., the ability to answer complex probabilistic queries. Moreover, in many sensitive domains like health- care and economical decision making, the result of these queries is required to be exact as approximations without guarantees would make the decision making process brittle. In all these scenarios, tractable probabilistic inference and learning are becoming more and more mandatory. In this tutorial, we will introduce the framework of probabilistic circuits (PCs) under which one can learn deep generative models that guarantee exact inference in polynomial (often linear) time. After certain recent algorithmic and theoretical results, which we will discuss in this tutorial, PCs have achieved impressive results in probabilistic modeling, sometimes outperforming intractable models such as variational autoencoders.  We will show the syntax and semantics of PCs and show how several commonly used ML models -- from Gaussian mixture models to HMMs and decision trees -- can be understood as computational graphs within the PC framework. We will discuss how PCs are special cases of neural networks, when restricting network with certain structural properties enables different tractability scenarios. This unified view of probabilistic ML models opens up a range of ways to learn PCs from data and use them in real-world applications. We will, in fact, provide a unifying view over several algorithms to learn both the structure and parameters of PCs and discuss modern approaches to scale them on GPU. Lastly, we will showcase several successful application scenarios where PCs have been employed as an alternative to or in conjunction with intractable models, including robust image classification, lossless compression, predictions in the presence of missing values, fairness certification, and scene understanding.",,,https://neurips.cc/virtual/2022/tutorial/55809,https://neurips.cc/virtual/2022/tutorial/55809,Not relevant,Other aspects,"['probabilistic circuits', 'probabilistic inference', 'probabilistic learning', 'deep generative models', 'tractable inference', 'computational graphs', 'Gaussian mixture models', 'HMMs', 'decision trees', 'neural networks', 'structural properties', 'tractability', 'scaling', 'GPU', 'robust image classification', 'lossless compression', 'missing values', 'fairness certification', 'scene understanding']","['healthcare', 'economical decision making', 'image classification', 'compression', 'missing values', 'fairness certification', 'scene understanding']",,,,,,
44,https://neurips.cc/virtual/2022/poster/55652,K-Radar: 4D Radar Object Detection for Autonomous Driving in Various Weather Conditions,Poster,NeurIPS,2022,"Unlike RGB cameras that use visible light bands (384∼769 THz) and Lidar that use infrared bands (361∼331 THz), Radars use relatively longer wavelength radio bands (77∼81 GHz), resulting in robust measurements in adverse weathers. Unfortunately, existing Radar datasets only contain a relatively small number of samples compared to the existing camera and Lidar datasets. This may hinder the development of sophisticated data-driven deep learning techniques for Radar-based perception. Moreover, most of the existing Radar datasets only provide 3D Radar tensor (3DRT) data that contain power measurements along the Doppler, range, and azimuth dimensions. As there is no elevation information, it is challenging to estimate the 3D bounding box of an object from 3DRT. In this work, we introduce KAIST-Radar (K-Radar), a novel large-scale object detection dataset and benchmark that contains 35K frames of 4D Radar tensor (4DRT) data with power measurements along the Doppler, range, azimuth, and elevation dimensions, together with carefully annotated 3D bounding box labels of objects on the roads. K-Radar includes challenging driving conditions such as adverse weathers (fog, rain, and snow) on various road structures (urban, suburban roads, alleyways, and highways). In addition to the 4DRT, we provide auxiliary measurements from carefully calibrated high-resolution Lidars, surround stereo cameras, and RTK-GPS. We also provide 4DRT-based object detection baseline neural networks (baseline NNs) and show that the height information is crucial for 3D object detection. And by comparing the baseline NN with a similarly-structured Lidar-based neural network, we demonstrate that 4D Radar is a more robust sensor for adverse weather conditions. All codes are available at https://github.com/kaist-avelab/k-radar.",,https://openreview.net/forum?id=W_bsDmzwaZ7,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55652.png?t=1669373425.232469,https://neurips.cc/virtual/2022/poster/55652,Robustness,Other aspects,"['Radar', 'Object detection', 'Autonomous driving', 'Weather conditions', 'Deep learning', '4D Radar Tensor', '3D Bounding box', 'Adverse weathers', 'Urban', 'Suburban roads', 'Alleyways', 'Highways', 'Lidar', 'Stereo cameras', 'RTK-GPS']",['Autonomous driving'],,,,,,
82,https://neurips.cc/virtual/2022/poster/52921,Sample-Efficient Reinforcement Learning of Partially Observable Markov Games,Poster,NeurIPS,2022,"This paper considers the challenging tasks of Multi-Agent Reinforcement Learning (MARL) under partial observability, where each agent only sees her own individual observations and actions that reveal incomplete information about the underlying state of system. This paper studies these tasks under the general model of multiplayer general-sum Partially Observable Markov Games (POMGs), which is significantly larger than the standard model of Imperfect Information Extensive-Form Games (IIEFGs). We identify a rich subclass of POMGs---weakly revealing POMGs---in which sample-efficient learning is tractable. In the self-play setting, we prove that a simple algorithm combining optimism and Maximum Likelihood Estimation (MLE) is sufficient to find approximate Nash equilibria, correlated equilibria, as well as coarse correlated equilibria of weakly revealing POMGs, in a polynomial number of samples when the number of agents is small. In the setting of playing against adversarial opponents, we show that a variant of our optimistic MLE algorithm is capable of achieving sublinear regret when being compared against the optimal maximin policies. To our best knowledge, this work provides the first line of sample-efficient results for learning POMGs.",,https://openreview.net/forum?id=HnIQrSY7vPI,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/52921.png?t=1669784560.1691034,https://neurips.cc/virtual/2022/poster/52921,Not relevant,Other aspects,"['Multi-Agent Reinforcement Learning (MARL)', 'Partially Observable Markov Games (POMGs)', 'Imperfect Information Extensive-Form Games (IIEFGs)', 'Nash equilibria', 'Correlated equilibria', 'Coarse correlated equilibria', 'Regret', 'Optimal maximin policies']",['Autonomous Driving'],,,,,,
84,https://neurips.cc/virtual/2022/poster/52933,Tight Analysis of Extra-gradient and Optimistic Gradient Methods For Nonconvex Minimax Problems,Poster,NeurIPS,2022,"Despite the established convergence theory of Optimistic Gradient Descent Ascent (OGDA) and Extragradient (EG) methods for the convex-concave minimax problems, little is known about the theoretical guarantees of these methods in nonconvex settings. To bridge this gap, for the first time, this paper establishes the convergence of OGDA and EG methods under the nonconvex-strongly-concave (NC-SC) and nonconvex-concave (NC-C) settings by providing a unified analysis through the lens of single-call extra-gradient methods. We further establish lower bounds on the convergence of GDA/OGDA/EG, shedding light on the tightness of our analysis. We also conduct experiments supporting our theoretical results. We believe our results will advance the theoretical understanding of OGDA and EG methods for solving complicated nonconvex minimax real-world problems, e.g., Generative Adversarial Networks (GANs) or robust neural networks training.",,https://openreview.net/forum?id=JLweqJeqhSq,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/52933.png?t=1669068701.9364486,https://neurips.cc/virtual/2022/poster/52933,Not relevant,Defence,"['Extra-gradient', 'Optimistic Gradient', 'Nonconvex Minimax', 'OGDA', 'EG', 'NC-SC', 'NC-C', 'Generative Adversarial Networks', 'GANs', 'Robust neural networks']",,Not relevant,Not relevant,Other aspects,Robustness,,
86,https://neurips.cc/virtual/2022/poster/52936,Robust Neural Posterior Estimation and Statistical Model Criticism,Poster,NeurIPS,2022,"Computer simulations have proven a valuable tool for understanding complex phenomena across the sciences. However, the utility of simulators for modelling and forecasting purposes is often restricted by low data quality, as well as practical limits to model fidelity. In order to circumvent these difficulties, we argue that modellers must treat simulators as idealistic representations of the true data generating process, and consequently should thoughtfully consider the risk of model misspecification. In this work we revisit neural posterior estimation (NPE), a class of algorithms that enable black-box parameter inference in simulation models, and consider the implication of a simulation-to-reality gap. While recent works have demonstrated reliable performance of these methods, the analyses have been performed using synthetic data generated by the simulator model itself, and have therefore only addressed the well-specified case. In this paper, we find that the presence of misspecification, in contrast, leads to unreliable inference when NPE is used naïvely. As a remedy we argue that principled scientific inquiry with simulators should incorporate a model criticism component, to facilitate interpretable identification of misspecification and a robust inference component, to fit ‘wrong but useful’ models. We propose robust neural posterior estimation (RNPE), an extension of NPE to simultaneously achieve both these aims, through explicitly modelling the discrepancies between simulations and the observed data. We assess the approach on a range of artificially misspecified examples, and find RNPE performs well across the tasks, whereas naïvely using NPE leads to misleading and erratic posteriors.",,https://openreview.net/forum?id=MHE27tjD8m3,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/52936.png?t=1669822072.5902953,https://neurips.cc/virtual/2022/poster/52936,Robustness,Other aspects,"['Robust neural posterior estimation', 'Statistical model criticism', 'Simulation-to-reality gap', 'Misspecification', 'Inference', 'Simulation models', 'Model criticism', 'Model misspecification']",['Sciences'],,,,,,
93,https://neurips.cc/virtual/2022/poster/53076,Outlier-Robust Sparse Estimation via Non-Convex Optimization,Poster,NeurIPS,2022,"We explore the connection between outlier-robust high-dimensional statistics and non-convex optimization in the presence of sparsity constraints, with a focus on the fundamental tasks of robust sparse mean estimation and robust sparse PCA. We develop novel and simple optimization formulations for these problems such that any approximate stationary point of the associated optimization problem yields a near-optimal solution for the underlying robust estimation task. As a corollary, we obtain that any first-order method that efficiently converges to stationarity yields an efficient algorithm for these tasks. The obtained algorithms are simple, practical, and succeed under broader distributional assumptions compared to prior work.",,https://openreview.net/forum?id=7YwwfU3DqKI,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53076.png?t=1669753051.4503462,https://neurips.cc/virtual/2022/poster/53076,Robustness,Other aspects,"['Outlier-Robust', 'Sparse Estimation', 'Non-Convex Optimization', 'Robust Sparse Mean Estimation', 'Robust Sparse PCA', 'High-dimensional Statistics', 'First-order method', 'Stationarity']",,,,,,,
94,https://neurips.cc/virtual/2022/poster/53117,Evaluating Robustness to Dataset Shift via Parametric Robustness Sets,Poster,NeurIPS,2022,"We give a method for proactively identifying small, plausible shifts in distribution which lead to large differences in model performance.  These shifts are defined via parametric changes in the causal mechanisms of observed variables, where constraints on parameters yield a ""robustness set"" of plausible distributions and a corresponding worst-case loss over the set. While the loss under an individual parametric shift can be estimated via reweighting techniques such as importance sampling, the resulting worst-case optimization problem is non-convex, and the estimate may suffer from large variance. For small shifts, however, we can construct a local second-order approximation to the loss under shift and cast the problem of finding a worst-case shift as a particular non-convex quadratic optimization problem, for which efficient algorithms are available.  We demonstrate that this second-order approximation can be estimated directly for shifts in conditional exponential family models, and we bound the approximation error. We apply our approach to a computer vision task (classifying gender from images), revealing sensitivity to shifts in non-causal attributes.",,https://openreview.net/forum?id=OTKJttKN5c,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53117.png?t=1669603701.3708622,https://neurips.cc/virtual/2022/poster/53117,Robustness,Defence,"['Robustness', 'Dataset shift', 'Parametric Robustness Sets', 'Worst-case loss', 'Non-convex optimization', 'Computer Vision']",Computer vision,,,,,,
106,https://neurips.cc/virtual/2022/poster/53396,On the Robustness of Graph Neural Diffusion to Topology Perturbations,Poster,NeurIPS,2022,"Neural diffusion on graphs is a novel class of graph neural networks that has attracted increasing attention recently. The capability of graph neural partial differential equations (PDEs) in addressing common hurdles of graph neural networks (GNNs), such as the problems of over-smoothing and bottlenecks, has been investigated but not their robustness to adversarial attacks. In this work, we explore the robustness properties of graph neural PDEs. We empirically demonstrate that graph neural PDEs are intrinsically more robust against topology perturbation as compared to other GNNs. We provide insights into this phenomenon by exploiting the stability of the heat semigroup under graph topology perturbations. We discuss various graph diffusion operators and relate them to existing graph neural PDEs. Furthermore, we propose a general graph neural PDE framework based on which a new class of robust GNNs can be defined. We verify that the new model achieves comparable state-of-the-art performance on several benchmark datasets.",,https://openreview.net/forum?id=-8tU21J6BcB,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53396.png?t=1669082646.7951474,https://neurips.cc/virtual/2022/poster/53396,Evasion,Defence,"['Robustness', 'Graph Neural Diffusion', 'Topology Perturbations', 'Adversarial attacks', 'Graph Neural Networks', 'Partial Differential Equations', 'Heat semigroup', 'Graph diffusion operators', 'Benchmark datasets']",,,,,,,
107,https://neurips.cc/virtual/2022/poster/53381,Distilled Gradient Aggregation: Purify Features for Input Attribution in the Deep Neural Network,Poster,NeurIPS,2022,"Measuring the attribution of input features toward the model output is one of the popular post-hoc explanations on the Deep Neural Networks (DNNs). Among various approaches to compute the attribution, the gradient-based methods are widely used to generate attributions, because of its ease of implementation and the model-agnostic characteristic. However, existing gradient integration methods such as Integrated Gradients (IG) suffer from (1) the noisy attributions which cause the unreliability of the explanation, and (2) the selection for the integration path which determines the quality of explanations. FullGrad (FG) is an another approach to construct the reliable attributions by focusing the locality of piece-wise linear network with the bias gradient. Although FG has shown reasonable performance for the given input, as the shortage of the global property, FG is vulnerable to the small perturbation, while IG which includes the exploration over the input space is robust. In this work, we design a new input attribution method which adopt the strengths of both local and global attributions.In particular, we propose a novel approach to distill input features using weak and extremely positive contributor masks. We aggregate the intermediate local attributions obtained from the distillation sequence to provide reliable attribution. We perform the quantitative evaluation compared to various attribution methods and show that our method outperforms others. We also provide the qualitative result that our method obtains object-aligned and sharp attribution heatmap.",,https://openreview.net/forum?id=OkLee4SfLKh,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53381.png?t=1669347770.8084722,https://neurips.cc/virtual/2022/poster/53381,Not relevant,Defence,"['Input attribution', 'Deep Neural Networks', 'Gradient-based methods', 'Integrated Gradients', 'FullGrad', 'Distilled Gradient Aggregation', 'Input features', 'Attribution methods', 'Attribution heatmap']",Not clear,,,,,,
115,https://neurips.cc/virtual/2022/poster/53532,Max-Min Off-Policy Actor-Critic Method Focusing on Worst-Case Robustness to Model Misspecification,Poster,NeurIPS,2022,"In the field of reinforcement learning, because of the high cost and risk of policy training in the real world, policies are trained in a simulation environment and transferred to the corresponding real-world environment.However, the simulation environment does not perfectly mimic the real-world environment, lead to model misspecification. Multiple studies report significant deterioration of policy performance in a real-world environment.In this study, we focus on scenarios involving a simulation environment with uncertainty parameters and the set of their possible values, called the uncertainty parameter set. The aim is to optimize the worst-case performance on the uncertainty parameter set to guarantee the performance in the corresponding real-world environment.To obtain a policy for the optimization, we propose an off-policy actor-critic approach called the Max-Min Twin Delayed Deep Deterministic Policy Gradient algorithm (M2TD3), which solves a max-min optimization problem using a simultaneous gradient ascent descent approach.Experiments in multi-joint dynamics with contact (MuJoCo) environments show that the proposed method exhibited a worst-case performance superior to several baseline approaches.",,https://openreview.net/forum?id=rcMG-hzYtR,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53532.png?t=1669463125.265198,https://neurips.cc/virtual/2022/poster/53532,Robustness,Defence,"['reinforcement learning', 'model misspecification', 'policy optimization', 'off-policy actor-critic', 'max-min optimization', 'worst-case performance']","['real-world environment', 'simulation environment', 'MuJoCo environment', 'multi-joint dynamics with contact']",,,,,,
126,https://neurips.cc/virtual/2022/poster/54585,Improving 3D-aware Image Synthesis with A Geometry-aware Discriminator,Poster,NeurIPS,2022,"3D-aware image synthesis aims at learning a generative model that can render photo-realistic 2D images while capturing decent underlying 3D shapes. A popular solution is to adopt the generative adversarial network (GAN) and replace the generator with a 3D renderer, where volume rendering with neural radiance field (NeRF) is commonly used. Despite the advancement of synthesis quality, existing methods fail to obtain moderate 3D shapes. We argue that, considering the two-player game in the formulation of GANs, only making the generator 3D-aware is not enough. In other words, displacing the generative mechanism only offers the capability, but not the guarantee, of producing 3D-aware images, because the supervision of the generator primarily comes from the discriminator. To address this issue, we propose GeoD through learning a geometry-aware discriminator to improve 3D-aware GANs. Concretely, besides differentiating real and fake samples from the 2D image space, the discriminator is additionally asked to derive the geometry information from the inputs, which is then applied as the guidance of the generator. Such a simple yet effective design facilitates learning substantially more accurate 3D shapes. Extensive experiments on various generator architectures and training datasets verify the superiority of GeoD over state-of-the-art alternatives. Moreover, our approach is registered as a general framework such that a more capable discriminator (i.e., with a third task of novel view synthesis beyond domain classification and geometry extraction) can further assist the generator with a better multi-view consistency. Project page can be found at https://vivianszf.github.io/geod.",,https://openreview.net/forum?id=QRp6viwPRaX,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54585.png?t=1670134635.4095366,https://neurips.cc/virtual/2022/poster/54585,Robustness,Defence,"['3D-aware image synthesis', 'generative adversarial network (GAN)', 'volume rendering', 'neural radiance field (NeRF)', 'geometry-aware discriminator', '3D shapes', '2D image space', 'geometry information', 'multi-view consistency']",,Not relevant,Not relevant,,,Arguable,Wrong
133,https://neurips.cc/virtual/2022/poster/53719,Tree Mover's Distance: Bridging Graph Metrics and Stability of Graph Neural Networks,Poster,NeurIPS,2022,"Understanding generalization and robustness of machine learning models fundamentally relies on assuming an appropriate metric on the data space. Identifying such a metric is particularly challenging for non-Euclidean data such as graphs. Here, we propose a pseudometric for attributed graphs, the Tree Mover's Distance (TMD), and study its relation to generalization. Via a hierarchical optimal transport problem, TMD reflects the local distribution of node attributes as well as the distribution of local computation trees, which are known to be decisive for the learning behavior of graph neural networks (GNNs). First, we show that TMD captures properties relevant for graph classification: a simple TMD-SVM can perform competitively with standard GNNs. Second, we relate TMD to generalization of GNNs under distribution shifts, and show that it correlates well with performance drop under such shifts.",,https://openreview.net/forum?id=Qh89hwiP5ZR,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53719.png?t=1669161545.7037225,https://neurips.cc/virtual/2022/poster/53719,Not relevant,Defence,"[""Tree Mover's Distance (TMD)"", 'Graph Metrics', 'Stability of Graph Neural Networks', 'Non-Euclidean data', 'Graph classification', 'Generalization', 'Distribution shifts', 'Performance drop', 'Pseudometric', 'Attributed graphs', 'Hierarchical optimal transport problem', 'Local distribution', 'Node attributes', 'Distribution of local computation trees', 'Graph neural networks (GNNs)']",,,,,,,
142,https://neurips.cc/virtual/2022/poster/53955,Generative multitask learning mitigates target-causing confounding,Poster,NeurIPS,2022,"We propose generative multitask learning (GMTL), a simple and scalable approach to causal representation learning for multitask learning. Our approach makes a minor change to the conventional multitask inference objective, and improves robustness to target shift. Since GMTL only modifies the inference objective, it can be used with existing multitask learning methods without requiring additional training. The improvement in robustness comes from mitigating unobserved confounders that cause the targets, but not the input. We refer to them as \emph{target-causing confounders}. These confounders induce spurious dependencies between the input and targets. This poses a problem for conventional multitask learning, due to its assumption that the targets are conditionally independent given the input. GMTL mitigates target-causing confounding at inference time, by removing the influence of the joint target distribution, and predicting all targets jointly. This removes the spurious dependencies between the input and targets, where the degree of removal is adjustable via a single hyperparameter. This flexibility is useful for managing the trade-off between in- and out-of-distribution generalization. Our results on the Attributes of People and Taskonomy datasets reflect an improved robustness to target shift across four multitask learning methods.",,https://openreview.net/forum?id=pyLFJ9TBZw,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53955.png?t=1669155061.6544843,https://neurips.cc/virtual/2022/poster/53955,Not relevant,Defence,"['generative multitask learning', 'causal representation learning', 'unobserved confounders', 'target shift', 'robustness']",Not clear,,,,,,
143,https://neurips.cc/virtual/2022/poster/53997,ORIENT: Submodular Mutual Information Measures for Data Subset Selection under Distribution Shift,Poster,NeurIPS,2022,"Real-world machine-learning applications require robust models that generalize well to distribution shift settings, which is typical in real-world situations. Domain adaptation techniques aim to address this issue of distribution shift by minimizing the disparities between domains to ensure that the model trained on the source domain performs well on the target domain. Nevertheless, the existing domain adaptation methods are computationally very expensive. In this work, we aim to improve the efficiency of existing supervised domain adaptation (SDA) methods by using a subset of source data that is similar to target data for faster model training. Specifically, we propose ORIENT, a subset selection framework that uses the submodular mutual information (SMI) functions to select a source data subset similar to the target data for faster training. Additionally, we demonstrate how existing robust subset selection strategies, such as GLISTER, GRADMATCH, and CRAIG, when used with a held-out query set, fit within our proposed framework and demonstrate the connections with them. Finally, we empirically demonstrate that SDA approaches like d-SNE, CCSA, and standard Cross-entropy training, when employed together with ORIENT, achieve a) faster training and b) better performance on the target data.",,https://openreview.net/forum?id=mhP6mHgrg1c,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53997.png?t=1669689712.6738722,https://neurips.cc/virtual/2022/poster/53997,Robustness,Defence,"['Submodular mutual information', 'Data subset selection', 'Distribution shift', 'Domain adaptation', 'Supervised domain adaptation', 'Efficiency', 'Robustness', 'Subset selection framework']",,,,,,,
151,https://neurips.cc/virtual/2022/poster/54226,Grounding Aleatoric Uncertainty for Unsupervised Environment Design,Poster,NeurIPS,2022,"Adaptive curricula in reinforcement learning (RL) have proven effective for producing policies robust to discrepancies between the train and test environment. Recently, the Unsupervised Environment Design (UED) framework generalized RL curricula to generating sequences of entire environments, leading to new methods with robust minimax regret properties. Problematically, in partially-observable or stochastic settings, optimal policies may depend on the ground-truth distribution over aleatoric parameters of the environment in the intended deployment setting, while curriculum learning necessarily shifts the training distribution. We formalize this phenomenon as curriculum-induced covariate shift (CICS), and describe how its occurrence in aleatoric parameters can lead to suboptimal policies. Directly sampling these parameters from the ground-truth distribution avoids the issue, but thwarts curriculum learning. We propose SAMPLR, a minimax regret UED method that optimizes the ground-truth utility function, even when the underlying training data is biased due to CICS. We prove, and validate on challenging domains, that our approach preserves optimality under the ground-truth distribution, while promoting robustness across the full range of environment settings.",,https://openreview.net/forum?id=AbLj0l8YbYt,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/634841a6831464b64c072c8510c7f35c.png?t=1667643587.8418705,https://neurips.cc/virtual/2022/poster/54226,Not relevant,Other aspects,"['Aleatoric Uncertainty', 'Unsupervised Environment Design', 'Reinforcement Learning', 'Curriculum-induced covariate shift', 'SAMPLR', 'Minimax regret']",Not clear,,,,,,
153,https://neurips.cc/virtual/2022/poster/54284,Learning Representations via a Robust Behavioral Metric for Deep Reinforcement Learning,Poster,NeurIPS,2022,"Learning an informative representation with behavioral metrics is able to accelerate the deep reinforcement learning process. There are two key research issues on behavioral metric-based representation learning: 1) how to relax the computation of a specific behavioral metric, which is difficult or even intractable to compute, and 2) how to approximate the relaxed metric by learning an embedding space for states. In this paper, we analyze the potential relaxation and/or approximation gaps for existing behavioral metric-based representation learning methods. Based on the analysis, we propose a new behavioral distance, the RAP distance, and develop a practical representation learning algorithm on top of it with a theoretical analysis. We conduct extensive experiments on DeepMind Control Suite with distraction, Robosuite, and autonomous driving simulator CARLA to demonstrate new state-of-the-art results.  ",,https://openreview.net/forum?id=7YXXt9lRls,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54284.png?t=1669794961.0878139,https://neurips.cc/virtual/2022/poster/54284,Not relevant,Other aspects,"['behavioral metrics', 'deep reinforcement learning', 'representation learning', 'RAP distance']",,,,,,,
155,https://neurips.cc/virtual/2022/poster/54330,LTMD: Learning Improvement of Spiking Neural Networks with Learnable Thresholding Neurons and Moderate Dropout,Poster,NeurIPS,2022,"Spiking Neural Networks (SNNs) have shown substantial promise in processing spatio-temporal data, mimicking biological neuronal mechanisms, and saving computational power. However, most SNNs use fixed model regardless of their locations in the network. This limits SNNs’ capability of transmitting precise information in the network, which becomes worse for deeper SNNs. Some researchers try to use specified parametric models in different network layers or regions, but most still use preset or suboptimal parameters. Inspired by the neuroscience observation that different neuronal mechanisms exist in disparate brain regions, we propose a new spiking neuronal mechanism, named learnable thresholding, to address this issue. Utilizing learnable threshold values, learnable thresholding enables flexible neuronal mechanisms across layers, proper information flow within the network, and fast network convergence. In addition, we propose a moderate dropout method to serve as an enhancement technique to minimize inconsistencies between independent dropout runs. Finally, we evaluate the robustness of the proposed learnable thresholding and moderate dropout for image classification with different initial thresholds for various types of datasets. Our proposed methods produce superior results compared to other approaches for almost all datasets with fewer timesteps. Our codes are available at https://github.com/sq117/LTMD.git.",,https://openreview.net/forum?id=BbaSRgUHW3,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54330.png?t=1669283962.3361492,https://neurips.cc/virtual/2022/poster/54330,Not relevant,Not relevant,"['Spiking Neural Networks', 'Learnable thresholding', 'Moderate Dropout', 'Neuronal mechanisms', 'Information flow', 'Convergence', 'Image classification', 'Robustness']",['None'],,,,,,
161,https://neurips.cc/virtual/2022/poster/54370,GT-GAN: General Purpose Time Series Synthesis with Generative Adversarial Networks,Poster,NeurIPS,2022,"Time series synthesis is an important research topic in the field of deep learning, which can be used for data augmentation. Time series data types can be broadly classified into regular or irregular. However, there are no existing generative models that show good performance for both types without any model changes. Therefore, we present a general purpose model capable of synthesizing regular and irregular time series data. To our knowledge, we are the first designing a general purpose time series synthesis model, which is one of the most challenging settings for time series synthesis. To this end, we design a generative adversarial network-based method, where many related techniques are carefully integrated into a single framework, ranging from neural ordinary/controlled differential equations to continuous time-flow processes. Our method outperforms all existing methods.",,https://openreview.net/forum?id=ez6VHWvuXEx,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/0ebb145bdffd37c6947bd60c251df1ba.png?t=1666610644.2620785,https://neurips.cc/virtual/2022/poster/54370,Not relevant,Not relevant,"['Time series synthesis', 'Generative Adversarial Networks', 'Data augmentation', 'Regular time series', 'Irregular time series']",['None'],Not relevant,Not relevant,,,,
164,https://neurips.cc/virtual/2022/poster/55032,BiT: Robustly Binarized Multi-distilled Transformer,Poster,NeurIPS,2022,"Modern pre-trained transformers have rapidly advanced the state-of-the-art in machine learning, but have also grown in parameters and computational complexity, making them increasingly difficult to deploy in resource-constrained environments. Binarization of the weights and activations of the network can significantly alleviate these issues, however, is technically challenging from an optimization perspective. In this work, we identify a series of improvements that enables binary transformers at a much higher accuracy than what was possible previously. These include a two-set binarization scheme, a novel elastic binary activation function with learned parameters, and a method to quantize a network to its limit by successively distilling higher precision models into lower precision students. These approaches allow for the first time, fully binarized transformer models that are at a practical level of accuracy, approaching a full-precision BERT baseline on the GLUE language understanding benchmark within as little as 5.9%. Code and models are available at:https://github.com/facebookresearch/bit.",,https://openreview.net/forum?id=wYgRIJ-oK6M,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/211c1e0b83b9c69fa9c4bdede203c1e3.png?t=1667785018.7968595,https://neurips.cc/virtual/2022/poster/55032,Not relevant,Defence,"['Binarization', 'Transformers', 'Pre-trained models', 'Resource-constrained environments', 'Optimization', 'Two-set binarization scheme', 'Elastic binary activation function', 'Distillation', 'Accuracy']",['GLUE language understanding benchmark'],,,,,,
168,https://neurips.cc/virtual/2022/poster/55118,Factored Adaptation for Non-Stationary Reinforcement Learning,Poster,NeurIPS,2022,"Dealing with non-stationarity in environments (e.g., in the transition dynamics) and objectives (e.g., in the reward functions) is a challenging problem that is crucial in real-world applications of reinforcement learning (RL). While most current approaches model the changes as a single shared embedding vector, we leverage insights from the recent causality literature to model non-stationarity in terms of individual latent change factors, and causal graphs across different environments. In particular, we propose Factored Adaptation for Non-Stationary RL (FANS-RL), a factored adaption approach that learns jointly both the causal structure in terms of a factored MDP, and a factored representation of the individual time-varying change factors. We prove that under standard assumptions, we can completely recover the causal graph representing the factored transition and reward function, as well as a partial structure between the individual change factors and the state components. Through our general framework, we can consider general non-stationary scenarios with different function types and changing frequency, including changes across episodes and within episodes. Experimental results demonstrate that FANS-RL outperforms existing approaches in terms of return, compactness of the latent state representation, and robustness to varying degrees of non-stationarity.",,https://openreview.net/forum?id=VQ9fogN1q6e,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55118.png?t=1669778720.950442,https://neurips.cc/virtual/2022/poster/55118,Not relevant,Other aspects,"['Non-stationarity', 'Reinforcement learning', 'Causality', 'Factored adaptation', 'Factored MDP', 'Latent change factors']",,,,,,,
174,https://neurips.cc/virtual/2022/poster/55227,Causally motivated multi-shortcut identification and removal,Poster,NeurIPS,2022,"For predictive models to provide reliable guidance in decision making processes, they are often required to be accurate and robust to distribution shifts. Shortcut learning--where a model relies on spurious correlations or shortcuts to predict the target label--undermines the robustness property, leading to models with poor out-of-distribution accuracy despite good in-distribution performance. Existing work on shortcut learning either assumes that the set of possible shortcuts is known a priori or is discoverable using interpretability methods such as saliency maps, which might not always be true. Instead, we propose a two step approach to (1) efficiently identify relevant shortcuts, and (2) leverage the identified shortcuts to build models that are robust to distribution shifts. Our approach relies on having access to a (possibly) high dimensional set of auxiliary labels at training time, some of which correspond to possible shortcuts. We show both theoretically and empirically that our approach is able to identify a sufficient set of shortcuts leading to more efficient predictors in finite samples.",,https://openreview.net/forum?id=-ZQOx6yaVa-,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/5caf41d62364d5b41a893adc1a9dd5d4.png?t=1667150283.947206,https://neurips.cc/virtual/2022/poster/55227,Robustness,Defence,"['shortcut learning', 'predictive models', 'distribution shifts', 'out-of-distribution accuracy', 'auxiliary labels']",,,,,,,
178,https://neurips.cc/virtual/2022/poster/56114,Learning Operators with Coupled Attention,Poster,NeurIPS,2022,"Supervised operator learning is an emerging machine learning paradigm with applications to modeling the evolution of spatio-temporal dynamical systems and approximating general black-box relationships between functional data. We propose a novel operator learning method, LOCA (Learning Operators with Coupled Attention), motivated from the recent success of the attention mechanism. In our architecture, the input functions are mapped to a finite set of features which are then averaged with attention weights that depend on the output query locations. By coupling these attention weights together with an integral transform, LOCA is able to explicitly learn correlations in the target output functions, enabling us to approximate nonlinear operators even when the number of output function measurements in the training set is very small. Our formulation is accompanied by rigorous approximation theoretic guarantees on the universal expressiveness of the proposed model. Empirically, we evaluate the performance of LOCA on several operator learning scenarios involving systems governed by ordinary and partial differential equations, as well as a black-box climate prediction problem. Through these scenarios we demonstrate state of the art accuracy, robustness with respect to noisy input data, and a consistently small spread of errors over testing data sets, even for out-of-distribution prediction tasks.",https://jmlr.org/papers/v23/21-1521.html,,https://neurips.cc/virtual/2022/poster/56114,https://neurips.cc/virtual/2022/poster/56114,Not relevant,Not relevant,"['supervised operator learning', 'spatio-temporal dynamical systems', 'black-box relationships', 'functional data', 'attention mechanism', 'integral transform', 'nonlinear operators', 'universal expressiveness', 'ordinary differential equations', 'partial differential equations', 'climate prediction problem', 'out-of-distribution prediction tasks']",,,,,,,
182,https://neurips.cc/virtual/2022/poster/53748,Rethinking the Reverse-engineering of Trojan Triggers,Poster,NeurIPS,2022,"Deep Neural Networks are vulnerable to Trojan (or backdoor) attacks. Reverse-engineering methods can reconstruct the trigger and thus identify affected models. Existing reverse-engineering methods only consider input space constraints, e.g., trigger size in the input space.Expressly, they assume the triggers are static patterns in the input space and fail to detect models with feature space triggers such as image style transformations. We observe that both input-space and feature-space Trojans are associated with feature space hyperplanes.Based on this observation, we design a novel reverse-engineering method that exploits the feature space constraint to reverse-engineer Trojan triggers. Results on four datasets and seven different attacks demonstrate that our solution effectively defends both input-space and feature-space Trojans. It outperforms state-of-the-art reverse-engineering methods and other types of defenses in both Trojaned model detection and mitigation tasks. On average, the detection accuracy of our method is 93%. For Trojan mitigation, our method can reduce the ASR (attack success rate) to only 0.26% with the BA (benign accuracy) remaining nearly unchanged. Our code can be found at https://github.com/RU-System-Software-and-Security/FeatureRE.",,https://openreview.net/forum?id=hPfJut2PeLa,https://neurips.cc/virtual/2022/poster/53748,https://neurips.cc/virtual/2022/poster/53748,Evasion,Defence,"['Trojan attack', 'Deep Neural Networks', 'Reverse-engineering', 'Trojan triggers', 'Input space constraints', 'Feature space triggers', 'Feature space hyperplanes', 'Trojaned model detection', 'Trojan mitigation']",['None'],Defence,Poisoning,,,Arguable,
187,https://neurips.cc/virtual/2022/poster/54491,You Only Live Once: Single-Life Reinforcement Learning,Poster,NeurIPS,2022,"Reinforcement learning algorithms are typically designed to learn a performant policy that can repeatedly and autonomously complete a task, usually starting from scratch. However, in many real-world situations, the goal might not be to learn a policy that can do the task repeatedly, but simply to perform a new task successfully once in a single trial. For example, imagine a disaster relief robot tasked with retrieving an item from a fallen building, where it cannot get direct supervision from humans. It must retrieve this  object within one test-time trial, and must do so while tackling unknown obstacles, though it may leverage knowledge it has of the building before the disaster. We formalize this problem setting, which we call single-life reinforcement learning (SLRL), where an agent must complete a task within a single episode without interventions, utilizing its prior experience while contending with some form of novelty. SLRL provides a natural setting to study the challenge of autonomously adapting to unfamiliar situations, and we find that algorithms designed for standard episodic reinforcement learning often struggle to recover from out-of-distribution states in this setting. Motivated by this observation, we propose an algorithm, Q-weighted adversarial learning (QWALE), which employs a distribution matching strategy that leverages the agent's prior experience as guidance in novel situations. Our experiments on several single-life continuous control problems indicate that methods based on our distribution matching formulation are 20-60% more successful because they can more quickly recover from novel states.",,https://openreview.net/forum?id=303XqIQ5c_d,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54491.png?t=1669104737.5790935,https://neurips.cc/virtual/2022/poster/54491,Not relevant,Defence,"['single-life reinforcement learning', 'SLRL', 'out-of-distribution states', 'Q-weighted adversarial learning', 'QWALE', 'distribution matching']",Disaster Relief,,,,,,
191,https://neurips.cc/virtual/2022/poster/54677,Falsification before Extrapolation in Causal Effect Estimation,Poster,NeurIPS,2022,"Randomized Controlled Trials (RCTs) represent a gold standard when developing policy guidelines. However, RCTs are often narrow, and lack data on broader populations of interest.  Causal effects in these populations are often estimated using observational datasets, which may suffer from unobserved confounding and selection bias.  Given a set of observational estimates (e.g., from multiple studies), we propose a meta-algorithm that attempts to reject observational estimates that are biased. We do so using validation effects, causal effects that can be inferred from both RCT and observational data. After rejecting estimators that do not pass this test, we generate conservative confidence intervals on the extrapolated causal effects for subgroups not observed in the RCT. Under the assumption that at least one observational estimator is asymptotically normal and consistent for both the validation and extrapolated effects, we provide guarantees on the coverage probability of the intervals output by our algorithm. To facilitate hypothesis testing in settings where causal effect transportation across datasets is necessary, we give conditions under which a doubly-robust estimator of group average treatment effects is asymptotically normal, even when flexible machine learning methods are used for estimation of nuisance parameters. We illustrate the properties of our approach on semi-synthetic experiments based on the IHDP dataset, and show that it compares favorably to standard meta-analysis techniques.",,https://openreview.net/forum?id=OmLNqwnZwmY,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54677.png?t=1669696434.0322416,https://neurips.cc/media/neurips-2022/Slides/54677.pdf,Not relevant,Other aspects,"['Randomized Controlled Trials (RCTs)', 'causal effect estimation', 'observational datasets', 'validation effects', 'meta-algorithm', 'unobserved confounding', 'selection bias', 'hypothesis testing', 'causal effect transportation', 'doubly-robust estimator', 'group average treatment effects', 'machine learning methods', 'nuisance parameters', 'meta-analysis techniques']",,,,,,,
197,https://neurips.cc/virtual/2022/poster/54853,NOTE: Robust Continual Test-time Adaptation Against Temporal Correlation,Poster,NeurIPS,2022,"Test-time adaptation (TTA) is an emerging paradigm that addresses distributional shifts between training and testing phases without additional data acquisition or labeling cost; only unlabeled test data streams are used for continual model adaptation. Previous TTA schemes assume that the test samples are independent and identically distributed (i.i.d.), even though they are often temporally correlated (non-i.i.d.) in application scenarios, e.g., autonomous driving. We discover that most existing TTA methods fail dramatically under such scenarios. Motivated by this, we present a new test-time adaptation scheme that is robust against non-i.i.d. test data streams. Our novelty is mainly two-fold: (a) Instance-Aware Batch Normalization (IABN) that corrects normalization for out-of-distribution samples, and (b) Prediction-balanced Reservoir Sampling (PBRS) that simulates i.i.d. data stream from non-i.i.d. stream in a class-balanced manner. Our evaluation with various datasets, including real-world non-i.i.d. streams, demonstrates that the proposed robust TTA not only outperforms state-of-the-art TTA algorithms in the non-i.i.d. setting, but also achieves comparable performance to those algorithms under the i.i.d. assumption. Code is available at https://github.com/TaesikGong/NOTE.",,https://openreview.net/forum?id=E9HNxrCFZPV,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54853.png?t=1669182743.146735,https://neurips.cc/virtual/2022/poster/54853,Not relevant,Defence,"['Test-time adaptation', 'Distributional shift', 'Temporal correlation', 'Instance-Aware Batch Normalization', 'Prediction-balanced Reservoir Sampling']",,,,,,,
200,https://neurips.cc/virtual/2022/poster/54900,Are GANs overkill for NLP?,Poster,NeurIPS,2022,"This work offers a novel theoretical perspective on why, despite numerous attempts, adversarial approaches to generative modeling (e.g., GANs) have not been as successful for certain generation tasks, particularly sequential tasks such as Natural Language Generation, as they have in others, such as Computer Vision. In particular, on sequential data such as text, maximum-likelihood approaches are significantly more utilized than GANs. We show that,  while it may seem that maximizing likelihood is inherently different than minimizing distinguishability, this distinction is largely an artifact of the limited representational capacity of the model family, for a wide class of adversarial objectives. We give a theoretical model in which minimizing KL-divergence (i.e., maximizing likelihood) is a more efficient approach to effectively minimizing the same distinguishability criteria that adversarial models seek to optimize. Reductions show that minimizing distinguishability can be seen as simply boosting likelihood for certain families of models including n-gram models and neural networks with a softmax output layer. To achieve a full polynomial-time reduction, a novel next-token distinguishability model is considered. Some preliminary empirical evidence is also provided to substantiate our theoretical analyses.",,https://openreview.net/forum?id=F02H1zNl213,https://neurips.cc/virtual/2022/poster/54900,https://neurips.cc/virtual/2022/poster/54900,Other attack,Defence,"['generative modeling', 'GANs', 'Natural Language Generation', 'maximum-likelihood', 'sequential data', 'text', 'representational capacity', 'KL-divergence', 'distinguishability', 'n-gram models', 'neural networks', 'softmax output layer']",,,,,,,
211,https://neurips.cc/virtual/2022/poster/54877,Equivariant Networks for Zero-Shot Coordination,Poster,NeurIPS,2022,"Successful coordination in Dec-POMDPs requires agents to adopt robust strategies and interpretable styles of play for their partner. A common failure mode is symmetry breaking, when agents arbitrarily converge on one out of many equivalent but mutually incompatible policies. Commonly these examples include partial observability, e.g. waving your right hand vs. left hand to convey a covert message. In this paper, we present a novel equivariant network architecture for use in Dec-POMDPs that prevents the agent from learning policies which break symmetries, doing so more effectively than prior methods. Our method also acts as a ""coordination-improvement operator"" for generic, pre-trained policies, and thus may be applied at test-time in conjunction with any self-play algorithm. We provide theoretical guarantees of our work and test on the AI benchmark task of Hanabi, where we demonstrate our methods outperforming other symmetry-aware baselines in zero-shot coordination, as well as able to improve the coordination ability of a variety of pre-trained policies. In particular, we show our method can be used to improve on the state of the art for zero-shot coordination on the Hanabi benchmark.",,https://openreview.net/forum?id=Jupoos_K4xt,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54877.png?t=1668216672.9282513,https://neurips.cc/virtual/2022/poster/54877,Not relevant,Other aspects,"['Dec-POMDPs', 'equivariant network architecture', 'zero-shot coordination', 'symmetry breaking', 'Hanabi benchmark']",['AI benchmark task'],,,,,,
212,https://neurips.cc/virtual/2022/poster/53313,Positive-Unlabeled Learning using Random Forests via Recursive Greedy Risk Minimization,Poster,NeurIPS,2022,"The need to learn from positive and unlabeled data, or PU learning, arises in many applications and has attracted increasing interest. While random forests are known to perform well on many tasks with positive and negative data, recent PU algorithms are generally based on deep neural networks, and the potential of tree-based PU learning is under-explored. In this paper, we propose new random forest algorithms for PU-learning. Key to our approach is a new interpretation of decision tree algorithms for positive and negative data as \emph{recursive greedy risk minimization algorithms}. We extend this perspective to the PU setting to develop new decision tree learning algorithms that directly minimizes PU-data based estimators for the expected risk. This allows us to develop an efficient PU random forest algorithm, PU extra trees. Our approach features three desirable properties: it is robust to the choice of the loss function in the sense that various loss functions lead to the same decision trees; it requires little hyperparameter tuning as compared to neural network based PU learning; it supports a feature importance that directly measures a feature's contribution to risk minimization. Our algorithms demonstrate strong performance on several datasets. Our code is available at \url{https://github.com/puetpaper/PUExtraTrees}.",,https://openreview.net/forum?id=B3hDVlw95r,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/0023a1e3447fdb31836536cc903f1310.png?t=1667443712.2563643,https://neurips.cc/media/neurips-2022/Slides/53313.pdf,Not relevant,Defence,"['Positive-Unlabeled Learning', 'Random Forests', 'Recursive Greedy Risk Minimization']",Not clear,,,,,,
232,https://neurips.cc/virtual/2022/poster/53190,Assaying Out-Of-Distribution Generalization in Transfer Learning,Poster,NeurIPS,2022,"Since out-of-distribution generalization is a generally ill-posed problem, various proxy targets (e.g., calibration, adversarial robustness, algorithmic corruptions, invariance across shifts) were studied across different research programs resulting in different recommendations. While sharing the same aspirational goal, these approaches have never been tested under the same experimental conditions on real data. In this paper, we take a unified view of previous work, highlighting message discrepancies that we address empirically, and providing recommendations on how to measure the robustness of a model and how to improve it. To this end, we collect 172 publicly available dataset pairs for training and out-of-distribution evaluation of accuracy, calibration error, adversarial attacks, environment invariance, and synthetic corruptions. We fine-tune over 31k networks, from nine different architectures in the many- and few-shot setting. Our findings confirm that in- and out-of-distribution accuracies tend to increase jointly, but show that their relation is largely dataset-dependent, and in general more nuanced and more complex than posited by previous, smaller scale studies.",,https://openreview.net/forum?id=57Ryl7lLD4h,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/c9f06258da6455f5bf50c5b9260efeff.png?t=1667835961.8828347,https://neurips.cc/virtual/2022/poster/53190,Robustness,Defence,"['Out-of-distribution generalization', 'Transfer learning', 'Calibration', 'Adversarial robustness', 'Algorithmic corruptions', 'Invariance']",,Other aspects,Robustness,,,,Wrong
233,https://neurips.cc/virtual/2022/poster/54380,What are the best Systems? New Perspectives on NLP Benchmarking,Poster,NeurIPS,2022,"In Machine Learning, a benchmark refers to an ensemble of datasets associated with one or multiple metrics together with a way to aggregate different systems performances. They are instrumental in {\it (i)}  assessing the progress of new methods along different axes and {\it (ii)} selecting the best systems for practical use. This is particularly the case for NLP with the development of large pre-trained models (\textit{e.g.} GPT, BERT) that are expected to generalize well on a variety of tasks. While the community mainly focused on developing new datasets and metrics, there has been little interest in the aggregation procedure, which is often reduced to a simple average over various performance measures. However, this procedure can be problematic when the metrics are on a different scale, which may lead to spurious conclusions. This paper proposes a new procedure to rank systems based on their performance across different tasks. Motivated by the social choice theory, the final system ordering is obtained through aggregating the rankings induced by each task and is theoretically grounded. We conduct extensive numerical experiments (on over 270k scores) to assess the soundness of our approach both on synthetic and real scores (\textit{e.g.} GLUE, EXTREM, SEVAL, TAC, FLICKR). In particular, we show that our method yields different conclusions on state-of-the-art systems than the mean-aggregation procedure while being both more reliable and robust.",,https://openreview.net/forum?id=kvtVrzQPvgb,https://neurips.cc/virtual/2022/poster/54380,https://neurips.cc/virtual/2022/poster/54380,Not relevant,Defence,"['NLP benchmarking', 'Machine Learning', 'Large pre-trained models', 'GPT', 'BERT', 'Social choice theory', 'Aggregation procedure', 'Performance measures', 'Ranking systems', 'Synthetic and real scores']",Not clear,,,,,,
242,https://neurips.cc/virtual/2022/poster/53608,Improving Certified Robustness via Statistical Learning with Logical Reasoning,Poster,NeurIPS,2022,"Intensive algorithmic efforts have been made to enable the rapid improvements of certificated robustness for complex ML models recently. However, current robustness certification methods are only able to certify under a limited perturbation radius. Given that existing pure data-driven statistical approaches have reached a bottleneck, in this paper, we propose to integrate statistical ML models with knowledge (expressed as logical rules) as a reasoning component using Markov logic networks (MLN), so as to further improve the overall certified robustness. This opens new research questions about certifying the robustness of such a paradigm, especially the reasoning component (e.g., MLN). As the first step towards understanding these questions, we first prove that the computational complexity of certifying the robustness of MLN is #P-hard. Guided by this hardness result, we then derive the first certified robustness bound for MLN by carefully analyzing different model regimes. Finally, we conduct extensive experiments on five datasets including both high-dimensional images and natural language texts, and we show that the certified robustness with knowledge-based logical reasoning indeed significantly outperforms that of the state-of-the-arts.",,https://openreview.net/forum?id=fY6OzqOiTnu,https://neurips.cc/virtual/2022/poster/53608,https://neurips.cc/virtual/2022/poster/53608,Robustness,Defence,"['Certified robustness', 'Statistical learning', 'Logical reasoning', 'Markov logic networks', 'Computational complexity']",Not clear,,,,,,
245,https://neurips.cc/virtual/2022/poster/54117,A Geometric Perspective on Variational Autoencoders,Poster,NeurIPS,2022,This paper introduces a new interpretation of the Variational Autoencoder framework by taking a fully geometric point of view. We argue that vanilla VAE models unveil naturally a Riemannian structure in their latent space and that taking into consideration those geometrical aspects can lead to better interpolations and an improved generation procedure. This new proposed sampling method consists in sampling from the uniform distribution deriving intrinsically from the learned Riemannian latent space and we show that using this scheme can make a vanilla VAE competitive and even better than more advanced versions on several benchmark datasets. Since generative models are known to be sensitive to the number of training samples we also stress the method's robustness in the low data regime.,,https://openreview.net/forum?id=PBmJC6rDnR6,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54117.png?t=1668105180.9373057,https://neurips.cc/virtual/2022/poster/54117,Not relevant,Defence,"['variational autoencoder', 'Riemannian structure', 'Latent space', 'Generative Models', 'Interpolations', 'Sampling method']",,,,,,,
249,https://neurips.cc/virtual/2022/poster/52782,Gradient Descent Is Optimal Under Lower Restricted Secant Inequality And Upper Error Bound,Poster,NeurIPS,2022,"The study of first-order optimization is sensitive to the assumptions made on the objective functions.These assumptions induce complexity classes which play a key role in worst-case analysis, includingthe fundamental concept of algorithm optimality. Recent work argues that strong convexity andsmoothness—popular assumptions in literature—lead to a pathological definition of the conditionnumber. Motivated by this result, we focus on the class of functionssatisfying a lower restricted secant inequality and an upper error bound. On top of being robust tothe aforementioned pathological behavior and including some non-convex functions, this pair ofconditions displays interesting geometrical properties. In particular, the necessary and sufficientconditions to interpolate a set of points and their gradients within the class can be separated intosimple conditions on each sampled gradient. This allows the performance estimation problem (PEP) to be solved analytically, leading to a lower boundon the convergence rate that proves gradient descent to be exactly optimal on this class of functionsamong all first-order algorithms.",,https://openreview.net/forum?id=s1yaWFDLxVG,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/52782.png?t=1669326553.4127274,https://neurips.cc/virtual/2022/poster/52782,Not relevant,Not relevant,"['gradient descent', 'first-order optimization', 'objective functions', 'complexity classes', 'condition number', 'strong convexity', 'smoothness', 'lower restricted secant inequality', 'upper error bound', 'non-convex functions', 'performance estimation problem', 'convergence rate']",,,,,,,
259,https://neurips.cc/virtual/2022/poster/53619,No Free Lunch from Deep Learning in Neuroscience: A Case Study through Models of the Entorhinal-Hippocampal Circuit,Poster,NeurIPS,2022,"Research in Neuroscience, as in many scientific disciplines, is undergoing a renaissance based on deep learning. Unique to Neuroscience, deep learning models can be used not only as a tool but interpreted as models of the brain. The central claims of recent deep learning-based models of brain circuits are that they make novel predictions about neural phenomena or shed light on the fundamental functions being optimized. We show, through the case-study of grid cells in the entorhinal-hippocampal circuit, that one may get neither. We begin by reviewing the principles of grid cell mechanism and function obtained from first-principles modeling efforts, then rigorously examine the claims of deep learning models of grid cells. Using large-scale hyperparameter sweeps and theory-driven experimentation, we demonstrate that the results of such models may be more strongly driven by particular, non-fundamental, and post-hoc implementation choices than fundamental truths about neural circuits or the loss function(s) they might optimize. We discuss why these models cannot be expected to produce accurate models of the brain without the addition of substantial amounts of inductive bias, an informal No Free Lunch result for Neuroscience. Based on first principles work, we provide hypotheses for what additional loss functions will produce grid cells more robustly. In conclusion, caution and consideration, together with biological knowledge, are warranted in building and interpreting deep learning models in Neuroscience.",,https://openreview.net/forum?id=syU-XvinTI1,https://neurips.cc/virtual/2022/poster/53619,https://neurips.cc/virtual/2022/poster/53619,Not relevant,Not relevant,"['deep learning', 'neuroscience', 'brain circuits', 'grid cells', 'entorhinal-hippocampal circuit', 'hyperparameter sweeps', 'inductive bias']",['neuroscience'],,,,,,
264,https://neurips.cc/virtual/2022/poster/55339,Stochastic Adaptive Activation Function,Poster,NeurIPS,2022,"The simulation of human neurons and neurotransmission mechanisms has been realized in deep neural networks based on the theoretical implementations of activation functions. However, recent studies have reported that the threshold potential of neurons exhibits different values according to the locations and types of individual neurons, and that the activation functions have limitations in terms of representing this variability. Therefore, this study proposes a simple yet effective activation function that facilitates different thresholds and adaptive activations according to the positions of units and the contexts of inputs. Furthermore, the proposed activation function mathematically exhibits a more generalized form of Swish activation function, and thus we denoted it as Adaptive SwisH (ASH). ASH highlights informative features that exhibit large values in the top percentiles in an input, whereas it rectifies low values. Most importantly, ASH exhibits trainable, adaptive, and context-aware properties compared to other activation functions. Furthermore, ASH represents general formula of the previously studied activation function and provides a reasonable mathematical background for the superior performance. To validate the effectiveness and robustness of ASH, we implemented ASH into many deep learning models for various tasks, including classification, detection, segmentation, and image generation. Experimental analysis demonstrates that our activation function can provide the benefits of more accurate prediction and earlier convergence in many deep learning applications.",,https://openreview.net/forum?id=wtuYr8_KhyM,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/8a0e1141fd37fa5b98d5bb769ba1a7cc.png?t=1667462280.1818702,https://neurips.cc/virtual/2022/poster/55339,Not relevant,Not relevant,"['Stochastic', 'Adaptive', 'Activation Function', 'Deep Neural Networks', 'Threshold potential', 'Neurons', 'Transmission mechanisms', 'Variability', 'Swish activation function', 'Classification', 'Detection', 'Segmentation', 'Image generation']",,,,,,,
269,https://neurips.cc/virtual/2022/poster/54292,Learning to Drop Out: An Adversarial Approach to Training Sequence VAEs,Poster,NeurIPS,2022,"In principle, applying variational autoencoders (VAEs) to sequential data offers a method for controlled sequence generation, manipulation, and structured representation learning. However, training sequence VAEs is challenging: autoregressive decoders can often explain the data without utilizing the latent space, known as posterior collapse. To mitigate this, state-of-the-art models ",,https://openreview.net/forum?id=wmdbwZz65FM,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/2afc4dfb14e55c6face649a1d0c1025b.png?t=1666470479.9798782,https://neurips.cc/virtual/2022/poster/54292,Not relevant,Defence,"['Sequential data', 'Variational autoencoders (VAEs)', 'Controlled sequence generation', 'Manipulation', 'Structured representation learning', 'Autoregressive decoders', 'Latent space', 'Posterior collapse']",Not Clear,Not relevant,Not relevant,,,,
279,https://neurips.cc/virtual/2022/poster/53057,Learning Robust Dynamics through Variational Sparse Gating,Poster,NeurIPS,2022,"Learning world models from their sensory inputs enables agents to plan for actions by imagining their future outcomes. World models have previously been shown to improve sample-efficiency in simulated environments with few objects, but have not yet been applied successfully to environments with many objects. In environments with many objects, often only a small number of them are moving or interacting at the same time. In this paper, we investigate integrating this inductive bias of sparse interactions into the latent dynamics of world models trained from pixels. First, we introduce Variational Sparse Gating (VSG), a latent dynamics model that updates its feature dimensions sparsely through stochastic binary gates. Moreover, we propose a simplified architecture Simple Variational Sparse Gating (SVSG) that removes the deterministic pathway of previous models, resulting in a fully stochastic transition function that leverages the VSG mechanism. We evaluate the two model architectures in the BringBackShapes (BBS) environment that features a large number of moving objects and partial observability, demonstrating clear improvements over prior models.",,https://openreview.net/forum?id=49TS-pwQWBa,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53057.png?t=1669128619.6405094,https://neurips.cc/virtual/2022/poster/53057,Not relevant,Other aspects,"['World models', 'Sample-efficiency', 'Simulated environments', 'Inductive bias', 'Sparse interactions', 'Latent dynamics', 'Variational sparse gating', 'Stochastic binary gates', 'Partial observability']",,,,,,,
286,https://neurips.cc/virtual/2022/poster/55121,TANGO: Text-driven Photorealistic and Robust 3D Stylization via Lighting Decomposition,Poster,NeurIPS,2022,"Creation of 3D content by stylization is a promising yet challenging problem in computer vision and graphics research. In this work, we focus on stylizing photorealistic appearance renderings of a given surface mesh of arbitrary topology. Motivated by the recent surge of cross-modal supervision of the Contrastive Language-Image Pre-training (CLIP) model, we propose TANGO, which transfers the appearance style of a given 3D shape according to a text prompt in a photorealistic manner. Technically, we propose to disentangle the appearance style as the spatially varying bidirectional reflectance distribution function, the local geometric variation, and the lighting condition, which are jointly optimized, via supervision of the CLIP loss, by a spherical Gaussians based differentiable renderer. As such, TANGO enables photorealistic 3D style transfer by automatically predicting reflectance effects even for bare, low-quality meshes, without training on a task-specific dataset. Extensive experiments show that TANGO outperforms existing methods of text-driven 3D style transfer in terms of photorealistic quality, consistency of 3D geometry, and robustness when stylizing low-quality meshes. Our codes and results are available at our project webpage https://cyw-3d.github.io/tango/.",,https://openreview.net/forum?id=zbuq101sCNV,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/39027dfad5138c9ca0c474d71db915c3.png?t=1667723881.6190128,https://neurips.cc/virtual/2022/poster/55121,Not relevant,Not relevant,"['3D content creation', 'stylization', 'photorealistic appearance', 'Contrastive Language-Image Pre-training (CLIP)', 'bidirectional reflectance distribution function', 'differentiable renderer']",[],,,,,,
297,https://neurips.cc/virtual/2022/poster/54252,SnAKe: Bayesian Optimization with Pathwise Exploration,Poster,NeurIPS,2022,"""Bayesian Optimization is a very effective tool for optimizing expensive black-box functions. Inspired by applications developing and characterizing reaction chemistry using droplet microfluidic reactors, we consider a novel setting where the expense of evaluating the function can increase significantly when making large input changes between iterations. We further assume we are working asynchronously, meaning we have to decide on new queries before we finish evaluating previous experiments. This paper investigates the problem and introduces 'Sequential Bayesian Optimization via Adaptive Connecting Samples' (SnAKe), which provides a solution by considering large batches of queries and preemptively building optimization paths that minimize input costs. We investigate some convergence properties and empirically show that the algorithm is able to achieve regret similar to classical Bayesian Optimization algorithms in both the synchronous and asynchronous settings, while reducing the input costs significantly. We show the method is robust to the choice of its single hyper-parameter and provide a parameter-free alternative.""",,https://openreview.net/forum?id=QudXypzItbt,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54252.png?t=1668181140.2504914,https://neurips.cc/virtual/2022/poster/54252,Not relevant,Not relevant,"['Bayesian Optimization', 'Pathwise Exploration', 'black-box functions', 'droplet microfluidic reactors', 'Sequential Bayesian Optimization', 'Adaptive Connecting Samples', 'SnAKe', 'regret', 'synchronous', 'asynchronous', 'input costs']",['Chemistry'],,,,,,
302,https://neurips.cc/virtual/2022/poster/54934,OST: Improving Generalization of DeepFake Detection via One-Shot Test-Time Training,Poster,NeurIPS,2022,"State-of-the-art deepfake detectors perform well in identifying forgeries when they are evaluated on a test set similar to the training set, but struggle to maintain good performance when the test forgeries exhibit different characteristics from the training images e.g., forgeries are created by unseen deepfake methods. Such a weak generalization capability hinders the applicability of deepfake detectors. In this paper, we introduce a new learning paradigm specially designed for the generalizable deepfake detection task. Our key idea is to construct a test-sample-specific auxiliary task to update the model before applying it to the sample. Specifically, we synthesize pseudo-training samples from each test image and create a test-time training objective to update the model. Moreover, we proposed to leverage meta-learning to ensure that a fast single-step test-time gradient descent, dubbed one-shot test-time training (OST), can be sufficient for good deepfake detection performance. Extensive results across several benchmark datasets demonstrate that our approach performs favorably against existing arts in terms of generalization to unseen data and robustness to different post-processing steps. ",,https://openreview.net/forum?id=YPoRoad6gzY,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/c60d870eaad6a3946ab3e8734466e532.png?t=1667485515.0708807,https://neurips.cc/virtual/2022/poster/54934,Robustness,Defence,"['deepfake detection', 'generalization', 'test-time training', 'meta-learning', 'robustness', 'post-processing']",,,,,,,
317,https://neurips.cc/virtual/2022/poster/54421,A Closer Look at the Adversarial Robustness of Deep Equilibrium Models,Poster,NeurIPS,2022,"Deep equilibrium models (DEQs) refrain from the traditional layer-stacking paradigm and turn to find the fixed point of a single layer. DEQs have achieved promising performance on different applications with featured memory efficiency. At the same time, the adversarial vulnerability of DEQs raises concerns. Several works propose to certify robustness for monotone DEQs. However, limited efforts are devoted to studying empirical robustness for general DEQs. To this end, we observe that an adversarially trained DEQ requires more forward steps to arrive at the equilibrium state, or even violates its fixed-point structure. Besides, the forward and backward tracks of DEQs are misaligned due to the black-box solvers. These facts cause gradient obfuscation when applying the ready-made attacks to evaluate or adversarially train DEQs. Given this, we develop approaches to estimate the intermediate gradients of DEQs and integrate them into the attacking pipelines. Our approaches facilitate fully white-box evaluations and lead to effective adversarial defense for DEQs. Extensive experiments on CIFAR-10 validate the adversarial robustness of DEQs competitive with deep networks of similar sizes.",,https://openreview.net/forum?id=_WHs1ruFKTD,https://neurips.cc/virtual/2022/poster/54421,https://neurips.cc/virtual/2022/poster/54421,Evasion,Defence,"['Deep equilibrium models (DEQs)', 'Adversarial robustness', 'Deep networks', 'Fixed point', 'Empirical robustness', 'Adversarially trained DEQs', 'Gradient obfuscation', 'Intermediate gradients']",,Defence,Evasion,,,,
320,https://neurips.cc/virtual/2022/poster/55037,Robust Calibration with Multi-domain Temperature Scaling,Poster,NeurIPS,2022,"Uncertainty quantification is essential for the reliable deployment of machine learning models to high-stakes application domains. Uncertainty quantification is all the more challenging when training distribution and test distribution are different, even if the distribution shifts are mild. Despite the ubiquity of distribution shifts in real-world applications, existing uncertainty quantification approaches mainly study the in-distribution setting where the train and test distributions are the same. In this paper, we develop a systematic calibration model to handle distribution shifts by leveraging data from multiple domains. Our proposed method---multi-domain temperature scaling---uses the heterogeneity in the domains to improve calibration robustness under distribution shift. Through experiments on three benchmark data sets, we find our proposed method outperforms existing methods as measured on both in-distribution and out-of-distribution test sets. ",,https://openreview.net/forum?id=UZJHudsQ7d,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55037.png?t=1669010496.29012,https://neurips.cc/virtual/2022/poster/55037,Robustness,Defence,"['uncertainty quantification', 'distribution shift', 'multi-domain temperature scaling', 'calibration', 'machine learning']",,,,,,,
321,https://neurips.cc/virtual/2022/poster/55034,Trading off Image Quality for Robustness is not Necessary with Regularized Deterministic Autoencoders,Poster,NeurIPS,2022,"The susceptibility of Variational Autoencoders (VAEs) to adversarial attacks indicates the necessity to evaluate the robustness of the learned representations along with the generation performance. The vulnerability of VAEs has been attributed to the limitations associated with their variational formulation. Deterministic autoencoders could overcome the practical limitations associated with VAEs and offer a promising alternative for image generation applications. In this work, we propose an adversarially robust deterministic autoencoder with superior performance in terms of both generation and robustness of the learned representations. We introduce a regularization scheme to incorporate adversarially perturbed data points to the training pipeline without increasing the computational complexity or compromising the generation fidelity by leveraging a loss based on the two-point Kolmogorov–Smirnov test between representations. We conduct extensive experimental studies on popular image benchmark datasets to quantify the robustness of the proposed approach based on the adversarial attacks targeted at VAEs. Our empirical findings show that the proposed method achieves significant performance in both robustness and fidelity when compared to the robust VAE models.",,https://openreview.net/forum?id=9YasTgzma8c,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/9978b7063e297d84bb2ac8e46c1c845f.png?t=1666622692.9629083,https://neurips.cc/virtual/2022/poster/55034,Robustness,Defence,"['Variational Autoencoders', 'Deterministic Autoencoders', 'Adversarially robust', 'Regularization', 'Two-point Kolmogorov-Smirnov test']",,Defence,Evasion,Other aspects,Robustness,,
334,https://neurips.cc/virtual/2022/poster/55347,Learning Multi-resolution Functional Maps with Spectral Attention for Robust Shape Matching,Poster,NeurIPS,2022,"In this work, we present a novel non-rigid shape matching framework based on multi-resolution functional maps with spectral attention. Existing functional map learning methods all rely on the critical choice of the spectral resolution hyperparameter, which can severely affect the overall accuracy or lead to overfitting, if not chosen carefully. In this paper, we show that spectral resolution tuning can be alleviated by introducing spectral attention. Our framework is applicable in both supervised and unsupervised settings, and we show that it is possible to train the network so that it can adapt the spectral resolution, depending on the given shape input. More specifically, we propose to compute multi-resolution functional maps that characterize correspondence across a range of spectral resolutions, and introduce a spectral attention network that helps to combine this representation into a single coherent final correspondence. Our approach is not only accurate with near-isometric input, for which a high spectral resolution is typically preferred, but also robust and able to produce reasonable matching even in the presence of significant non-isometric distortion, which poses great challenges to existing methods. We demonstrate the superior performance of our approach through experiments on a suite of challenging near-isometric and non-isometric shape matching benchmarks.",,https://openreview.net/forum?id=2EwEWrNADpT,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/3a066bda8c96b9478bb0512f0a43028c.png?t=1666727145.4437401,https://neurips.cc/virtual/2022/poster/55347,Robustness,Defence,"['Multi-resolution functional maps', 'Spectral attention', 'Robust shape matching', 'Functional map learning', 'Spectral resolution tuning', 'Non-rigid shape matching']",,,,,,,
335,https://neurips.cc/virtual/2022/poster/55138,Multi-Instance Causal Representation Learning for Instance Label Prediction and Out-of-Distribution Generalization,Poster,NeurIPS,2022,"Multi-instance learning (MIL) deals with objects represented as bags of instances and can predict instance labels from bag-level supervision. However, significant performance gaps exist between instance-level MIL algorithms and supervised learners since the instance labels are unavailable in MIL. Most existing MIL algorithms tackle the problem by treating multi-instance bags as harmful ambiguities and predicting instance labels by reducing the supervision inexactness. This work studies MIL from a new perspective by considering bags as auxiliary information, and utilize it to identify instance-level causal representations from bag-level weak supervision. We propose the CausalMIL algorithm, which not only excels at instance label prediction but also provides robustness to distribution change by synergistically integrating MIL with identifiable variational autoencoder. Our approach is based on a practical and general assumption: the prior distribution over the instance latent representations belongs to the non-factorized exponential family conditioning on the multi-instance bags. Experiments on synthetic and real-world datasets demonstrate that our approach significantly outperforms various baselines on instance label prediction and out-of-distribution generalization tasks.",,https://openreview.net/forum?id=2ktj0977QGO,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/270edd69788dce200a3b395a6da6fdb7.png?t=1666258573.5564873,https://neurips.cc/virtual/2022/poster/55138,Robustness,Defence,"['Multi-instance learning', 'Instance label prediction', 'Out-of-distribution generalization', 'Causal representation', 'Identifiable variational autoencoder']",,,,,,,
338,https://neurips.cc/virtual/2022/poster/53563,Unsupervised Point Cloud Completion and Segmentation by Generative Adversarial Autoencoding Network,Poster,NeurIPS,2022,"Most existing point cloud completion methods assume the input partial point cloud is clean, which is not practical in practice, and are Most existing point cloud completion methods assume the input partial point cloud is clean, which is not the case in practice, and are generally based on supervised learning. In this paper, we present an unsupervised generative adversarial autoencoding network, named UGAAN, which completes the partial point cloud contaminated by surroundings from real scenes and cutouts the object simultaneously, only using artificial CAD models as assistance. The generator of UGAAN learns to predict the complete point clouds on real data from both the discriminator and the autoencoding process of artificial data. The latent codes from generator are also fed to discriminator which makes encoder only extract object features rather than noises. We also devise a refiner for generating better complete cloud with a segmentation module to separate the object from background. We train our UGAAN with one real scene dataset and evaluate it with the other two. Extensive experiments and visualization demonstrate our superiority, generalization and robustness. Comparisons against the previous method show that our method achieves the state-of-the-art performance on unsupervised point cloud completion and segmentation on real data. ",,https://openreview.net/forum?id=2jTCojmmh82,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53563.png?t=1669024569.247076,https://neurips.cc/virtual/2022/poster/53563,Robustness,Defence,"['Point Cloud Completion', 'Segmentation', 'Generative Adversarial Autoencoding Network', 'Unsupervised learning', 'CAD models', 'Latent codes', 'Discriminator', 'Encoder']",,Not relevant,Not relevant,,,Arguable,Wrong
339,https://neurips.cc/virtual/2022/poster/54243,EvenNet: Ignoring Odd-Hop Neighbors Improves Robustness of Graph Neural Networks,Poster,NeurIPS,2022,"Graph Neural Networks (GNNs) have received extensive research attention for their promising performance in graph machine learning. Despite their extraordinary predictive accuracy, existing approaches, such as GCN and GPRGNN, are not robust in the face of homophily changes on test graphs, rendering these models vulnerable to graph structural attacks and with limited capacity in generalizing to graphs of varied homophily levels. Although many methods have been proposed to improve the robustness of GNN models, most of these techniques are restricted to the spatial domain and employ complicated defense mechanisms, such as learning new graph structures or calculating edge attentions. In this paper, we study the problem of designing simple and robust GNN models in the spectral domain. We propose EvenNet, a spectral GNN corresponding to an even-polynomial graph filter. Based on our theoretical analysis in both spatial and spectral domains, we demonstrate that EvenNet outperforms full-order models in generalizing across homophilic and heterophilic graphs, implying that ignoring odd-hop neighbors improves the robustness of GNNs.  We conduct experiments on both synthetic and real-world datasets to demonstrate the effectiveness of EvenNet. Notably, EvenNet outperforms existing defense models against structural attacks without introducing additional computational costs and maintains competitiveness in traditional node classification tasks on homophilic and heterophilic graphs.",,https://openreview.net/forum?id=SPoiDLr3WE7,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54243.png?t=1668688199.856389,https://neurips.cc/virtual/2022/poster/54243,Robustness,Defence,"['Graph Neural Networks', 'homophily changes', 'graph structural attacks', 'generalizing to graphs of varied homophily levels', 'spectral domain', 'EvenNet', 'even-polynomial graph filter', 'node classification tasks']",,,,,,,
341,https://neurips.cc/virtual/2022/poster/54645,ComGAN: Unsupervised Disentanglement and Segmentation via Image Composition,Poster,NeurIPS,2022,"We propose ComGAN, a simple unsupervised generative model, which simultaneously generates realistic images and high semantic masks under an adversarial loss and a binary regularization. In this paper, we first investigate two kinds of trivial solutions in the compositional generation process, and demonstrate their source is vanishing gradients on the mask. Then, we solve trivial solutions from the perspective of architecture. Furthermore, we redesign two fully unsupervised modules based on ComGAN (DS-ComGAN), where the disentanglement module associates the foreground, background and mask with three independent variables, and the segmentation module learns object segmentation. Experimental results show that (i) ComGAN's network architecture effectively avoids trivial solutions without any supervised information and regularization; (ii) DS-ComGAN achieves remarkable results and outperforms existing semi-supervised and weakly supervised methods by a large margin in both the image disentanglement and unsupervised segmentation tasks. It implies that the redesign of ComGAN is a possible direction for future unsupervised work.",,https://openreview.net/forum?id=0xbhGxgzd1t,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/71a8b2ffe0b594a5c1b3c28090384fd7.png?t=1666066022.7663674,https://neurips.cc/media/neurips-2022/Slides/54645.pdf,Not relevant,Defence,"['unsupervised', 'generative model', 'adversarial loss', 'binary regularization', 'disentanglement', 'segmentation', 'image composition']",,Not relevant,Not relevant,,,,
377,https://neurips.cc/virtual/2022/workshop/50012,Robustness in Sequence Modeling,Workshop,NeurIPS,2022,"As machine learning models find increasing use in the real world, ensuring their safe and reliable deployment depends on ensuring their robustness to distribution shift. This is especially true for sequential data, which occurs naturally in various data domains such as natural language processing, healthcare, computational biology, and finance. However, building models for sequence data which are robust to distribution shifts presents a unique challenge. Sequential data are often discrete rather than continuous, exhibit difficult to characterize distributions, and can display a much greater range of types of distributional shifts. Although many methods for improving model robustness exist for imaging or tabular data, extending these methods to sequential data is a challenging research direction that often requires fundamentally different techniques.",,,https://neurips.cc/virtual/2022/workshop/50012,https://neurips.cc/virtual/2022/workshop/50012,Robustness,Defence,"['Robustness', 'Sequence Modeling', 'Machine Learning', 'Distribution Shift', 'Discrete Data', 'Natural Language Processing', 'Healthcare', 'Computational Biology', 'Finance']","['Natural Language Processing', 'Healthcare', 'Computational Biology', 'Finance']",,,,,,
392,https://neurips.cc/virtual/2022/workshop/49986,Workshop on Machine Learning Safety,Workshop,NeurIPS,2022,"Designing systems to operate safely in real-world settings is a topic of growing interest in machine learning. As ML becomes more capable and widespread, long-term and long-tail safety risks will grow in importance. To make the adoption of ML more beneficial, various aspects of safety engineering and oversight need to be proactively addressed by the research community. This workshop will bring together researchers from machine learning communities to focus on research topics in Robustness, Monitoring, Alignment, and Systemic Safety. ",,,https://neurips.cc/virtual/2022/workshop/49986,https://neurips.cc/virtual/2022/workshop/49986,Not relevant,Other aspects,"['Machine Learning Safety', 'Robustness', 'Monitoring', 'Alignment', 'Systemic Safety']",Not Clear,,,,,,
412,https://neurips.cc/virtual/2022/workshop/49997,5th Robot Learning Workshop: Trustworthy Robotics,Workshop,NeurIPS,2022,"Machine learning (ML) has been one of the premier drivers of recent advances in robotics research and has made its way into impacting several real-world robotic applications in unstructured and human-centric environments, such as transportation, healthcare, and manufacturing. At the same time, robotics has been a key motivation for numerous research problems in artificial intelligence research, from efficient algorithms to robust generalization of decision models. However, there are still considerable obstacles to fully leveraging state-of-the-art ML in real-world robotics applications. For capable robots equipped with ML models, guarantees on the robustness and additional analysis of the social implications of these models are required for their utilization in real-world robotic domains that interface with humans (e.g. autonomous vehicles, and tele-operated or assistive robots).",,,https://neurips.cc/virtual/2022/workshop/49997,https://neurips.cc/virtual/2022/workshop/49997,Robustness,Defence,"['Robustness', 'Social implications', 'Real-world robotics applications', 'Trustworthy Robotics', 'Machine Learning', 'Robotics research']","['Transportation', 'Healthcare', 'Manufacturing']",,,,,,
427,https://neurips.cc/virtual/2022/poster/54659,Unifying Voxel-based Representation with Transformer for 3D Object Detection,Poster,NeurIPS,2022,"In this work, we present a unified framework for multi-modality 3D object detection, named UVTR. The proposed method aims to unify multi-modality representations in the voxel space for accurate and robust single- or cross-modality 3D detection. To this end, the modality-specific space is first designed to represent different inputs in the voxel feature space. Different from previous work, our approach preserves the voxel space without height compression to alleviate semantic ambiguity and enable spatial connections. To make full use of the inputs from different sensors, the cross-modality interaction is then proposed, including knowledge transfer and modality fusion. In this way, geometry-aware expressions in point clouds and context-rich features in images are well utilized for better performance and robustness. The transformer decoder is applied to efficiently sample features from the unified space with learnable positions, which facilitates object-level interactions. In general, UVTR presents an early attempt to represent different modalities in a unified framework. It surpasses previous work in single- or multi-modality entries. The proposed method achieves leading performance in the nuScenes test set for both object detection and the following object tracking task. Code is made publicly available at https://github.com/dvlab-research/UVTR.",,https://openreview.net/forum?id=XA4ru9mfxTP,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54659.png?t=1668436526.9194381,https://neurips.cc/virtual/2022/poster/54659,Not relevant,Not relevant,"['3D object detection', 'Voxel-based representation', 'Transformer', 'Multi-modality', 'Knowledge transfer', 'Modality fusion', 'Point clouds', 'Images']",['Automotive'],,,,,,
430,https://neurips.cc/virtual/2022/poster/53376,Improving Out-of-Distribution Generalization by Adversarial Training with Structured Priors,Poster,NeurIPS,2022,"Deep models often fail to generalize well in test domains when the data distribution differs from that in the training domain. Among numerous approaches to address this Out-of-Distribution (OOD) generalization problem, there has been a growing surge of interest in exploiting Adversarial Training (AT) to improve OOD performance. Recent works have revealed that the robust model obtained by conducting sample-wise AT also retains transferability to biased test domains. In this paper, we empirically show that sample-wise AT has limited improvement on OOD performance. Specifically, we find that AT can only maintain performance at smaller scales of perturbation while Universal AT (UAT) is more robust to larger-scale perturbations. This provides us with clues that adversarial perturbations with universal (low dimensional) structures can enhance the robustness against large data distribution shifts that are common in OOD scenarios. Inspired by this, we propose two AT variants with low-rank structures to train OOD-robust models. Extensive experiments on DomainBed benchmark show that our proposed approaches outperform Empirical Risk Minimization (ERM) and sample-wise AT. Our code is available at https://github.com/NOVAglow646/NIPS22-MAT-and-LDAT-for-OOD.",,https://openreview.net/forum?id=Ku1afTnmozi,https://neurips.cc/virtual/2022/poster/53376,https://neurips.cc/virtual/2022/poster/53376,Robustness,Defence,"['Out-of-Distribution Generalization', 'Adversarial Training', 'Structured Priors', 'Deep models', 'OOD performance', 'Transferability', 'Universal AT', 'Empirical Risk Minimization']",,,,,,,
432,https://neurips.cc/virtual/2022/poster/54752,VTC-LFC: Vision Transformer Compression with Low-Frequency Components,Poster,NeurIPS,2022,"Although Vision transformers (ViTs) have recently dominated many vision tasks, deploying ViT models on resource-limited devices remains a challenging problem. To address such a challenge, several methods have been proposed to compress ViTs. Most of them borrow experience in convolutional neural networks (CNNs) and mainly focus on the spatial domain. However, the compression only in the spatial domain suffers from a dramatic performance drop without fine-tuning and is not robust to noise, as the noise in the spatial domain can easily confuse the pruning criteria, leading to some parameters/channels being pruned incorrectly. Inspired by recent findings that self-attention is a low-pass filter and low-frequency signals/components are more informative to ViTs, this paper proposes compressing ViTs with low-frequency components. Two metrics named low-frequency sensitivity (LFS) and low-frequency energy (LFE) are proposed for better channel pruning and token pruning. Additionally, a bottom-up cascade pruning scheme is applied to compress different dimensions jointly. Extensive experiments demonstrate that the proposed method could save 40% ～ 60% of the FLOPs in ViTs, thus significantly increasing the throughput on practical devices with less than 1% performance drop on ImageNet-1K.",,https://openreview.net/forum?id=HuiLIB6EaOk,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/51e6d6e679953c6311757004d8cbbba9.png?t=1666063029.5552578,https://neurips.cc/virtual/2022/poster/54752,Robustness,Defence,"['Vision transformer', 'Compression', 'Low-frequency components', 'Pruning', 'Self-attention', 'ImageNet-1K']",,,,,,,
435,https://neurips.cc/virtual/2022/poster/54449,Moderate-fitting as a Natural Backdoor Defender for Pre-trained Language Models,Poster,NeurIPS,2022,"Despite the great success of pre-trained language models (PLMs) in a large set of natural language processing (NLP) tasks, there has been a growing concern about their security in real-world applications. Backdoor attack, which poisons a small number of training samples by inserting backdoor triggers, is a typical threat to security. Trained on the poisoned dataset, a victim model would perform normally on benign samples but predict the attacker-chosen label on samples containing pre-defined triggers. The vulnerability of PLMs under backdoor attacks has been proved with increasing evidence in the literature. In this paper, we present several simple yet effective training strategies that could effectively defend against such attacks. To the best of our knowledge, this is the first work to explore the possibility of backdoor-free adaptation for PLMs. Our motivation is based on the observation that, when trained on the poisoned dataset, the PLM's adaptation follows a strict order of two stages: (1) a moderate-fitting stage, where the model mainly learns the major features corresponding to the original task instead of subsidiary features of backdoor triggers, and (2) an overfitting stage, where both features are learned adequately. Therefore, if we could properly restrict the PLM's adaptation to the moderate-fitting stage, the model would neglect the backdoor triggers but still achieve satisfying performance on the original task. To this end, we design three methods to defend against backdoor attacks by reducing the model capacity, training epochs, and learning rate, respectively. Experimental results demonstrate the effectiveness of our methods in defending against several representative NLP backdoor attacks. We also perform visualization-based analysis to attain a deeper understanding of how the model learns different features, and explore the effect of the poisoning ratio. Finally, we explore whether our methods could defend against backdoor attacks for the pre-trained CV model. The codes are publicly available at https://github.com/thunlp/Moderate-fitting.",,https://openreview.net/forum?id=C7cv9fh8m-b,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/bc9c8c705927bf419147ab7491c54896.png?t=1666261995.278044,https://neurips.cc/virtual/2022/poster/54449,Poisoning,Defence,"['Backdoor attack', 'Pre-trained Language Models', 'Natural Language Processing', 'Moderate-fitting', 'Backdoor-free adaptation', 'Model capacity', 'Training epochs', 'Learning rate', 'Visualization-based analysis', 'Poisoning ratio', 'CV model']",N/A,,,,,,
439,https://neurips.cc/virtual/2022/poster/55016,A Unified Diversity Measure for Multiagent Reinforcement Learning,Poster,NeurIPS,2022,"Promoting behavioural diversity is of critical importance in multi-agent reinforcement learning, since it helps the agent population maintain robust performance when encountering unfamiliar opponents at test time, or,  when the game is highly non-transitive in the strategy space (e.g., Rock-Paper-Scissor). While a myriad of diversity metrics have been proposed, there are no widely accepted or unified  definitions in the literature, making the consequent diversity-aware learning algorithms difficult to evaluate and the insights elusive. In this work, we propose a novel  metric called the Unified Diversity Measure (UDM) that offers a unified view for existing diversity metrics. Based on UDM, we design the UDM-Fictitious Play (UDM-FP) and UDM-Policy Space Response Oracle (UDM-PSRO) algorithms as efficient solvers for  normal-form games and open-ended games. In theory, we prove that UDM-based methods can enlarge the gamescape by increasing the response capacity of the strategy pool, and have convergence guarantee to two-player Nash equilibrium. We validate our  algorithms on games that show strong non-transitivity, and empirical results show that our algorithms achieve better performances than strong PSRO baselines in terms of the exploitability and population effectivity. ",,https://openreview.net/forum?id=H-6iczs__Ro,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/f6c79f4af478638c39b206ec30ab166b.png?t=1665582438.4547207,https://neurips.cc/media/neurips-2022/Slides/55016_yHHgT64.pdf,Not relevant,Defence,"['multi-agent reinforcement learning', 'behavioral diversity', 'Unified Diversity Measure (UDM)', 'Fictitious Play (FP)', 'Policy Space Response Oracle (PSRO)', 'Nash equilibrium']",,,,,,,
446,https://neurips.cc/virtual/2022/poster/54834,FOF: Learning Fourier Occupancy Field for Monocular Real-time Human Reconstruction,Poster,NeurIPS,2022,"The advent of deep learning has led to significant progress in monocular human reconstruction. However, existing representations, such as parametric models, voxel grids, meshes and implicit neural representations, have difficulties achieving high-quality results and real-time speed at the same time. In this paper, we propose Fourier Occupancy Field (FOF), a novel, powerful, efficient and flexible 3D geometry representation, for monocular real-time and accurate human reconstruction. A FOF represents a 3D object with a 2D field orthogonal to the view direction where at each 2D position the occupancy field of the object along the view direction is compactly represented with the first few terms of Fourier series, which retains the topology and neighborhood relation in the 2D domain. A FOF can be stored as a multi-channel image, which is compatible with 2D convolutional neural networks and can bridge the gap between 3D geometries and 2D images. A FOF is very flexible and extensible, \eg, parametric models can be easily integrated into a FOF as a prior to generate more robust results. Meshes and our FOF can be easily inter-converted. Based on FOF, we design the first 30+FPS high-fidelity real-time monocular human reconstruction framework. We demonstrate the potential of FOF on both public datasets and real captured data. The code is available for research purposes at http://cic.tju.edu.cn/faculty/likun/projects/FOF.  ",,https://openreview.net/forum?id=qtQ9thon9fV,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54834.png?t=1668503239.7160087,https://neurips.cc/media/neurips-2022/Slides/54834.pdf,Not relevant,Other aspects,"['Fourier Occupancy Field (FOF)', 'monocular human reconstruction', '3D geometry representation', '2D field', 'Fourier series', '2D convolutional neural networks', 'parametric models', 'meshes', 'real-time', 'high-fidelity']",,,,,,,
449,https://neurips.cc/virtual/2022/poster/53283,"Personalized Federated Learning towards Communication Efficiency, Robustness and Fairness",Poster,NeurIPS,2022,"Personalized Federated Learning faces many challenges such as expensive communication costs, training-time adversarial attacks, and performance unfairness across devices. Recent developments witness a trade-off between a reference model and local models to achieve personalization. We follow the avenue and propose a personalized FL method towards the three goals. When it is time to communicate, our method projects local models into a shared-and-fixed low-dimensional random subspace and uses infimal convolution to control the deviation between the reference model and projected local models. We theoretically show our method converges for smooth objectives with square regularizers and the convergence dependence on the projection dimension is mild. We also illustrate the benefits of robustness and fairness on a class of linear problems. Finally, we conduct a large number of experiments to show the empirical superiority of our method over several state-of-the-art methods on the three aspects.",,https://openreview.net/forum?id=wFymjzZEEkH,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53283.png?t=1668434576.0142317,https://neurips.cc/media/neurips-2022/Slides/53283.pdf,Other attack,Defence,"['Federated Learning', 'Personalization', 'Communication efficiency', 'Robustness', 'Fairness', 'Adversarial attacks', 'Reference model', 'Local models', 'Projection dimension', 'Convergence']",,,,,,,
452,https://neurips.cc/virtual/2022/poster/55363,Towards Lightweight Black-Box Attack Against Deep Neural Networks,Poster,NeurIPS,2022,"Black-box attacks can generate adversarial examples without accessing the parameters of target model, largely exacerbating the threats of deployed deep neural networks (DNNs). However, previous works state that black-box attacks fail to mislead target models when their training data and outputs are inaccessible. In this work, we argue that black-box attacks can pose practical attacks in this extremely restrictive scenario where only several test samples are available.  Specifically, we find that attacking the shallow layers of DNNs trained on a few test samples can generate powerful adversarial examples. As only a few samples are required, we refer to these attacks as lightweight black-box attacks. The main challenge to promoting lightweight attacks is to mitigate the adverse impact caused by the approximation error of shallow layers. As it is hard to mitigate the approximation error with few available samples, we propose Error TransFormer (ETF) for lightweight attacks. Namely, ETF transforms the approximation error in the parameter space into a perturbation in the feature space and alleviates the error by disturbing features. In experiments, lightweight black-box attacks with the proposed ETF achieve surprising results. For example, even if only 1 sample per category available, the attack success rate in lightweight black-box attacks is only about 3% lower than that of the black-box attacks with complete training data. ",,https://openreview.net/forum?id=Gpqqm4p91Ez,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55363.png?t=1669174316.6765318,https://neurips.cc/media/neurips-2022/Slides/55363.pdf,Evasion,Attack,"['Black-box attack', 'Deep neural networks', 'Adversarial examples', 'Shallow layers', 'Error TransFormer', 'Approximation error', 'Parameter space', 'Feature space']",,Attack,Evasion,,,,
455,https://neurips.cc/virtual/2022/poster/53285,Pre-trained Adversarial Perturbations,Poster,NeurIPS,2022,"Self-supervised pre-training has drawn increasing attention in recent years due to its superior performance on numerous downstream tasks after fine-tuning. However, it is well-known that deep learning models lack the robustness to adversarial examples, which can also invoke security issues to pre-trained models, despite being less explored. In this paper, we delve into the robustness of pre-trained models by introducing Pre-trained Adversarial Perturbations (PAPs), which are universal perturbations crafted for the pre-trained models to maintain the effectiveness when attacking fine-tuned ones without any knowledge of the downstream tasks. To this end, we propose a Low-Level Layer Lifting Attack (L4A) method to generate effective PAPs by lifting the neuron activations of low-level layers of the pre-trained models. Equipped with an enhanced noise augmentation strategy, L4A is effective at generating more transferable PAPs against the fine-tuned models. Extensive experiments on typical pre-trained vision models and ten downstream tasks demonstrate that our method improves the attack success rate by a large margin compared to the state-of-the-art methods.",,https://openreview.net/forum?id=ZLcwSgV-WKH,https://neurips.cc/virtual/2022/poster/53285,https://neurips.cc/virtual/2022/poster/53285,Evasion,Attack,"['Adversarial examples', 'Pre-trained models', 'Robustness', 'Security', 'Self-supervised pre-training', 'Downstream tasks', 'Perturbations', 'Low-level layers', 'Noise augmentation', 'Attack success rate']",,Attack,Evasion,,,,
457,https://neurips.cc/virtual/2022/poster/55146,Multi-dataset Training of Transformers for Robust Action Recognition,Poster,NeurIPS,2022,"We study the task of robust feature representations, aiming to generalize well on multiple datasets for action recognition. We build our method on Transformers for its efficacy. Although we have witnessed great progress for video action recognition in the past decade, it remains challenging yet valuable how to train a single model that can perform well across multiple datasets. Here, we propose a novel multi-dataset training paradigm, MultiTrain, with the design of two new loss terms, namely informative loss and projection loss, aiming tolearn robust representations for action recognition. In particular, the informative loss maximizes the expressiveness of the feature embedding while the projection loss for each dataset mines the intrinsic relations between classes across datasets. We verify the effectiveness of our method on five challenging datasets, Kinetics-400, Kinetics-700, Moments-in-Time, Activitynet and Something-something-v2 datasets. Extensive experimental results show that our method can consistently improve state-of-the-art performance. Code and models are released.",,https://openreview.net/forum?id=aGFQDrNb-KO,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/798ed7d4ee7138d49b8828958048130a.png?t=1667813359.2773557,https://neurips.cc/virtual/2022/poster/55146,Robustness,Defence,"['Robust feature representations', 'Multi-dataset Training', 'Transformers', 'Action Recognition', 'Informative loss', 'Projection loss']",['Video action recognition'],,,,,,
464,https://neurips.cc/virtual/2022/poster/53921,Natural image synthesis for the retina with variational information bottleneck representation,Poster,NeurIPS,2022,"In the early visual system, high dimensional natural stimuli are encoded into the trains of neuronal spikes that transmit the information to the brain to produce perception. However, is all the visual scene information required to explain the neuronal responses? In this work, we search for answers to this question by developing a joint model of the natural visual input and neuronal responses using the Information Bottleneck (IB) framework that can represent features of the input data into a few latent variables that play a role in the prediction of the outputs. The correlations between data samples acquired from published experiments on ex-vivo retinas are accounted for in the model by a Gaussian Process (GP) prior. The proposed IB-GP model performs competitively to the state-of-the-art feedforward convolutional networks in predicting spike responses to natural stimuli. Finally, the IB-GP model is used in a closed-loop iterative process to obtain reduced-complexity inputs that elicit responses as elicited by the original stimuli. We found three properties of the retina's IB-GP model. First, the reconstructed stimuli from the latent variables show robustness in spike prediction across models. Second, surprisingly the dynamics of the high-dimensional stimuli and RGCs' responses are very well represented in the embeddings of the IB-GP model. Third, the minimum stimuli consist of different patterns: Gabor-type locally high-frequency filters, on- and off-center Gaussians, or a mixture of both. Overall, this work demonstrates that the IB-GP model provides a principled approach for joint learning of the stimuli and retina codes, capturing dynamics of the stimuli-RGCs in the latent space which could help better understand the computation of the early visual system.",,https://openreview.net/forum?id=FkPZGtTxXx6,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/4bb236de7787ceedafdff83bb8ea4710.png?t=1667753193.4154286,https://neurips.cc/virtual/2022/poster/53921,Not relevant,Not relevant,"['Information Bottleneck', 'Gaussian Process', 'retina', 'latent variables', 'spike responses', 'natural stimuli', 'early visual system', 'high-dimensional stimuli', 'feature representation', 'closed-loop iterative process', 'embeddings', 'latent space']",['Retina'],,,,,,
471,https://neurips.cc/virtual/2022/poster/55132,Are You Stealing My Model? Sample Correlation for Fingerprinting Deep Neural Networks,Poster,NeurIPS,2022,"An off-the-shelf model as a commercial service could be stolen by model stealing attacks, posing great threats to the rights of the model owner. Model fingerprinting aims to verify whether a suspect model is stolen from the victim model, which gains more and more attention nowadays. Previous methods always leverage the transferable adversarial examples as the model fingerprint, which is sensitive to adversarial defense or transfer learning scenarios. To address this issue, we consider the pairwise relationship between samples instead and propose a novel yet simple model stealing detection method based on SAmple Correlation (SAC). Specifically, we present SAC-w that selects wrongly classified normal samples as model inputs and calculates the mean correlation among their model outputs. To reduce the training time, we further develop SAC-m that selects CutMix Augmented samples as model inputs, without the need for training the surrogate models or generating adversarial examples. Extensive results validate that SAC successfully defends against various model stealing attacks, even including adversarial training or transfer learning, and detects the stolen models with the best performance in terms of AUC across different datasets and model architectures. The codes are available at https://github.com/guanjiyang/SAC.",,https://openreview.net/forum?id=ebuR5LWzkk0,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/ba1b3eba322eab5d895aa3023fe78b9c.png?t=1666939528.4642982,https://neurips.cc/virtual/2022/poster/55132,Model Extraction,Defence,"['Model fingerprinting', 'Model stealing', 'Sample Correlation', 'Adversarial defense', 'Transfer learning']",,Defence,Model Extraction,,,,
472,https://neurips.cc/virtual/2022/poster/54926,Enhance the Visual Representation via Discrete Adversarial Training,Poster,NeurIPS,2022,"Adversarial Training (AT), which is commonly accepted as one of the most effective approaches defending against adversarial examples, can largely harm the standard performance, thus has limited usefulness on industrial-scale production and applications. Surprisingly, this phenomenon is totally opposite in Natural Language Processing (NLP) task, where AT can even benefit for generalization. We notice the merit of AT in NLP tasks could derive from the discrete and symbolic input space. For borrowing the advantage from NLP-style AT, we propose Discrete Adversarial Training (DAT). DAT leverages VQGAN to reform the image data to discrete text-like inputs, i.e. visual words. Then it minimizes the maximal risk on such discrete images with symbolic adversarial perturbations. We further give an explanation from the perspective of distribution to demonstrate the effectiveness of DAT. As a plug-and-play technique for enhancing the visual representation, DAT achieves significant improvement on multiple tasks including image classification, object detection and self-supervised learning. Especially, the model pre-trained with Masked Auto-Encoding (MAE) and fine-tuned by our DAT without extra data can get 31.40 mCE on ImageNet-C and 32.77% top-1 accuracy on Stylized-ImageNet, building the new state-of-the-art. The code will be available at https://github.com/alibaba/easyrobust.",,https://openreview.net/forum?id=qtZac7A3-F,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/daaaf13651380465fc284db6940d8478.png?t=1665978894.4088287,https://neurips.cc/virtual/2022/poster/54926,Not relevant,Defence,"['Adversarial Training', 'Discrete Adversarial Training', 'VQGAN', 'Image Classification', 'Object Detection', 'Self-supervised Learning', 'Masked Auto-Encoding', 'ImageNet-C', 'Stylized-ImageNet']",,,,,,,
475,https://neurips.cc/virtual/2022/poster/55148,Scalable Infomin Learning,Poster,NeurIPS,2022,"The task of infomin learning aims to learn a representation with high utility while being uninformative about a specified target, with the latter achieved by minimising the mutual information between the representation and the target. It has broad applications, ranging from training fair prediction models against protected attributes, to unsupervised learning with disentangled representations. Recent works on infomin learning mainly use adversarial training, which involves training a neural network to estimate mutual information or its proxy and thus is slow and difficult to optimise. Drawing on recent advances in slicing techniques, we propose a new infomin learning approach, which uses a novel proxy metric to mutual information. We further derive an accurate and analytically computable approximation to this proxy metric, thereby removing the need of constructing neural network-based mutual information estimators. Compared to baselines, experiments on algorithmic fairness, disentangled representation learning and domain adaptation verify that our method can more effectively remove unwanted information with limited time budget.",,https://openreview.net/forum?id=Ojakr9ofova,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55148.png?t=1667859264.9339767,https://neurips.cc/virtual/2022/poster/55148,Not relevant,Defence,"['infomin learning', 'adversarial training', 'slicing techniques', 'mutual information', 'algorithmic fairness', 'disentangled representation learning', 'domain adaptation']",,,,,,,
476,https://neurips.cc/virtual/2022/poster/54576,When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture,Poster,NeurIPS,2022,"Vision Transformers (ViTs) have recently achieved competitive performance in broad vision tasks. Unfortunately, on popular threat models, naturally trained ViTs are shown to provide no more adversarial robustness than convolutional neural networks (CNNs). Adversarial training is still required for ViTs to defend against such adversarial attacks. In this paper, we provide the first and comprehensive study on the adversarial training recipe of ViTs via extensive evaluation of various training techniques across benchmark datasets. We find that pre-training and SGD optimizer are necessary for ViTs' adversarial training. Further considering ViT as a new type of model architecture, we investigate its adversarial robustness from the perspective of its unique architectural components. We find, when randomly masking gradients from some attention blocks or masking perturbations on some patches during adversarial training, the adversarial robustness of ViTs can be remarkably improved, which may potentially open up a line of work to explore the architectural information inside the newly designed models like ViTs. Our code is available at https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers.",,https://openreview.net/forum?id=ZV9WAe-Q0J,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54576.png?t=1668011201.1238594,https://neurips.cc/virtual/2022/poster/54576,Evasion,Defence,"['Vision Transformers', 'Adversarial training', 'Pre-training', 'SGD optimizer', 'Architectural components', 'Attention blocks', 'Patches']",,Defence,Evasion,,,,
478,https://neurips.cc/virtual/2022/poster/54307,MExMI: Pool-based Active Model Extraction Crossover Membership Inference,Poster,NeurIPS,2022,"With increasing popularity of Machine Learning as a Service (MLaaS), ML models trained from public and proprietary data are deployed in the cloud and deliver prediction services to users. However, as the prediction API becomes a new attack surface, growing concerns have arisen on the confidentiality of ML models. Existing literatures show their vulnerability under model extraction (ME) attacks, while their private training data is vulnerable to another type of attacks, namely, membership inference (MI). In this paper, we show that ME and MI can reinforce each other through a chained and iterative reaction, which can significantly boost ME attack accuracy and improve MI by saving the query cost. As such, we build a framework MExMI for pool-based active model extraction (PAME) to exploit MI through three modules: “MI Pre-Filter”, “MI Post-Filter”, and “semi-supervised boosting”. Experimental results show that MExMI can improve up to 11.14% from the best known PAME attack and reach 94.07% fidelity with only 16k queries. Furthermore, the precision and recall of the MI attack in MExMI are on par with state-of-the-art MI attack which needs 150k queries.",,https://openreview.net/forum?id=o8H6h13Avjy,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/543e83748234f7cbab21aa0ade66565f.png?t=1666620781.2348783,https://neurips.cc/virtual/2022/poster/54307,Model Extraction,Attack,"['model extraction', 'membership inference', 'Machine Learning as a Service (MLaaS)', 'attack surface', 'confidentiality', 'PAME', 'MI Pre-Filter', 'MI Post-Filter', 'semi-supervised boosting']",Not clear,Attack,Model Extraction,,Data Extraction,,
482,https://neurips.cc/virtual/2022/poster/54817,Exploring Figure-Ground Assignment Mechanism in Perceptual Organization,Poster,NeurIPS,2022,"Perceptual organization is a challenging visual task that aims to perceive and group the individual visual element so that it is easy to understand the meaning of the scene as a whole. Most recent methods building upon advanced Convolutional Neural Network (CNN) come from learning discriminative representation and modeling context hierarchically. However, when the visual appearance difference between foreground and background is obscure, the performance of existing methods degrades significantly due to the visual ambiguity in the discrimination process. In this paper, we argue that the figure-ground assignment mechanism, which conforms to human vision cognitive theory, can be explored to empower CNN to achieve a robust perceptual organization despite visual ambiguity. Specifically, we present a novel Figure-Ground-Aided (FGA) module to learn the configural statistics of the visual scene and leverage it for the reduction of visual ambiguity. Particularly, we demonstrate the benefit of using stronger supervisory signals by teaching (FGA) module to perceive configural cues, \ie, convexity and lower region, that human deem important for the perceptual organization. Furthermore, an Interactive Enhancement Module (IEM) is devised to leverage such configural priors to assist representation learning, thereby achieving robust perception organization with complex visual ambiguities. In addition, a well-founded visual segregation test is designed to validate the capability of the proposed FGA mechanism explicitly. Comprehensive evaluation results demonstrate our proposed FGA mechanism can effectively enhance the capability of perception organization on various baseline models. Nevertheless, the model augmented via our proposed FGA mechanism also outperforms state-of-the-art approaches on four challenging real-world applications.",,https://openreview.net/forum?id=c3HrNgQE7d,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/98b17f068d5d9b7668e19fb8ae470841.png?t=1666001293.1610782,https://neurips.cc/media/neurips-2022/Slides/54817.pdf,Not relevant,Defence,"['Perceptual organization', 'Convolutional Neural Network (CNN)', 'Figure-ground assignment', 'Visual ambiguity', 'Supervisory signals', 'Configural cues', 'Convexity', 'Lower region', 'Interactive Enhancement Module (IEM)', 'Real-world applications']",,,,,,,
494,https://neurips.cc/virtual/2022/poster/54095,Defending Against Adversarial Attacks via Neural Dynamic System,Poster,NeurIPS,2022,"Although deep neural networks (DNN) have achieved great success, their applications in safety-critical areas are hindered due to their vulnerability to adversarial attacks. Some recent works have accordingly proposed to enhance the robustness of DNN from a dynamic system perspective. Following this line of inquiry, and inspired by the asymptotic stability of the general nonautonomous dynamical system, we propose to make each clean instance be the asymptotically stable equilibrium points of a slowly time-varying system in order to defend against adversarial attacks. We present a theoretical guarantee that if a clean instance is an asymptotically stable equilibrium point and the adversarial instance is in the neighborhood of this point, the asymptotic stability will reduce the adversarial noise to bring the adversarial instance close to the clean instance. Motivated by our theoretical results, we go on to propose a nonautonomous neural ordinary differential equation (ASODE) and place constraints on its corresponding linear time-variant system to make all clean instances act as its asymptotically stable equilibrium points. Our analysis suggests that the constraints can be converted to regularizers in implementation. The experimental results show that ASODE improves robustness against adversarial attacks and outperforms state-of-the-art methods.",,https://openreview.net/forum?id=9BL0-oS7W7_,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/ebbdfea212e3a756a1fded7b35578525.png?t=1666404939.0395753,https://neurips.cc/virtual/2022/poster/54095,Evasion,Defence,"['Adversarial attacks', 'Deep neural networks', 'Robustness', 'Dynamic system', 'Asymptotic stability', 'Nonautonomous dynamical system']",,Defence,Evasion,,,,
500,https://neurips.cc/virtual/2022/poster/53192,Trustworthy Monte Carlo,Poster,NeurIPS,2022,"Monte Carlo integration is a key technique for designing randomized approximation schemes for counting problems, with applications, e.g., in machine learning and statistical physics. The technique typically enables massively parallel computation, however, with the risk that some of the delegated computations contain spontaneous or adversarial errors. We present an orchestration of the computations such that the outcome is accompanied with a proof of correctness that can be verified with substantially less computational resources than it takes to run the computations from scratch with state-of-the-art algorithms. Specifically, we adopt an algebraic proof system developed in computational complexity theory, in which the proof is represented by a polynomial; evaluating the polynomial at a random point amounts to a verification of the proof with probabilistic guarantees. We give examples of known Monte Carlo estimators that admit verifiable extensions with moderate computational overhead: for the permanent of zero--one matrices, for the model count of disjunctive normal form formulas, and for the gradient of logistic regression models. We also discuss the prospects and challenges of engineering efficient verifiable approximation schemes more generally.",,https://openreview.net/forum?id=jglXPY6gH-1,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/c0f6fb5d3a389de216345e490469145e.png?t=1667570806.2218943,https://neurips.cc/virtual/2022/poster/53192,Not relevant,Defence,"['Monte Carlo integration', 'counting problems', 'machine learning', 'statistical physics', 'randomized approximation schemes', 'algebraic proof system', 'computational complexity theory', 'permanent of zero-one matrices', 'model count of disjunctive normal form formulas', 'gradient of logistic regression models', 'verifiable approximation schemes']",,,,,,,
526,https://neurips.cc/virtual/2022/poster/55046,GAR: Generalized Autoregression for Multi-Fidelity Fusion,Poster,NeurIPS,2022,"In many scientiﬁc research and engineering applications, where repeated simulations of complex systems are conducted, a surrogate is commonly adopted to quickly estimate the whole system. To reduce the expensive cost of generating training examples, it has become a promising approach to combine the results of low-ﬁdelity (fast but inaccurate) and high-ﬁdelity (slow but accurate) simulations. Despite the fast developments of multi-ﬁdelity fusion techniques, most existing methods require particular data structures and do not scale well to high-dimensional output. To resolve these issues, we generalize the classic autoregression (AR), which is wildly used due to its simplicity, robustness, accuracy, and tractability, and propose generalized autoregression (GAR) using tensor formulation and latent features. GAR can deal with arbitrary dimensional outputs and arbitrary multiﬁdelity data structure to satisfy the demand of multi-ﬁdelity fusion for complex problems; it admits a fully tractable likelihood and posterior requiring no approximate inference and scales well to high-dimensional problems. Furthermore, we prove the autokrigeability theorem based on GAR in the multi-ﬁdelity case and develop CIGAR, a simpliﬁed GAR with the same predictive mean accuracy but requires signiﬁcantly less computation. In experiments of canonical PDEs and scientiﬁc computational examples, the proposed method consistently outperforms the SOTA methods with a large margin (up to 6x improvement in RMSE) with only a few high-ﬁdelity training samples.",,https://openreview.net/forum?id=aLNWp0pn1Ij,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55046.png?t=1669475279.7306182,https://neurips.cc/media/neurips-2022/Slides/55046_uubcAyK.pdf,Not relevant,Other aspects,"['multi-fidelity fusion', 'autoregression', 'tensor formulation', 'latent features', 'autokrigeability', 'predictive mean accuracy', 'complex systems', 'simulations', 'surrogate']","['Scientific research', 'Engineering']",,,,,,
527,https://neurips.cc/virtual/2022/poster/54977,Generalized One-shot Domain Adaptation of Generative Adversarial Networks,Poster,NeurIPS,2022,"The adaptation of a Generative Adversarial Network (GAN) aims to transfer a pre-trained GAN to a target domain with limited training data. In this paper, we focus on the one-shot case, which is more challenging and rarely explored in previous works. We consider that the adaptation from a source domain to a target domain can be decoupled into two parts: the transfer of global style like texture and color, and the emergence of new entities that do not belong to the source domain. While previous works mainly focus on style transfer, we propose a novel and concise framework to address the \textit{generalized one-shot adaptation} task for both style and entity transfer, in which a reference image and its binary entity mask are provided. Our core idea is to constrain the gap between the internal distributions of the reference and syntheses by sliced Wasserstein distance. To better achieve it, style fixation is used at first to roughly obtain the exemplary style, and an auxiliary network is introduced to the generator to disentangle entity and style transfer. Besides, to realize cross-domain correspondence, we propose the variational Laplacian regularization to constrain the smoothness of the adapted generator. Both quantitative and qualitative experiments demonstrate the effectiveness of our method in various scenarios. Code is available at \url{https://github.com/zhangzc21/Generalized-One-shot-GAN-adaptation}.",,https://openreview.net/forum?id=mfxq7BrMfga,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/5314b9674c86e3f9d1ba25ef9bb32895.png?t=1665488329.5503101,https://neurips.cc/virtual/2022/poster/54977,Evasion,Defence,"['Generative Adversarial Network', 'Transfer learning', 'One-shot adaptation', 'Style transfer', 'Entity transfer', 'Sliced Wasserstein distance', 'Variational Laplacian regularization']",,Not relevant,Not relevant,,,Wrong,Wrong
530,https://neurips.cc/virtual/2022/poster/53793,OPEN: Orthogonal Propagation with Ego-Network Modeling,Poster,NeurIPS,2022,"To alleviate the unfavorable effect of noisy topology in Graph Neural networks (GNNs), some efforts perform the local topology refinement through the pairwise propagation weight learning and the multi-channel extension. Unfortunately, most of them suffer a common and fatal drawback: irrelevant propagation to one node and in multi-channels. These two kinds of irrelevances make propagation weights in multi-channels free to be determined by the labeled data, and thus the GNNs are exposed to overfitting. To tackle this issue, a novel Orthogonal Propagation with Ego-Network modeling (OPEN) is proposed by modeling relevances between propagations. Specifically, the relevance between propagations to one node is modeled by whole ego-network modeling, while the relevance between propagations in multi-channels is modeled via diversity requirement. By interpreting the propagations to one node from the perspective of dimension reduction, propagation weights are inferred from principal components of the ego-network, which are orthogonal to each other. Theoretical analysis and experimental evaluations reveal four attractive characteristics of OPEN as modeling high-order relationships beyond pairwise one, preventing overfitting, robustness, and high efficiency. ",,https://openreview.net/forum?id=G25uStbmC7,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/6b3c49bdba5be0d322334e30c459f8bd.png?t=1667050561.225726,https://neurips.cc/virtual/2022/poster/53793,Not relevant,Other aspects,"['Graph Neural networks (GNNs)', 'Noisy topology', 'local topology refinement', 'pairwise propagation weight learning', 'multi-channel extension', 'overfitting', 'Orthogonal Propagation', 'Ego-Network modeling', 'dimension reduction', 'principal components']",,,,,,,
533,https://neurips.cc/virtual/2022/poster/53554,Grounded Reinforcement Learning: Learning to Win the Game under Human Commands,Poster,NeurIPS,2022,"We consider the problem of building a reinforcement learning (RL) agent that can both accomplish non-trivial tasks, like winning a real-time strategy game, and strictly follow high-level language commands from humans, like “attack”, even if a command is sub-optimal. We call this novel yet important problem, Grounded Reinforcement Learning (GRL). Compared with other language grounding tasks, GRL is particularly non-trivial and cannot be simply solved by pure RL or behavior cloning (BC). From the RL perspective, it is extremely challenging to derive a precise reward function for human preferences since the commands are abstract and the valid behaviors are highly complicated and multi-modal. From the BC perspective, it is impossible to obtain perfect demonstrations since human strategies in complex games are typically sub-optimal. We tackle GRL via a simple, tractable, and practical constrained RL objective and develop an iterative RL algorithm, REinforced demonstration Distillation (RED), to obtain a strong GRL policy. We evaluate the policies derived by RED, BC and pure RL methods on a simplified real-time strategy game, MiniRTS. Experiment results and human studies show that the RED policy is able to consistently follow human commands and achieve a higher win rate than the baselines. We release our code and present more examples at https://sites.google.com/view/grounded-rl.",,https://openreview.net/forum?id=YYyAVk8TrOQ,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53554.png?t=1669130794.798932,https://neurips.cc/virtual/2022/poster/53554,Not relevant,Other aspects,"['Grounded Reinforcement Learning', 'Human commands', 'Real-time strategy game', 'Reinforcement learning', 'Behavior cloning', 'Grounding tasks', 'Constrained RL objective', 'REinforced demonstration Distillation', 'MiniRTS']",,Not relevant,Not relevant,,,,
546,https://neurips.cc/virtual/2022/poster/54428,Pre-Trained Image Encoder for Generalizable Visual Reinforcement Learning,Poster,NeurIPS,2022,"Learning generalizable policies that can adapt to unseen environments remains challenging in visual Reinforcement Learning (RL). Existing approaches try to acquire a robust representation via diversifying the appearances of in-domain observations for better generalization. Limited by the specific observations of the environment, these methods ignore the possibility of exploring diverse real-world image datasets. In this paper, we investigate how a visual RL agent would benefit from the off-the-shelf visual representations. Surprisingly, we find that the early layers in an ImageNet pre-trained ResNet model could provide rather generalizable representations for visual RL. Hence, we propose Pre-trained Image Encoder for Generalizable visual reinforcement learning (PIE-G), a simple yet effective framework that can generalize to the unseen visual scenarios in a zero-shot manner. Extensive experiments are conducted on DMControl Generalization Benchmark, DMControl Manipulation Tasks, Drawer World, and CARLA to verify the effectiveness of PIE-G. Empirical evidence suggests PIE-G improves sample efficiency and significantly outperforms previous state-of-the-art methods in terms of generalization performance. In particular, PIE-G boasts a 55% generalization performance gain on average in the challenging video background setting. Project Page: https://sites.google.com/view/pie-g/home.",,https://openreview.net/forum?id=FQtku8rkp3,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54428.png?t=1669045023.8608913,https://neurips.cc/virtual/2022/poster/54428,Not relevant,Defence,"['Reinforcement Learning', 'Visual Reinforcement Learning', 'Image Encoder', 'Generalizable visual reinforcement learning', 'Pre-trained Image Encoder', 'Off-the-shelf visual representation', 'ResNet model', 'DMControl Generalization Benchmark', 'DMControl Manipulation Tasks', 'Drawer World', 'CARLA']",['Video background'],,,,,,
548,https://neurips.cc/virtual/2022/poster/53831,Byzantine-tolerant federated Gaussian process regression for streaming data,Poster,NeurIPS,2022,"In this paper, we consider Byzantine-tolerant federated learning for streaming data using Gaussian process regression (GPR). In particular, a cloud and a group of agents aim to collaboratively learn a latent function where some agents are subject to Byzantine attacks. We develop a Byzantine-tolerant federated GPR algorithm, which includes three modules: agent-based local GPR, cloud-based aggregated GPR and agent-based fused GPR. We derive the upper bounds on prediction error between the mean from the cloud-based aggregated GPR and the target function provided that Byzantine agents are less than one quarter of all the agents. We also characterize the lower and upper bounds of the predictive variance. Experiments on a synthetic dataset and two real-world datasets are conducted to evaluate the proposed algorithm.",,https://openreview.net/forum?id=Nx4gNemvNvx,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53831.png?t=1668046115.0023313,https://neurips.cc/media/neurips-2022/Slides/53831.pdf,Not relevant,Defence,"['Byzantine-tolerant', 'federated learning', 'streaming data', 'Gaussian process regression', 'latent function', 'Byzantine attacks', 'prediction error', 'predictive variance']",['None'],,,,,,
570,https://neurips.cc/virtual/2022/poster/53785,Label-invariant Augmentation for Semi-Supervised Graph Classification,Poster,NeurIPS,2022,"Recently, contrastiveness-based augmentation surges a new climax in the computer vision domain, where some operations, including rotation, crop, and flip, combined with dedicated algorithms, dramatically increase the model generalization and robustness. Following this trend, some pioneering attempts employ the similar idea to graph data. Nevertheless, unlike images, it is much more difficult to design reasonable augmentations without changing the nature of graphs. Although exciting, the current graph contrastive learning does not achieve as promising performance as visual contrastive learning. We conjecture the current performance of graph contrastive learning might be limited by the violation of the label-invariant augmentation assumption. In light of this, we propose a label-invariant augmentation for graph-structured data to address this challenge. Different from the node/edge modification and subgraph extraction, we conduct the augmentation in the representation space and generate the augmented samples in the most difficult direction while keeping the label of augmented data the same as the original samples. In the semi-supervised scenario, we demonstrate our proposed method outperforms the classical graph neural network based methods and recent graph contrastive learning on eight benchmark graph-structured data, followed by several in-depth experiments to further explore the label-invariant augmentation in several aspects.",,https://openreview.net/forum?id=rg_yN3HpCp,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53785.png?t=1668445739.8311691,https://neurips.cc/virtual/2022/poster/53785,Not relevant,Defence,"['label-invariant augmentation', 'graph classification', 'contrastiveness-based augmentation', 'graph contrastive learning', 'graph neural network']",,,,,,,
573,https://neurips.cc/virtual/2022/poster/53386,BadPrompt: Backdoor Attacks on Continuous Prompts,Poster,NeurIPS,2022,"The prompt-based learning paradigm has gained much research attention recently. It has achieved state-of-the-art performance on several NLP tasks, especially in the few-shot scenarios. While steering the downstream tasks, few works have been reported to investigate the security problems of the prompt-based models. In this paper, we conduct the first study on the vulnerability of the continuous prompt learning algorithm to backdoor attacks. We observe that the few-shot scenarios have posed a great challenge to backdoor attacks on the prompt-based models, limiting the usability of existing NLP backdoor methods. To address this challenge, we propose BadPrompt, a lightweight and task-adaptive algorithm, to backdoor attack continuous prompts. Specially, BadPrompt first generates candidate triggers which are indicative for predicting the targeted label and dissimilar to the samples of the non-targeted labels. Then, it automatically selects the most effective and invisible trigger for each sample with an adaptive trigger optimization algorithm. We evaluate the performance of BadPrompt on five datasets and two continuous prompt models. The results exhibit the abilities of BadPrompt to effectively attack continuous prompts while maintaining high performance on the clean test sets, outperforming the baseline models by a large margin. The source code of BadPrompt is publicly available.",,https://openreview.net/forum?id=rlN6fO3OrP,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53386.png?t=1668227918.5759254,https://neurips.cc/virtual/2022/poster/53386,Other attack,Attack,"['backdoor attacks', 'prompt-based learning', 'NLP', 'continuous prompts', 'few-shot scenarios', 'security problems', 'vulnerability', 'BadPrompt', 'lightweight', 'task-adaptive algorithm', 'candidate triggers', 'targeted label', 'non-targeted labels', 'adaptive trigger optimization algorithm']",,,,,,,
582,https://neurips.cc/virtual/2022/poster/56085,[Re] An Implementation of Fair Robust Learning,Poster,NeurIPS,2022,"Reproducibility Summary
Scope of Reproducibility This work attempts to reproduce the results of the 2021 ICML paper 'To be Robust or to be Fair: Towards Fairness in Adversarial Training.' I first reproduce classwise accuracy and robustness discrepancies resulting from adversarial training, and then implement the authors' proposed Fair Robust Learning (FRL) algorithms for correcting this bias.
Methodology In the spirit of education and public accessibility, this work attempts to replicate the results of the paper from first principles using Google Colab resources. To account for the limitations imposed by Colab, a much smaller model and dataset are used. All results can be replicated in approximately 10 GPU hours, within the usual timeout window of an active Colab session. Serialization is also built into the example notebooks in the case of crashes to prevent too much loss, and serialized models are also included in the repository to allow others to explore the results without having to run hours of code.
Results This work finds that (1) adversarial training does in fact lead to classwise performance discrepancies not only in standard error (accuracy) but also in attack robustness, (2) these discrepancies exacerbate existing biases in the model, (3) upweighting the standard and robust errors of poorly performing classes during training decreased this discrepancy for both both the standard error and robustness and (4) increasing the attack margin for poorly performing classes during training also decreased these discrepancies, at the cost of some performance. (1) (2) and (3) match the conclusions of the original paper, while (4) deviated in that it was unsuccessful in helping increasing the robustness the most poorly performing classes. Because the model and datasets used were totally different from the original paper's, it is hard to to quantify the exact similarity of our results. Conceptually however, I find very similar conclusions.
What was easy It was easy to identify the unfairness resulting from existing adversarial training methods and implement the authors' FRL (reweight) and FRL (remargin) approaches for combating this bias. The algorithm and training approaches are well outlined in the original paper, and are relatively accessible even for those with little experience in adversarial training.
What was difficult Because of the resource limitations imposed, I was unable to successfully implement the suggested training process using the authors' specific model and dataset. Also, even with a smaller model and dataset it was difficult to thoroughly tune the hyperparameters of the model and algorithm.
Communication with original authors I did not have contact with the authors during the process of this reproduction. I reached out for feedback once I had a draft of the report, but did not hear back. ",https://rescience.github.io/bibliography/Hardy_2022.html,,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/b76a04b05dcebb1472dee6fc6e50ee6d.png?t=1666413113.2312598,https://neurips.cc/virtual/2022/poster/56085,Not relevant,Defence,"['Fair robust learning', 'Adversarial training', 'Classwise accuracy', 'Robustness discrepancies', 'Attack robustness', 'Bias']",Not clear,,,,,,
584,https://neurips.cc/virtual/2022/poster/54081,Spectrum Random Masking for Generalization in Image-based Reinforcement Learning,Poster,NeurIPS,2022,"Generalization in image-based reinforcement learning (RL) aims to learn a robust policy that could be applied directly on unseen visual environments, which is a challenging task since agents usually tend to overfit to their training environment. To handle this problem, a natural approach is to increase the data diversity by image based augmentations. However, different with most vision tasks such as classification and detection, RL tasks are not always invariant to spatial based augmentations due to the entanglement of environment dynamics and visual appearance.  In this paper, we argue with two principles for augmentations in RL: First, the augmented observations should facilitate learning a universal policy, which is robust to various distribution shifts. Second, the augmented data should be invariant to the learning signals such as action and reward. Following these rules, we revisit image-based RL tasks from the view of frequency domain and propose a novel augmentation method, namely Spectrum Random Masking (SRM),which is able to help agents to learn the whole frequency spectrum of observation for coping with various distributions and compatible with the pre-collected action and reward corresponding to original observation. Extensive experiments conducted on DMControl Generalization Benchmark   demonstrate the proposed SRM achieves the state-of-the-art performance with strong generalization potentials.",,https://openreview.net/forum?id=m16lH6XJsbb,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/f4e369c0a468d3aeeda0593ba90b5e55.png?t=1666429131.4204578,https://neurips.cc/virtual/2022/poster/54081,Not relevant,Defence,"['Generalization', 'Image-based Reinforcement Learning', 'Spectrum Random Masking', 'Data Diversity', 'Augmentations', 'Frequency Domain']",RL Tasks,,,,,,
595,https://neurips.cc/virtual/2022/poster/54257,Learning to Generate Inversion-Resistant Model Explanations,Poster,NeurIPS,2022,"The wide adoption of deep neural networks (DNNs) in mission-critical applications has spurred the need for interpretable models that provide explanations of the model's decisions. Unfortunately, previous studies have demonstrated that model explanations facilitate information leakage, rendering DNN models vulnerable to model inversion attacks. These attacks enable the adversary to reconstruct original images based on model explanations, thus leaking privacy-sensitive features. To this end, we present Generative Noise Injector for Model Explanations (GNIME), a novel defense framework that perturbs model explanations to minimize the risk of model inversion attacks while preserving the interpretabilities of the generated explanations. Specifically, we formulate the defense training as a two-player minimax game between the inversion attack network on the one hand, which aims to invert model explanations, and the noise generator network on the other, which aims to inject perturbations to tamper with model inversion attacks. We demonstrate that GNIME significantly decreases the information leakage in model explanations, decreasing transferable classification accuracy in facial recognition models by up to 84.8% while preserving the original functionality of model explanations.",,https://openreview.net/forum?id=iy2G-yLGuku,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54257.png?t=1669094738.3133647,https://neurips.cc/virtual/2022/poster/54257,Other attack,Defence,"['Model inversion attack', 'Interpretable models', 'Deep neural networks', 'Explainable AI', 'Information leakage', 'Generative Noise Injector for Model Explanations (GNIME)', 'Minimax game', 'Inversion attack network', 'Noise generator network', 'Facial recognition models']",,,,,,,
600,https://neurips.cc/virtual/2022/poster/55143,ZARTS: On Zero-order Optimization for Neural Architecture Search,Poster,NeurIPS,2022,"Differentiable architecture search (DARTS) has been a popular one-shot paradigm for NAS due to its high efficiency. It introduces trainable architecture parameters to represent the importance of candidate operations and proposes first/second-order approximation to estimate their gradients, making it possible to solve NAS by gradient descent algorithm. However, our in-depth empirical results show that the approximation often distorts the loss landscape, leading to the biased objective to optimize and, in turn, inaccurate gradient estimation for architecture parameters. This work turns to zero-order optimization and proposes a novel NAS scheme, called ZARTS, to search without enforcing the above approximation. Specifically, three representative zero-order optimization methods are introduced: RS, MGS, and GLD, among which MGS performs best by balancing the accuracy and speed. Moreover, we explore the connections between RS/MGS and gradient descent algorithm and show that our ZARTS can be seen as a robust gradient-free counterpart to DARTS. Extensive experiments on multiple datasets and search spaces show the remarkable performance of our method. In particular, results on 12 benchmarks verify the outstanding robustness of ZARTS, where the performance of DARTS collapses due to its known instability issue. Also, we search on the search space of DARTS to compare with peer methods, and our discovered architecture achieves 97.54\% accuracy on CIFAR-10 and 75.7\% top-1 accuracy on ImageNet. Finally, we combine our ZARTS with three orthogonal variants of DARTS for faster search speed and better performance.  Source code will be made publicly available at:  \url{https://github.com/vicFigure/ZARTS}.",,https://openreview.net/forum?id=QzFJmwwBMd,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/acab0116c354964a558e65bdd07ff047.png?t=1667717984.2368054,https://neurips.cc/virtual/2022/poster/55143,Evasion,Defence,"['Differentiable architecture search (DARTS)', 'Zero-order optimization', 'Neural Architecture Search (NAS)', 'Gradient descent algorithm', 'Loss landscape', 'Gradient estimation', 'Robustness', 'Instability issue']",,,,,,,
601,https://neurips.cc/virtual/2022/poster/53420,A Win-win Deal: Towards Sparse and Robust Pre-trained Language Models,Poster,NeurIPS,2022,"Despite the remarkable success of pre-trained language models (PLMs), they still face two challenges: First, large-scale PLMs are inefficient in terms of memory footprint and computation. Second, on the downstream tasks, PLMs tend to rely on the dataset bias and struggle to generalize to out-of-distribution (OOD) data. In response to the efficiency problem, recent studies show that dense PLMs can be replaced with sparse subnetworks without hurting the performance. Such subnetworks can be found in three scenarios: 1) the fine-tuned PLMs, 2) the raw PLMs and then fine-tuned in isolation, and even inside 3) PLMs without any parameter fine-tuning. However, these results are only obtained in the in-distribution (ID) setting. In this paper, we extend the study on PLMs subnetworks to the OOD setting, investigating whether sparsity and robustness to dataset bias can be achieved simultaneously. To this end, we conduct extensive experiments with the pre-trained BERT model on three natural language understanding (NLU) tasks. Our results demonstrate that \textbf{sparse and robust subnetworks (SRNets) can consistently be found in BERT}, across the aforementioned three scenarios, using different training and compression methods. Furthermore, we explore the upper bound of SRNets using the OOD information and show that \textbf{there exist sparse and almost unbiased BERT subnetworks}. Finally, we present 1) an analytical study that provides insights on how to promote the efficiency of SRNets searching process and 2) a solution to improve subnetworks' performance at high sparsity. The code is available at \url{https://github.com/llyx97/sparse-and-robust-PLM}.",,https://openreview.net/forum?id=UmaiVbwN1v,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53420.png?t=1668326288.147487,https://neurips.cc/virtual/2022/poster/53420,Not relevant,Defence,"['pre-trained language models', 'efficiency', 'generalization', 'out-of-distribution', 'subnetworks', 'dataset bias', 'performance', 'compression methods']",Natural Language Understanding,,,,,,
605,https://neurips.cc/virtual/2022/poster/55435,GAMA: Generative Adversarial Multi-Object Scene Attacks,Poster,NeurIPS,2022,"The majority of methods for crafting adversarial attacks have focused on scenes with a single dominant object (e.g., images from ImageNet). On the other hand, natural scenes include multiple dominant objects that are semantically related. Thus, it is crucial to explore designing attack strategies that look beyond learning on single-object scenes or attack single-object victim classifiers. Due to their inherent property of strong transferability of perturbations to unknown models, this paper presents the first approach of using generative models for adversarial attacks on multi-object scenes. In order to represent the relationships between different objects in the input scene, we leverage upon the open-sourced pre-trained vision-language model CLIP (Contrastive Language-Image Pre-training), with the motivation to exploit the encoded semantics in the language space along with the visual space. We call this attack approach Generative Adversarial Multi-object Attacks (GAMA). GAMA demonstrates the utility of the CLIP model as an attacker's tool to train formidable perturbation generators for multi-object scenes. Using the joint image-text features to train the generator, we show that GAMA can craft potent transferable perturbations in order to fool victim classifiers in various attack settings. For example, GAMA triggers ~16% more misclassification than state-of-the-art generative approaches in black-box settings where both the classifier architecture and data distribution of the attacker are different from the victim. Our code is available here: https://abhishekaich27.github.io/gama.html",,https://openreview.net/forum?id=DRckHIGk8qw,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/9b8619251a19057cff70779273e95aa6.png?t=1666578584.2458084,https://neurips.cc/media/neurips-2022/Slides/55435.pdf,Evasion,Attack,"['Adversarial attacks', 'Multi-object scenes', 'Generative models', 'CLIP', 'Vision-language model', 'Perturbation generators', 'Transferable perturbations', 'Black-box settings', 'Victim classifiers', 'Misclassification']",,,,,,,
611,https://neurips.cc/virtual/2022/poster/54827,Factored DRO: Factored Distributionally Robust Policies for Contextual Bandits,Poster,NeurIPS,2022,"While there has been extensive work on learning from offline data for contextual multi-armed bandit settings, existing methods typically assume there is no environment shift: that the learned policy will operate in the same environmental process as that of data collection. However, this assumption may limit the use of these methods for many practical situations where there may be distribution shifts. In this work we propose Factored Distributionally Robust Optimization (Factored-DRO), which is able to separately handle distribution shifts in the context distribution and shifts in the reward generating process. Prior work that either ignores potential shifts in the context, or considers them jointly, can lead to performance that is too conservative, especially under certain forms of reward feedback. Our Factored-DRO objective mitigates this by considering the shifts separately, and our proposed estimators are consistent and converge asymptotically. We also introduce a practical algorithm and demonstrate promising empirical results in environments based on real-world datasets, such as voting outcomes and scene classification.",,https://openreview.net/forum?id=k6WzeLZjxuP,https://neurips.cc/virtual/2022/poster/54827,https://neurips.cc/virtual/2022/poster/54827,Robustness,Defence,"['Factored Distributionally Robust Optimization (Factored-DRO)', 'environmental process', 'distribution shifts', 'context distribution', 'reward generating process', 'Factored-DRO objective', 'estimators', 'consistent', 'converge asymptotically', 'algorithm', 'empirical results', 'voting outcomes', 'scene classification']",['real-world datasets'],,,,,,
621,https://neurips.cc/virtual/2022/poster/54749,Random Normalization Aggregation for Adversarial Defense,Poster,NeurIPS,2022,"The vulnerability of deep neural networks has been widely found in various models as well as tasks where slight perturbations on the inputs could lead to incorrect predictions. These perturbed inputs are known as adversarial examples and one of the intriguing properties of them is Adversarial Transfersability, i.e. the capability of adversarial examples to fool other models. Traditionally, this transferability is always regarded as a critical threat to the defense against adversarial attacks, however, we argue that the network robustness can be significantly boosted by utilizing adversarial transferability from a new perspective. In this work, we first discuss the influence of different popular normalization layers on the adversarial transferability, and then provide both empirical evidence and theoretical analysis to shed light on the relationship between normalization types and transferability. Based on our theoretical analysis, we propose a simple yet effective module named Random Normalization Aggregation (RNA) which replaces the batch normalization layers in the networks and aggregates different selected normalization types to form a huge random space. Specifically, a random path is sampled during each inference procedure so that the network itself can be treated as an ensemble of a wide range of different models. Since the entire random space is designed with low adversarial transferability, it is difficult to perform effective attacks even when the network parameters are accessible. We conduct extensive experiments on various models and datasets, and demonstrate the strong superiority of proposed algorithm. The PyTorch code is available at https://github.com/UniSerj/Random-Norm-Aggregation and the MindSpore code is available at https://gitee.com/mindspore/models/tree/master/research/cv/RNA.",,https://openreview.net/forum?id=K4W92FUXSF9,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/e8e0dd181e4ee545195120626098bfba.png?t=1666250143.7181442,https://neurips.cc/virtual/2022/poster/54749,Evasion,Defence,"['Adversarial defense', 'Adversarial examples', 'Adversarial transferability', 'Deep neural networks', 'Random Normalization Aggregation', 'RNA', 'Normalization layers']",,,,,,,
629,https://neurips.cc/virtual/2022/poster/54700,Rethinking Image Restoration for Object Detection,Poster,NeurIPS,2022,"Although image restoration has achieved significant progress, its potential to assist object detectors in adverse imaging conditions lacks enough attention. It is reported that the existing image restoration methods cannot improve the object detector performance and sometimes even reduce the detection performance. To address the issue, we propose a targeted adversarial attack in the restoration procedure to boost object detection performance after restoration. Specifically, we present an ADAM-like adversarial attack to generate pseudo ground truth for restoration training. Resultant restored images are close to original sharp images, and at the same time, lead to better results of object detection. We conduct extensive experiments in image dehazing and low light enhancement and show the superiority of our method over conventional training and other domain adaptation and multi-task methods. The proposed pipeline can be applied to all restoration methods and detectors in both one- and two-stage.",,https://openreview.net/forum?id=se2oxj-6Nz,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54700.png?t=1669036300.7452343,https://neurips.cc/virtual/2022/poster/54700,Evasion,Defence,"['Image restoration', 'Object detection', 'Adversarial attack', 'ADAM-like adversarial attack', 'Dehazing', 'Low light enhancement', 'Domain adaptation', 'Multi-task learning']",Not clear,,,,,,
631,https://neurips.cc/virtual/2022/poster/55389,Phase Transition from Clean Training to Adversarial Training,Poster,NeurIPS,2022,"Adversarial training is one important algorithm to achieve robust machine learning models. However, numerous empirical results show a great performance degradation from clean training to adversarial training (e.g., 90+\% vs 67\% testing accuracy on CIFAR-10 dataset), which does not match the theoretical guarantee delivered by the existing studies. Such a gap inspires us to explore the existence of an (asymptotic) phase transition phenomenon with respect to the attack strength: adversarial training is as well behaved as clean training in the small-attack regime, but there is a sharp transition from clean training to adversarial training in the large-attack regime. We validate this conjecture in linear regression models, and conduct comprehensive experiments in deep neural networks.",,https://openreview.net/forum?id=gwsnBjNcVEe,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55389.png?t=1667918155.3438783,https://neurips.cc/virtual/2022/poster/55389,Robustness,Defence,"['Adversarial training', 'Robust machine learning', 'Phase transition', 'Linear regression', 'Deep neural networks']",Not clear,Other aspects,Evasion,,,Arguable,Arguable
632,https://neurips.cc/virtual/2022/poster/55382,Adversarial Style Augmentation for Domain Generalized Urban-Scene Segmentation,Poster,NeurIPS,2022,"In this paper, we consider the problem of domain generalization in semantic segmentation, which aims to learn a robust model using only labeled synthetic (source) data. The model is expected to perform well on unseen real (target) domains. Our study finds that the image style variation can largely influence the model's performance and the style features can be well represented by the channel-wise mean and standard deviation of images. Inspired by this, we propose a novel adversarial style augmentation (AdvStyle) approach, which can dynamically generate hard stylized images during training and thus can effectively prevent the model from overfitting on the source domain. Specifically, AdvStyle regards the style feature as a learnable parameter and updates it by adversarial training. The learned adversarial style feature is used to construct an adversarial image for robust model training. AdvStyle is easy to implement and can be readily applied to different models. Experiments on two synthetic-to-real semantic segmentation benchmarks demonstrate that AdvStyle can significantly improve the model performance on unseen real domains and show that we can achieve the state of the art. Moreover, AdvStyle can be employed to domain generalized image classification and produces a clear improvement on the considered datasets.",,https://openreview.net/forum?id=lXUp6skJ7r,https://neurips.cc/virtual/2022/poster/55382,https://neurips.cc/virtual/2022/poster/55382,Robustness,Defence,"['adversarial training', 'domain generalization', 'semantic segmentation', 'style augmentation', 'channel-wise mean and standard deviation', 'adversarial style feature', 'adversarial image', 'image classification', 'overfitting', 'robust model training']",Urban-Scene Segmentation,,,,,,
637,https://neurips.cc/virtual/2022/poster/55357,Hierarchical  Normalization for Robust Monocular Depth Estimation,Poster,NeurIPS,2022,"In this paper, we address monocular depth estimation with deep neural networks. To enable training of deep monocular estimation models with various sources of datasets, state-of-the-art methods adopt image-level normalization strategies to generate affine-invariant depth representations. However, learning with the image-level normalization mainly emphasizes the relations of pixel representations with the global statistic in the images, such as the structure of the scene, while the fine-grained depth difference may be overlooked. In this paper, we propose a novel multi-scale depth normalization method that hierarchically normalizes the depth representations based on spatial information and depth distributions. Compared with previous normalization strategies applied only at the holistic image level, the proposed hierarchical normalization can effectively preserve the fine-grained details and improve accuracy. We present two strategies that define the hierarchical normalization contexts in the depth domain and the spatial domain, respectively. Our extensive experiments show that the proposed normalization strategy remarkably outperforms previous normalization methods, and we set new state-of-the-art on five zero-shot transfer benchmark datasets. ",,https://openreview.net/forum?id=BNqRpzwyOFU,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55357.png?t=1668518493.464126,https://neurips.cc/virtual/2022/poster/55357,Robustness,Defence,"['Monocular depth estimation', 'Deep neural networks', 'Image-level normalization', 'Hierarchical normalization', 'Multi-scale depth normalization', 'Spatial information', 'Depth distributions', 'Fine-grained details', 'Accuracy']",,,,,,,
661,https://neurips.cc/virtual/2022/poster/55172,Bayesian Risk Markov Decision Processes,Poster,NeurIPS,2022,"We consider finite-horizon Markov Decision Processes where parameters, such as transition probabilities, are unknown and estimated from data. The popular distributionally robust approach to addressing the parameter uncertainty can sometimes be overly conservative. In this paper, we propose a new formulation, Bayesian risk Markov decision process (BR-MDP), to address parameter uncertainty in MDPs, where a risk functional is applied in nested form to the expected total cost with respect to the Bayesian posterior distributions of the unknown parameters. The proposed formulation provides more flexible risk attitudes towards parameter uncertainty and takes into account the availability of data in future time stages. To solve the proposed formulation with the conditional value-at-risk (CVaR) risk functional, we propose an efficient approximation algorithm by deriving an analytical approximation of the value function and utilizing the convexity of CVaR. We demonstrate the empirical performance of the BR-MDP formulation and proposed algorithms on a gambler’s betting problem and an inventory control problem.",,https://openreview.net/forum?id=PO6cKxILdi,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/8f19793b2671094e63a15ab883d50137.png?t=1665958474.8129158,https://neurips.cc/virtual/2022/poster/55172,Not relevant,Other aspects,"['Bayesian Risk Markov Decision Processes', 'parameter uncertainty', 'MDPs', 'risk functional', 'Bayesian posterior distributions', 'conditional value-at-risk', 'CVaR', 'approximation algorithm', ""gambler's betting problem"", 'inventory control problem']",Not Clear,,,,,,
663,https://neurips.cc/virtual/2022/poster/55165,Watermarking for Out-of-distribution Detection,Poster,NeurIPS,2022,"Out-of-distribution (OOD) detection aims to identify OOD data based on representations extracted from well-trained deep models. However, existing methods largely ignore the reprogramming property of deep models and thus may not fully unleash their intrinsic strength: without modifying parameters of a well-trained deep model, we can reprogram this model for a new purpose via data-level manipulation (e.g., adding a specific feature perturbation). This property motivates us to reprogram a classification model to excel at OOD detection (a new task), and thus we propose a general methodology named watermarking in this paper. Specifically, we learn a unified pattern that is superimposed onto features of original data, and the model's detection capability is largely boosted after watermarking. Extensive experiments verify the effectiveness of watermarking, demonstrating the significance of the reprogramming property of deep models in OOD detection.",,https://openreview.net/forum?id=6rhl2k1SUGs,https://neurips.cc/virtual/2022/poster/55165,https://neurips.cc/virtual/2022/poster/55165,Robustness,Defence,"['Out-of-distribution detection', 'Deep models', 'OOD data', 'Classification model', 'Watermarking', 'Feature perturbation']",Not clear,Other aspects,Robustness,,,,Wrong
664,https://neurips.cc/virtual/2022/poster/55161,Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection,Poster,NeurIPS,2022,"Deep neural networks (DNNs) have demonstrated their superiority in practice. Arguably, the rapid development of DNNs is largely benefited from high-quality (open-sourced) datasets, based on which researchers and developers can easily evaluate and improve their learning methods. Since the data collection is usually time-consuming or even expensive, how to protect their copyrights is of great significance and worth further exploration. In this paper, we revisit dataset ownership verification. We find that existing verification methods introduced new security risks in DNNs trained on the protected dataset, due to the targeted nature of poison-only backdoor watermarks. To alleviate this problem, in this work, we explore the untargeted backdoor watermarking scheme, where the abnormal model behaviors are not deterministic. Specifically, we introduce two dispersibilities and prove their correlation, based on which we design the untargeted backdoor watermark under both poisoned-label and clean-label settings. We also discuss how to use the proposed untargeted backdoor watermark for dataset ownership verification. Experiments on benchmark datasets verify the effectiveness of our methods and their resistance to existing backdoor defenses.",,https://openreview.net/forum?id=kcQiIrvA_nz,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55161.png?t=1669264295.2967985,https://neurips.cc/virtual/2022/poster/55161,Other attack,Defence,"['deep neural networks', 'dataset ownership verification', 'poison-only backdoor watermarks', 'untargeted backdoor watermark', 'poisoned-label', 'clean-label']",,Defence,Other attack,,Poisoning,,
669,https://neurips.cc/virtual/2022/poster/55107,Why Do Artificially Generated Data Help Adversarial Robustness,Poster,NeurIPS,2022,"In the adversarial training framework of \cite{carmon2019unlabeled,gowal2021improving}, people use generated/real unlabeled data with pseudolabels to improve adversarial robustness. We provide statistical insights to explain why the artificially generated data improve adversarial training. In particular, we study how the attack strength and the quality of the unlabeled data affect adversarial robustness in this framework. Our results show that with a high-quality unlabeled data generator, adversarial training can benefit greatly from this framework under large attack strength, while a poor generator can still help to some extent. To make adaptions concerning the quality of generated data, we propose an algorithm that performs online adjustment to the weight between the labeled real data and the generated data, aiming to optimize the adversarial risk. Numerical studies are conducted to verify our theories and show the effectiveness of the proposed algorithm.",,https://openreview.net/forum?id=W-Z8n9HrWn0,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55107.png?t=1667918129.9935443,https://neurips.cc/virtual/2022/poster/55107,Robustness,Defence,"['adversarial training', 'unlabeled data', 'attack strength', 'quality of unlabeled data', 'adversarial robustness', 'generated data', 'real data', 'algorithm', 'online adjustment', 'weight']",,Defence,Evasion,Other aspects,,Arguable,
697,https://neurips.cc/virtual/2022/poster/54898,Sampling without Replacement Leads to Faster Rates in Finite-Sum Minimax Optimization,Poster,NeurIPS,2022,"We analyze the convergence rates of stochastic gradient algorithms for smooth finite-sum minimax optimization and show that, for many such algorithms, sampling the data points \emph{without replacement} leads to faster convergence compared to sampling with replacement. For the smooth and strongly convex-strongly concave setting, we consider gradient descent ascent and the proximal point method, and present a unified analysis of two popular without-replacement sampling strategies, namely \emph{Random Reshuffling} (RR), which shuffles the data every epoch, and \emph{Single Shuffling} or \emph{Shuffle Once} (SO), which shuffles only at the beginning. We obtain tight convergence rates for RR and SO and demonstrate that these strategies lead to faster convergence than uniform sampling. Moving beyond convexity, we obtain similar results for smooth nonconvex-nonconcave objectives satisfying a two-sided Polyak-\L{}ojasiewicz inequality. Finally, we demonstrate that our techniques are general enough to analyze the effect of \emph{data-ordering attacks}, where an adversary manipulates the order in which data points are supplied to the optimizer. Our analysis also recovers tight rates for the \emph{incremental gradient} method, where the data points are not shuffled at all.",,https://openreview.net/forum?id=CTqjKUAyRBt,https://neurips.cc/virtual/2022/poster/54898,https://neurips.cc/virtual/2022/poster/54898,Not relevant,Defence,"['Finite-sum minimax optimization', 'Stochastic gradient algorithms', 'Smooth and strongly convex-strongly concave', 'Proximal point method', 'Random Reshuffling', 'Single Shuffling', 'Shuffle Once', 'Uniform sampling', 'Nonconvex-nonconcave objectives', 'Polyak-Lojasiewicz inequality', 'Data-ordering attacks', 'Incremental gradient method']",,,,,,,
699,https://neurips.cc/virtual/2022/poster/54875,Practical Adversarial Multivalid Conformal Prediction,Poster,NeurIPS,2022,"We give a simple, generic conformal prediction method for sequential prediction that achieves target empirical coverage guarantees on adversarial data. It is computationally lightweight --- comparable to split conformal prediction --- but does not require having a held-out validation set, and so all data can be used for training models from which to derive a conformal score. Furthermore, it gives stronger than marginal coverage guarantees in two ways. First, it gives threshold-calibrated prediction sets that have correct empirical coverage even conditional on the threshold used to form the prediction set from the conformal score. Second, the user can specify an arbitrary collection of subsets of the feature space --- possibly intersecting --- and the coverage guarantees will also hold conditional on membership in each of these subsets. We call our algorithm MVP, short for MultiValid Prediction. We give both theory and an extensive set of empirical evaluations. ",,https://openreview.net/forum?id=QNjyrDBx6tz,https://neurips.cc/virtual/2022/poster/54875,https://neurips.cc/virtual/2022/poster/54875,Evasion,Defence,"['adversarial data', 'conformal prediction', 'sequential prediction', 'empirical coverage', 'feature space', 'threshold-calibrated prediction sets', 'MultiValid Prediction (MVP)']",Not clear,,,,,,
703,https://neurips.cc/virtual/2022/poster/54637,GBA: A Tuning-free Approach to Switch between Synchronous and Asynchronous Training for Recommendation Models,Poster,NeurIPS,2022,"High-concurrency asynchronous training upon parameter server (PS) architecture and high-performance synchronous training upon all-reduce (AR) architecture are the most commonly deployed distributed training modes for recommendation models. Although synchronous AR training is designed to have higher training efficiency, asynchronous PS training would be a better choice for training speed when there are stragglers (slow workers) in the shared cluster, especially under limited computing resources. An ideal way to take full advantage of these two training modes is to switch between them upon the cluster status. However, switching training modes often requires tuning hyper-parameters, which is extremely time- and resource-consuming. We find two obstacles to a tuning-free approach: the different distribution of the gradient values and the stale gradients from the stragglers. This paper proposes Global Batch gradients Aggregation (GBA) over PS, which aggregates and applies gradients with the same global batch size as the synchronous training. A token-control process is implemented to assemble the gradients and decay the gradients with severe staleness. We provide the convergence analysis to reveal that GBA has comparable convergence properties with the synchronous training, and demonstrate the robustness of GBA the recommendation models against the gradient staleness. Experiments on three industrial-scale recommendation tasks show that GBA is an effective tuning-free approach for switching. Compared to the state-of-the-art derived asynchronous training, GBA achieves up to 0.2% improvement on the AUC metric, which is significant for the recommendation models. Meanwhile, under the strained hardware resource, GBA speeds up at least 2.4x compared to synchronous training.",,https://openreview.net/forum?id=vphSm8QmLFm,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54637.png?t=1668159538.7063897,https://neurips.cc/virtual/2022/poster/54637,Not relevant,Defence,"['Synchronous training', 'Asynchronous training', 'Recommendation models', 'Parameter server', 'All-reduce', 'Cluster status', 'Hyper-parameter tuning', 'Gradient values', 'Stale gradients', 'Global Batch gradients Aggregation', 'Token-control', 'Convergence analysis', 'Robustness', 'Industrial-scale recommendation tasks', 'AUC metric', 'Hardware resource', 'Speed up']",['Recommendation models'],,,,,,
705,https://neurips.cc/virtual/2022/poster/54634,Learning Robust Rule Representations for Abstract Reasoning via Internal Inferences,Poster,NeurIPS,2022,"Abstract reasoning, as one of the hallmarks of human intelligence, involves collecting information, identifying abstract rules, and applying the rules to solve new problems. Although neural networks have achieved human-level performances in several tasks, the abstract reasoning techniques still far lag behind due to the complexity of learning and applying the logic rules, especially in an unsupervised manner. In this work, we propose a novel framework, ARII, that learns rule representations for Abstract Reasoning via Internal Inferences. The key idea is to repeatedly apply a rule to different instances in hope of having a comprehensive understanding (i.e., representations) of the rule. Specifically, ARII consists of a rule encoder, a reasoner, and an internal referrer. Based on the representations produced by the rule encoder, the reasoner draws the conclusion while the referrer performs internal inferences to regularize rule representations to be robust and generalizable. We evaluate ARII on two benchmark datasets, including PGM and I-RAVEN. We observe that ARII achieves new state-of-the-art records on the majority of the reasoning tasks, including most of the generalization tests in PGM. Our codes are available at https://github.com/Zhangwenbo0324/ARII.",,https://openreview.net/forum?id=UwzrP-B38jK,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54634.png?t=1669471502.9469075,https://neurips.cc/virtual/2022/poster/54634,Not relevant,Other aspects,"['Abstract reasoning', 'Logic rules', 'Unsupervised learning', 'Rule representations', 'Internal inferences', 'Reasoner', 'Referrer', 'PGM', 'I-RAVEN']",['None'],,,,,,
709,https://neurips.cc/virtual/2022/poster/54552,u-HuBERT: Unified Mixed-Modal Speech Pretraining And Zero-Shot Transfer to Unlabeled Modality,Poster,NeurIPS,2022,"While audio-visual speech models can yield superior performance and robustness compared to audio-only models, their development and adoption are hindered by the lack of labeled and unlabeled audio-visual data and the cost to deploy one model per modality. In this paper, we present u-HuBERT, a self-supervised pre-training framework that can leverage both multimodal and unimodal speech with a unified masked cluster prediction objective. By utilizing modality dropout during pre-training, we demonstrate that a single fine-tuned model can achieve performance on par or better than the state-of-the-art modality-specific models. Moreover, our model fine-tuned only on audio can perform well with audio-visual and visual speech input, achieving zero-shot modality generalization for multiple speech processing tasks. In particular, our single model yields 1.2%/1.4%/27.2% speech recognition word error rate on LRS3 with audio-visual/audio/visual input.",,https://openreview.net/forum?id=zrAUoI2JA2,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54552.png?t=1669139335.2237504,https://neurips.cc/virtual/2022/poster/54552,Not relevant,Defence,"['u-HuBERT', 'Mixed-Modal Speech Pretraining', 'Zero-Shot Transfer', 'Unlabeled Modality', 'audio-visual speech', 'masked cluster prediction', 'modality dropout', 'speech recognition', 'word error rate']",['Speech Processing'],,,,,,
710,https://neurips.cc/virtual/2022/poster/54601,LogiGAN: Learning Logical Reasoning via Adversarial Pre-training,Poster,NeurIPS,2022,"We present LogiGAN, an unsupervised adversarial pre-training framework for improving logical reasoning abilities of language models. Upon automatic identification of logical reasoning phenomena in massive text corpus via detection heuristics, we train language models to predict the masked-out logical statements. Inspired by the facilitation effect of reflective thinking in human learning, we analogically simulate the learning-thinking process with an adversarial Generator-Verifier architecture to assist logic learning. LogiGAN implements a novel sequential GAN approach that (a) circumvents the non-differentiable challenge of the sequential GAN by leveraging the Generator as a sentence-level generative likelihood scorer with a learning objective of reaching scoring consensus with the Verifier; (b) is computationally feasible for large-scale pre-training with arbitrary target length. Both base and large size language models pre-trained with LogiGAN demonstrate obvious performance improvement on 12 datasets requiring general reasoning abilities, revealing the fundamental role of logic in broad reasoning, as well as the effectiveness of LogiGAN. Ablation studies on LogiGAN components reveal the relative orthogonality between linguistic and logic abilities and suggest that reflective thinking's facilitation effect might also generalize to machine learning.",,https://openreview.net/forum?id=skgJy0CjAO,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54601.png?t=1669108608.477311,https://neurips.cc/virtual/2022/poster/54601,Robustness,Defence,"['LogiGAN', 'Logical Reasoning', 'Adversarial Pre-training', 'Language models', 'Automatic identification', 'Logical statements', 'Generator-Verifier architecture', 'Sequential GAN', 'Pre-training', 'Target length', 'Reasoning abilities', 'Ablation studies']",,Not relevant,Not relevant,Other aspects,Robustness,,Wrong
718,https://neurips.cc/virtual/2022/poster/54502,SALSA: Attacking Lattice Cryptography with Transformers,Poster,NeurIPS,2022,"Currently deployed public-key cryptosystems will be vulnerable to attacks by full-scale quantum computers. Consequently, ""quantum resistant"" cryptosystems are in high demand, and lattice-based cryptosystems, based on a hard problem known as Learning With Errors (LWE), have emerged as strong contenders for standardization. In this work, we train transformers to perform modular arithmetic and mix half-trained models and statistical cryptanalysis techniques to propose SALSA: a machine learning attack on LWE-based cryptographic schemes. SALSA can fully recover secrets for small-to-mid size LWE instances with sparse binary secrets, and may scale to attack real world LWE-based cryptosystems.",,https://openreview.net/forum?id=p4xLHcTLRwh,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/34f9a343f945196b66f807e0eb6249fd.png?t=1666282876.0916111,https://neurips.cc/virtual/2022/poster/54502,Other attack,Attack,"['Lattice cryptography', 'quantum resistant', 'Learning With Errors (LWE)', 'Modular arithmetic', 'Transformer', 'Machine learning attack']",['Cryptography'],Attack,Data Extraction,Other aspects,Other attack,,
721,https://neurips.cc/virtual/2022/poster/54477,Online Minimax Multiobjective Optimization: Multicalibeating and Other Applications,Poster,NeurIPS,2022,"We introduce a simple but general online learning framework in which a learner plays against an adversary in a vector-valued game that changes every round. Even though the learner's objective is not convex-concave (and so the minimax theorem does not apply), we give a simple algorithm that can compete with the setting in which the adversary must announce their action first, with optimally diminishing regret. We demonstrate the power of our framework by using it to (re)derive optimal bounds and efficient algorithms across a variety of domains, ranging from multicalibration to a large set of no-regret algorithms, to a variant of Blackwell's approachability theorem for polytopes with fast convergence rates. As a new application, we show how to ``(multi)calibeat'' an arbitrary collection of forecasters --- achieving an exponentially improved dependence on the number of models we are competing against, compared to prior work. ",,https://openreview.net/forum?id=Epk1RQUpOj0,https://neurips.cc/virtual/2022/poster/54477,https://neurips.cc/virtual/2022/poster/54477,Not relevant,Defence,"['Online Minimax Multiobjective Optimization', 'Multicalibeating', 'Regret', 'Adversary', 'Vector-valued game', 'Minimax theorem', 'Algorithm', 'Optimal bounds', 'Efficient algorithms', ""Blackwell's approachability theorem"", 'Forecasters']",,Not relevant,Not relevant,,,,
722,https://neurips.cc/virtual/2022/poster/54446,ConfounderGAN: Protecting Image Data Privacy with Causal Confounder,Poster,NeurIPS,2022,"The success of deep learning is partly attributed to the availability of massive data downloaded freely from the Internet. However, it also means that users' private data may be collected by commercial organizations without consent and used to train their models. Therefore, it's important and necessary to develop a method or tool to prevent unauthorized data exploitation. In this paper, we propose ConfounderGAN, a generative adversarial network (GAN) that can make personal image data unlearnable to protect the data privacy of its owners. Specifically, the noise produced by the generator for each image has the confounder property. It can build spurious correlations between images and labels, so that the model cannot learn the correct mapping from images to labels in this noise-added dataset. Meanwhile, the discriminator is used to ensure that the generated noise is small and imperceptible, thereby remaining the normal utility of the encrypted image for humans. The experiments are conducted in six image classification datasets, including three natural object datasets and three medical datasets. The results demonstrate that our method not only outperforms state-of-the-art methods in standard settings, but can also be applied to fast encryption scenarios. Moreover, we show a series of transferability and stability experiments to further illustrate the effectiveness and superiority of our method.",,https://openreview.net/forum?id=XxmOKCt8dO9,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/dd03de08bfdff4d8ab01117276564cc7.png?t=1666159400.6940215,https://neurips.cc/virtual/2022/poster/54446,Evasion,Defence,"['Adversarial machine learning', 'Generative Adversarial Networks (GANs)', 'Data privacy', 'Confounder property', 'Spurious correlation', 'Transferability', 'Stability']","['Image classification', 'Natural object datasets', 'Medical datasets']",,,,,,
724,https://neurips.cc/virtual/2022/poster/54359,Robust Rent Division,Poster,NeurIPS,2022,"In fair rent division, the problem is to assign rooms to roommates and fairly split the rent based on roommates' reported valuations for the rooms. Envy-free rent division is the most popular application on the fair division website Spliddit. The standard model assumes that agents can correctly report their valuations for each room. In practice, agents may be unsure about their valuations, for example because they have had only limited time to inspect the rooms. Our goal is to find a robust rent division that remains fair even if agent valuations are slightly different from the reported ones. We introduce the lexislack solution, which selects a rent division that remains envy-free for valuations within as large a radius as possible of the reported valuations. We also consider robustness notions for valuations that come from a probability distribution, and use results from learning theory to show how we can find rent divisions that (almost) maximize the probability of being envy-free, or that minimize the expected envy. We show that an almost optimal allocation can be identified based on polynomially many samples from the valuation distribution. Finding the best allocation given these samples is NP-hard, but in practice such an allocation can be found using integer linear programming.",,https://openreview.net/forum?id=eRBVi61Vct1,https://neurips.cc/virtual/2022/poster/54359,https://neurips.cc/virtual/2022/poster/54359,Not relevant,Other aspects,"['rent division', 'envy-free', 'fairness', 'robustness', 'lexislack solution', 'valuations', 'radius', 'probability distribution', 'learning theory', 'integer linear programming']",['Roommates'],,,,,,
728,https://neurips.cc/virtual/2022/poster/54343,SwinTrack: A Simple and Strong Baseline for Transformer Tracking,Poster,NeurIPS,2022,"Recently Transformer has been largely explored in tracking and shown state-of-the-art (SOTA) performance. However, existing efforts mainly focus on fusing and enhancing features generated by convolutional neural networks (CNNs). The potential of Transformer in representation learning remains under-explored. In this paper, we aim to further unleash the power of Transformer by proposing a simple yet efficient fully-attentional tracker, dubbed SwinTrack, within classic Siamese framework. In particular, both representation learning and feature fusion in SwinTrack leverage the Transformer architecture, enabling better feature interactions for tracking than pure CNN or hybrid CNN-Transformer frameworks. Besides, to further enhance robustness, we present a novel motion token that embeds historical target trajectory to improve tracking by providing temporal context. Our motion token is lightweight with negligible computation but brings clear gains. In our thorough experiments, SwinTrack exceeds existing approaches on multiple benchmarks. Particularly, on the challenging LaSOT, SwinTrack sets a new record with 0.713 SUC score. It also achieves SOTA results on other benchmarks. We expect SwinTrack to serve as a solid baseline for Transformer tracking and facilitate future research. Our codes and results are released at https://github.com/LitingLin/SwinTrack.",,https://openreview.net/forum?id=9h3KsOVXhLZ,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/fc0cc602ce843b5393684a7fc1b566bc.png?t=1666525545.9815378,https://neurips.cc/virtual/2022/poster/54343,Not relevant,Defence,"['Transformer', 'Tracking', 'CNN', 'Siamese', 'Robustness', 'Motion token', 'Temporal context', 'SUC score', 'Baseline']",,,,,,,
729,https://neurips.cc/virtual/2022/poster/54336,Receding Horizon Inverse Reinforcement Learning,Poster,NeurIPS,2022,"Inverse reinforcement learning (IRL) seeks to infer a cost function that explains the underlying goals and preferences of expert demonstrations. This paper presents Receding Horizon Inverse Reinforcement Learning (RHIRL), a new IRL algorithm for high-dimensional, noisy, continuous systems with black-box dynamic models. RHIRL addresses two key challenges of IRL: scalability and robustness. To handle high-dimensional continuous systems, RHIRL matches the induced optimal trajectories with expert demonstrations locally in a receding horizon manner and ""stitches"" together the local solutions to learn the cost; it thereby avoids the ""curse of dimensionality"". This contrasts sharply with earlier algorithms that match with expert demonstrations globally over the entire high-dimensional state space. To be robust against imperfect expert demonstrations and control noise, RHIRL learns a state-dependent cost function ``disentangled'' from system dynamics under mild conditions. Experiments on benchmark tasks show that RHIRL outperforms several leading IRL algorithms in most instances. We also prove that the cumulative error of RHIRL grows linearly with the task duration.  ",,https://openreview.net/forum?id=CgkjJaKBvkX,https://neurips.cc/virtual/2022/poster/54336,https://neurips.cc/virtual/2022/poster/54336,Not relevant,Defence,"['Inverse reinforcement learning', 'Receding Horizon', 'high-dimensional', 'noisy', 'continuous systems', 'black-box dynamic models', 'scalability', 'robustness', 'stitching', 'curse of dimensionality', 'state-dependent cost function', 'disentangled', 'benchmark tasks', 'cumulative error', 'task duration']",,,,,,,
752,https://neurips.cc/virtual/2022/poster/54261,Revisiting Injective Attacks on Recommender Systems,Poster,NeurIPS,2022,"Recent studies have demonstrated that recommender systems (RecSys) are vulnerable to injective attacks.Given a limited fake user budget, attackers can inject fake users with carefully designed behaviors into the open platforms, making RecSys recommend a target item to more real users for profits. In this paper, we first revisit existing attackers and reveal that they suffer from the difficulty-agnostic and diversity-deficit issues. Existing attackers concentrate their efforts on difficult users who have low tendencies toward the target item, thus reducing their effectiveness. Moreover, they are incapable of affecting the target RecSys to recommend the target item to real users in a diverse manner, because their generated fake user behaviors are dominated  by large communities. To alleviate these two issues, we propose a difficulty and diversity aware attacker, namely DADA. We design the difficulty-aware and diversity-aware objectives to enable easy users from various communities to contribute more weights when optimizing attackers. By incorporating these two objectives, the proposed attacker DADA can concentrate on easy users while also affecting a broader range of real users simultaneously, thereby boosting the effectiveness. Extensive experiments on three real-world datasets demonstrate the effectiveness of our proposed attacker.",,https://openreview.net/forum?id=e5HTq2VA7mu,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54261.png?t=1669379403.8736582,https://neurips.cc/virtual/2022/poster/54261,Other attack,Attack,"['Recommender systems', 'Injective attacks', 'Fake user budget', 'Open platforms', 'Target item', 'Difficulty-agnostic', 'Diversity-deficit', 'Difficulty-aware', 'Diversity-aware', 'DADA']",,Attack,Evasion,,Other attack,,
755,https://neurips.cc/virtual/2022/poster/54236,On Robust Multiclass Learnability,Poster,NeurIPS,2022,"This work analyzes the robust learning problem in the multiclass setting. Under the framework of Probably Approximately Correct (PAC) learning, we first show that the graph dimension and the Natarajan dimension, which characterize the standard multiclass learnability, are no longer applicable in robust learning problem. We then generalize these notions to the robust learning setting, denoted as the adversarial graph dimension (AG-dimension) and the adversarial Natarajan dimension (AN-dimension). Upper and lower bounds of the sample complexity of robust multiclass learning are rigorously derived based on the AG-dimension and AN-dimension, respectively. Moreover, we calculate the AG-dimension and AN-dimension of the class of linear multiclass predictors, and show that the graph (Natarajan) dimension is of the same order as the AG(AN)-dimension. Finally, we prove that the AG-dimension and AN-dimension are not equivalent.",,https://openreview.net/forum?id=KCN0ZRqxcDm,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/0a988fc2992add2d3233e19c7aadfdea.png?t=1666408176.8972294,https://neurips.cc/virtual/2022/poster/54236,Robustness,Defence,"['Robust learning', 'Multiclass setting', 'Probably Approximately Correct (PAC) learning', 'Adversarial graph dimension (AG-dimension)', 'Adversarial Natarajan dimension (AN-dimension)', 'Sample complexity', 'Linear multiclass predictors']",,,,,,,
760,https://neurips.cc/virtual/2022/poster/54164,Pre-activation Distributions Expose Backdoor Neurons,Poster,NeurIPS,2022,"Convolutional neural networks (CNN) can be manipulated to perform specific behaviors when encountering a particular trigger pattern without affecting the performance on normal samples, which is referred to as backdoor attack. The backdoor attack is usually achieved by injecting a small proportion of poisoned samples into the training set, through which the victim trains a model embedded with the designated backdoor. In this work, we demonstrate that backdoor neurons are exposed by their pre-activation distributions, where populations from benign data and poisoned data show significantly different moments. This property is shown to be attack-invariant and allows us to efficiently locate backdoor neurons. On this basis, we make several proper assumptions on the neuron activation distributions, and propose two backdoor neuron detection strategies based on (1) the differential entropy of the neurons, and (2) the Kullback-Leibler divergence between the benign sample distribution and a poisoned statistics based hypothetical distribution. Experimental results show that our proposed defense strategies are both efficient and effective against various backdoor attacks. ",,https://openreview.net/forum?id=wwW-1k1ljIg,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/29586cb449c90e249f1f09a0a4ee245a.png?t=1666525254.6826682,https://neurips.cc/media/neurips-2022/Slides/54164.pdf,Poisoning,Defence,"['backdoor attack', 'convolutional neural networks (CNN)', 'poisoned samples', 'backdoor neurons', 'pre-activation distributions', 'differential entropy', 'Kullback-Leibler divergence', 'benign sample distribution', 'poisoned statistics']",None,,,,,,
778,https://neurips.cc/virtual/2022/poster/53496,Kernel Memory Networks: A Unifying Framework for Memory Modeling,Poster,NeurIPS,2022,"We consider the problem of training a neural network to store a set of patterns with maximal noise robustness. A solution, in terms of optimal weights and state update rules, is derived by training each individual neuron to perform either kernel classification or interpolation with a minimum weight norm. By applying this method to feed-forward and recurrent networks, we derive optimal models, termed kernel memory networks, that include, as special cases, many of the hetero- and auto-associative memory models that have been proposed over the past years, such as modern Hopfield networks and Kanerva's sparse distributed memory. We modify Kanerva's model and demonstrate a simple way to design a kernel memory network that can store an exponential number of continuous-valued patterns with a finite basin of attraction. The framework of kernel memory networks offers a simple and intuitive way to understand the storage capacity of previous memory models, and allows for new biological interpretations in terms of dendritic non-linearities and synaptic cross-talk.",,https://openreview.net/forum?id=px87A_nzK-T,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53496.png?t=1669216813.0195637,https://neurips.cc/virtual/2022/poster/53496,Not relevant,Other aspects,"['Kernel Memory Networks', 'Memory Modeling', 'Noise robustness', 'Neuron classification', 'Interpolation', 'Feed-forward networks', 'Recurrent networks', 'Hopfield networks', ""Kanerva's model"", 'Storage capacity', 'Dendritic non-linearities', 'Synaptic cross-talk']",,,,,,,
783,https://neurips.cc/virtual/2022/poster/53459,Measuring Data Reconstruction Defenses in Collaborative Inference Systems,Poster,NeurIPS,2022,"The collaborative inference systems are designed to speed up the prediction processes in edge-cloud scenarios, where the local devices and the cloud system work together to run a complex deep-learning model. However, those edge-cloud collaborative inference systems are vulnerable to emerging Reconstruction Attacks, where malicious cloud service providers are able to recover the edge-side users’ private data. To defend against such attacks, several countermeasures have been recently introduced. Unfortunately, little is known about the robustness of those defense countermeasures. In this paper, we take the first step towards measuring the robustness of those state-of-the-art defenses with respect to reconstruction attacks. Specifically, we show the latent privacy features still retain in the obfuscated representations. Motivated by such an observation, we propose a novel technology called Sensitive Feature Distillation (SFD) to restore sensitive information from the protected feature representations. Our experiments show that SFD can break through defense mechanisms in model partitioning scenarios, demonstrating the inadequacy of existing defense mechanisms as a privacy-preserving technique against reconstruction attacks. We hope our findings inspire further work in improving the robustness of defense mechanisms against reconstruction attacks for collaborative inference systems.",,https://openreview.net/forum?id=UMdY6-r7yRu,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53459.png?t=1669446618.2812228,https://neurips.cc/virtual/2022/poster/53459,Data Extraction,Defence,"['collaborative inference systems', 'reconstruction attacks', 'defense countermeasures', 'latent privacy features', 'obfuscated representations', 'Sensitive Feature Distillation (SFD)']",,Attack,Data Extraction,,,,Correct
790,https://neurips.cc/virtual/2022/poster/53348,Using Mixup as a Regularizer Can Surprisingly Improve Accuracy & Out-of-Distribution Robustness,Poster,NeurIPS,2022,"We show that the effectiveness of the well celebrated Mixup can be further improved if instead of using it as the sole learning objective, it is utilized as an additional regularizer to the standard cross-entropy loss. This simple change not only improves accuracy but also significantly improves the quality of the predictive uncertainty estimation of Mixup in most cases under various forms of covariate shifts and out-of-distribution detection experiments. In fact, we observe that Mixup otherwise yields much degraded performance on detecting out-of-distribution samples possibly, as we show empirically, due to its tendency to learn models exhibiting high-entropy throughout; making it difficult to differentiate in-distribution samples from out-of-distribution ones. To show the efficacy of our approach (RegMixup), we provide thorough analyses and experiments on vision datasets (ImageNet & CIFAR-10/100) and compare it with a suite of recent approaches for reliable uncertainty estimation. ",,https://openreview.net/forum?id=5j6fWcPccO,https://neurips.cc/virtual/2022/poster/53348,https://neurips.cc/virtual/2022/poster/53348,Robustness,Defence,"['Mixup', 'Regularizer', 'Accuracy', 'Out-of-distribution robustness', 'Predictive uncertainty estimation', 'Covariate shifts', 'OOD detection', 'High-entropy', 'In-distribution samples', 'Out-of-distribution samples', 'RegMixup', 'Vision datasets', 'ImageNet', 'CIFAR-10/100', 'Uncertainty estimation']",['Vision'],,,,,,
793,https://neurips.cc/virtual/2022/poster/54042,Lifelong Neural Predictive Coding: Learning Cumulatively Online without Forgetting,Poster,NeurIPS,2022,"In lifelong learning systems based on artificial neural networks, one of the biggest obstacles is the inability to retain old knowledge as new information is encountered. This phenomenon is known as catastrophic forgetting. In this paper, we propose a new kind of connectionist architecture, the Sequential Neural Coding Network, that is robust to forgetting when learning from streams of data points and, unlike networks of today, does not learn via the popular back-propagation of errors. Grounded in the neurocognitive theory of predictive coding, our model adapts its synapses in a biologically-plausible fashion while another neural system learns to direct and control this cortex-like structure, mimicking some of the task-executive control functionality of the basal ganglia. In our experiments, we demonstrate that our self-organizing system experiences significantly less forgetting compared to standard neural models, outperforming a swath of previously proposed methods, including rehearsal/data buffer-based methods, on both standard (SplitMNIST, Split Fashion MNIST, etc.) and custom benchmarks even though it is trained in a stream-like fashion. Our work offers evidence that emulating mechanisms in real neuronal systems, e.g., local learning, lateral competition, can yield new directions and possibilities for tackling the grand challenge of lifelong machine learning.",,https://openreview.net/forum?id=ccYOWWNa5v2,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54042.png?t=1669779667.2667553,https://neurips.cc/virtual/2022/poster/54042,Robustness,Defence,"['lifelong learning', 'neural networks', 'predictive coding', 'catastrophic forgetting', 'Sequential Neural Coding Network', 'back-propagation', 'neurocognitive theory', 'biologically-plausible', 'basal ganglia', 'self-organizing system', 'SplitMNIST', 'Split Fashion MNIST']",,,,,,,
818,https://neurips.cc/virtual/2022/poster/53750,A Statistical Online Inference Approach in Averaged Stochastic Approximation,Poster,NeurIPS,2022,"In this paper we propose a general framework to perform statistical online inference in a class of constant step size stochastic approximation (SA) problems, including the well-known stochastic gradient descent (SGD) and Q-learning. Regarding a constant step size SA procedure as a time-homogeneous Markov chain, we establish a functional central limit theorem (FCLT) for it under weaker conditions, and then construct confidence intervals for parameters via random scaling. To leverage the FCLT results in the Markov chain setting, an alternative condition that is more applicable for SA problems is established. We conduct experiments to perform inference with both random scaling and other traditional inference methods, and finds that the former has a more accurate and robust performance.",,https://openreview.net/forum?id=boItpVtQ14K,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/0e080857e96278e6dba76ac029faf291.png?t=1667632950.9378495,https://neurips.cc/virtual/2022/poster/53750,Not relevant,Defence,"['Statistical online inference', 'Averaged stochastic approximation', 'Stochastic gradient descent', 'Q-learning', 'Markov chain', 'Functional central limit theorem', 'Confidence intervals', 'Random scaling', 'Inference methods']",,,,,,,
821,https://neurips.cc/virtual/2022/poster/53741,Merging Models with Fisher-Weighted Averaging,Poster,NeurIPS,2022,"Averaging the parameters of models that have the same architecture and initialization can provide a means of combining their respective capabilities. In this paper, we take the perspective that this ""merging"" operation can be seen as choosing parameters that approximately maximize the joint likelihood of the posteriors of the models' parameters. Computing a simple average of the models' parameters therefore corresponds to making an isotropic Gaussian approximation to their posteriors. We develop an alternative merging procedure based on the Laplace approximation where we approximate each model's posterior as a Gaussian distribution whose precision matrix corresponds to its Fisher information. We first show that our ""Fisher merging"" technique provides a performance boost in settings where simple parameter averaging is currently used -- specifically, robust fine-tuning and model ensembling. Then, we compare merging to standard gradient-based transfer learning and demonstrate that merging enables a fundamentally different method for transferring capabilities across models. Specifically, we show that Fisher merging is competitive with gradient-based transfer learning approaches (while being significantly cheaper) in intermediate-task training and domain-adaptive pre-training. We also show that our merging procedure makes it possible to combine models in previously unexplored ways. We release our code to facilitate future research into methods for merging models.",,https://openreview.net/forum?id=LSKlp_aceOC,https://neurips.cc/virtual/2022/poster/53741,https://neurips.cc/virtual/2022/poster/53741,Not relevant,Other aspects,"['Averaging', 'merging', 'models', 'Fisher-Weighted', 'likelihood', 'posteriors', 'Laplace approximation', 'Fisher information', 'fine-tuning', 'model ensembling', 'transfer learning', 'domain-adaptive pre-training']",[],,,,,,
825,https://neurips.cc/virtual/2022/poster/53102,Controlled Sparsity via Constrained Optimization or: How I Learned to Stop Tuning Penalties and Love Constraints,Poster,NeurIPS,2022,"The performance of trained neural networks is robust to harsh levels of pruning. Coupled with the ever-growing size of deep learning models, this observation has motivated extensive research on learning sparse models. In this work, we focus on the task of controlling the level of sparsity when performing sparse learning. Existing methods based on sparsity-inducing penalties involve expensive trial-and-error tuning of the penalty factor, thus lacking direct control of the resulting model sparsity. In response, we adopt a constrained formulation: using the gate mechanism proposed by Louizos et al. (2018), we formulate a constrained optimization problem where sparsification is guided by the training objective and the desired sparsity target in an end-to-end fashion. Experiments on CIFAR-{10, 100}, TinyImageNet, and ImageNet using WideResNet and ResNet{18, 50} models validate the effectiveness of our proposal and demonstrate that we can reliably achieve pre-determined sparsity targets without compromising on predictive performance.",,https://openreview.net/forum?id=XUvSYc6TqDF,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53102.png?t=1669223021.1363986,https://neurips.cc/virtual/2022/poster/53102,Not relevant,Other aspects,"['controlled sparsity', 'constrained optimization', 'sparse learning', 'sparsity-inducing penalties', 'gate mechanism', 'end-to-end fashion']",[],,,,,,
826,https://neurips.cc/virtual/2022/poster/53096,Drawing out of Distribution with Neuro-Symbolic Generative Models,Poster,NeurIPS,2022,"Learning general-purpose representations from perceptual inputs is a hallmark of human intelligence. For example, people can write out numbers or characters, or even draw doodles, by characterizing these tasks as different instantiations of the same generic underlying process---compositional arrangements of different forms of pen strokes. Crucially, learning to do one task, say writing, implies reasonable competence at another, say drawing, on account of this shared process. We present Drawing out of Distribution (DooD), a neuro-symbolic generative model of stroke-based drawing that can learn such general-purpose representations. In contrast to prior work, DooD operates directly on images, requires no supervision or expensive test-time inference, and performs unsupervised amortized inference with a symbolic stroke model that better enables both interpretability and generalization. We evaluate DooD on its ability to generalize across both data and tasks. We first perform zero-shot transfer from one dataset (e.g. MNIST) to another (e.g. Quickdraw), across five different datasets, and show that DooD clearly outperforms different baselines. An analysis of the learnt representations further highlights the benefits of adopting a symbolic stroke model. We then adopt a subset of the Omniglot challenge tasks, and evaluate its ability to generate new exemplars (both unconditionally and conditionally), and perform one-shot classification, showing that DooD matches the state of the art. Taken together, we demonstrate that DooD does indeed capture general-purpose representations across both data and task, and takes a further step towards building general and robust concept-learning systems.",,https://openreview.net/forum?id=tIZtD2kZ6zx,https://neurips.cc/virtual/2022/poster/53096,https://neurips.cc/virtual/2022/poster/53096,Not relevant,Other aspects,"['Drawing out of Distribution', 'Neuro-Symbolic Generative Models', 'Perceptual inputs', 'Human intelligence', 'Stroke-based drawing', 'Unsupervised amortized inference', 'Symbolic stroke model', 'Interpretability', 'Generalization', 'Zero-shot transfer', 'MNIST', 'Quickdraw', 'Omniglot', 'One-shot classification', 'Concept-learning systems']",,,,,,,
837,https://neurips.cc/virtual/2022/poster/53669,Training with More Confidence: Mitigating Injected and Natural Backdoors During Training,Poster,NeurIPS,2022,"The backdoor or Trojan attack is a severe threat to deep neural networks (DNNs). Researchers find that DNNs trained on benign data and settings can also learn backdoor behaviors, which is known as the natural backdoor. Existing works on anti-backdoor learning are based on weak observations that the backdoor and benign behaviors can differentiate during training. An adaptive attack with slow poisoning can bypass such defenses. Moreover, these methods cannot defend natural backdoors. We found the fundamental differences between backdoor-related neurons and benign neurons: backdoor-related neurons form a hyperplane as the classification surface across input domains of all affected labels. By further analyzing the training process and model architectures, we found that piece-wise linear functions cause this hyperplane surface. In this paper, we design a novel training method that forces the training to avoid generating such hyperplanes and thus remove the injected backdoors. Our extensive experiments on five datasets against five state-of-the-art attacks and also benign training show that our method can outperform existing state-of-the-art defenses. On average, the ASR (attack success rate) of the models trained with NONE is 54.83 times lower than undefended models under standard poisoning backdoor attack and 1.75 times lower under the natural backdoor attack. Our code is available at https://github.com/RU-System-Software-and-Security/NONE.",,https://openreview.net/forum?id=yNPsd3oG_s,https://neurips.cc/virtual/2022/poster/53669,https://neurips.cc/virtual/2022/poster/53669,Other attack,Defence,"['backdoor attack', 'deep neural networks', 'anti-backdoor learning', 'poisoning attack', 'adaptive attack', 'piece-wise linear functions', 'training method', 'attack success rate', 'poisoning backdoor attack', 'natural backdoor attack']",,Defence,Poisoning,,,Correct,
839,https://neurips.cc/virtual/2022/poster/53647,On the Complexity of Adversarial Decision Making,Poster,NeurIPS,2022,"A central problem in online learning and decision making---from bandits to reinforcement learning---is to understand what modeling assumptions lead to sample-efficient learning guarantees. We consider a general adversarial decision making framework that encompasses (structured) bandit problems with adversarial rewards and reinforcement learning problems with adversarial dynamics. Our main result is to show---via new upper and lower bounds---that the Decision-Estimation Coefficient, a complexity measure introduced by Foster et al. in the stochastic counterpart to our setting, is necessary and sufficient to obtain low regret for adversarial decision making. However, compared to the stochastic setting, one must apply the Decision-Estimation Coefficient to the convex hull of the class of models (or, hypotheses) under consideration. This establishes that the price of accommodating adversarial rewards or dynamics is governed by the behavior of the model class under convexification, and recovers a number of existing results --both positive and negative. En route to obtaining these guarantees, we provide new structural results that connect the Decision-Estimation Coefficient to variants of other well-known complexity measures, including the Information Ratio of Russo and Van Roy and the Exploration-by-Optimization objective of Lattimore and György.",,https://openreview.net/forum?id=pgBpQYss2ba,https://neurips.cc/virtual/2022/poster/53647,https://neurips.cc/virtual/2022/poster/53647,Not relevant,Defence,"['Adversarial Decision Making', 'Online Learning', 'Decision Making', 'Bandits', 'Reinforcement Learning', 'Modeling Assumptions', 'Sample-efficient Learning', 'Decision-Estimation Coefficient', 'Regret', 'Convex Hull', 'Model Class', 'Information Ratio', 'Exploration-by-Optimization']",,,,,,,
840,https://neurips.cc/virtual/2022/poster/53642,Scalable Distributional Robustness in a Class of Non-Convex Optimization with Guarantees,Poster,NeurIPS,2022,"Distributionally robust optimization (DRO) has shown a lot of promise in providing robustness in learning as well as sample-based optimization problems. We endeavor to provide DRO solutions for a class of sum of fractionals, non-convex optimization which is used for decision making in prominent areas such as facility location and security games. In contrast to previous work, we find it more tractable to optimize the equivalent variance regularized form of DRO rather than the minimax form. We transform the variance regularized form to a mixed-integer second-order cone program (MISOCP), which, while guaranteeing global optimality, does not scale enough to solve problems with real-world datasets. We further propose two abstraction approaches based on clustering and stratified sampling to increase scalability, which we then use for real-world datasets. Importantly, we provide global optimality guarantees for our approach and show experimentally that our solution quality is better than the locally optimal ones achieved by state-of-the-art gradient-based methods. We experimentally compare our different approaches and baselines and reveal nuanced properties of a DRO solution.",,https://openreview.net/forum?id=62GLWUoOLb5,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53642.png?t=1669243022.2677445,https://neurips.cc/virtual/2022/poster/53642,Robustness,Defence,"['Distributionally robust optimization', 'Non-convex optimization', 'Decision making', 'Facility location', 'Security games', 'Variance regularized form', 'Mixed-integer second-order cone program', 'Clustering', 'Stratified sampling', 'Global optimality guarantees']",,,,,,,
845,https://neurips.cc/virtual/2022/poster/53625,Uncertainty Estimation for Multi-view Data: The Power of Seeing the Whole Picture,Poster,NeurIPS,2022,"Uncertainty estimation is essential to make neural networks trustworthy in real-world applications. Extensive research efforts have been made to quantify and reduce predictive uncertainty. However, most existing works are designed for unimodal data, whereas multi-view uncertainty estimation has not been sufficiently investigated. Therefore, we propose a new multi-view classification framework for better uncertainty estimation and out-of-domain sample detection, where we associate each view with an uncertainty-aware classifier and combine the predictions of all the views in a principled way. The experimental results with real-world datasets demonstrate that our proposed approach is an accurate, reliable, and well-calibrated classifier, which predominantly outperforms the multi-view baselines tested in terms of expected calibration error, robustness to noise, and accuracy for the in-domain sample classification and the out-of-domain sample detection tasks",,https://openreview.net/forum?id=9WJU4Lu2KTX,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/b1adda14824f50ef24ff1c05bb66faf3.png?t=1666912026.761161,https://neurips.cc/virtual/2022/poster/53625,Robustness,Defence,"['Uncertainty estimation', 'multi-view data', 'classification framework', 'out-of-domain sample detection', 'classifier', 'expected calibration error', 'robustness to noise', 'accuracy']",['real-world datasets'],,,,,,
853,https://neurips.cc/virtual/2022/poster/53538,Robust Feature-Level Adversaries are Interpretability Tools,Poster,NeurIPS,2022,"The literature on adversarial attacks in computer vision typically focuses on pixel-level perturbations. These tend to be very difficult to interpret. Recent work that manipulates the latent representations of image generators to create ""feature-level"" adversarial perturbations gives us an opportunity to explore perceptible, interpretable adversarial attacks. We make three contributions. First, we observe that feature-level attacks provide useful classes of inputs for studying representations in models. Second, we show that these adversaries are versatile and highly robust. We demonstrate that they can be used to produce targeted, universal, disguised, physically-realizable, and black-box attacks at the ImageNet scale. Third, we show how these adversarial images can be used as a practical interpretability tool for identifying bugs in networks. We use these adversaries to make predictions about spurious associations between features and classes which we then test by designing ""copy/paste"" attacks in which one natural image is pasted into another to cause a targeted misclassification. Our results suggest that feature-level attacks are a promising approach for rigorous interpretability research. They support the design of tools to better understand what a model has learned and diagnose brittle feature associations.",,https://openreview.net/forum?id=lQ--doSB2o,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53538.png?t=1669239410.6768074,https://neurips.cc/virtual/2022/poster/53538,Evasion,Attack,"['Adversarial attacks', 'Computer vision', 'Pixel-level perturbations', 'Latent representations', 'Image generators', 'Interpretable adversarial attacks', 'Targeted', 'Universal', 'Disguised', 'Physically-realizable', 'Black-box attacks', 'ImageNet scale', 'Practical interpretability tool', 'Spurious associations', 'Features', 'Classes', 'Copy/paste attacks', 'Targeted misclassification', 'Rigorous interpretability research', 'Model understanding', 'Diagnose brittle feature associations']",,Attack,Evasion,Other aspects,Robustness,,
857,https://neurips.cc/virtual/2022/poster/53325,MCL-GAN: Generative Adversarial Networks with Multiple Specialized Discriminators,Poster,NeurIPS,2022,"We propose a framework of generative adversarial networks with multiple discriminators, which collaborate to represent a real dataset more effectively. Our approach facilitates learning a generator consistent with the underlying data distribution based on real images and thus mitigates the chronic mode collapse problem. From the inspiration of multiple choice learning, we guide each discriminator to have expertise in a subset of the entire data and allow the generator to find reasonable correspondences between the latent and real data spaces automatically without extra supervision for training examples. Despite the use of multiple discriminators, the backbone networks are shared across the discriminators and the increase in training cost is marginal. We demonstrate the effectiveness of our algorithm using multiple evaluation metrics in the standard datasets for diverse tasks.",,https://openreview.net/forum?id=CQaqJDWUGJ,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53325.png?t=1669053436.1750612,https://neurips.cc/virtual/2022/poster/53325,Not relevant,Attack,"['Generative Adversarial Networks', 'Multiple Specialized Discriminators', 'Mode collapse problem', 'Multiple choice learning', 'Latent data space', 'Real data space']",,,,,,,
858,https://neurips.cc/virtual/2022/poster/52834,Nonstationary Dual Averaging and Online Fair Allocation,Poster,NeurIPS,2022,"We consider the problem of fairly allocating sequentially arriving items to a set of individuals. For this problem, the recently-introduced PACE algorithm leverages the dual averaging algorithm to approximate competitive equilibria and thus generate online fair allocations. PACE is simple, distributed, and parameter-free, making it appealing for practical use in large-scale systems. However, current performance guarantees for PACE require i.i.d. item arrivals. Since real-world data is rarely i.i.d., or even stationary, we study the performance of PACE on nonstationary data. We start by developing new convergence results for the general dual averaging algorithm under three nonstationary input models: adversarially-corrupted stochastic input, ergodic input, and block-independent (including periodic) input. Our results show convergence of dual averaging up to errors caused by nonstationarity of the data, and recover the classical bounds when the input data is i.i.d. Using these results, we show that the PACE algorithm for online fair allocation simultaneously achieves ``best of many worlds'' guarantees against any of these nonstationary input models as well as against i.i.d. input. Finally, numerical experiments show strong empirical performance of PACE against nonstationary inputs. ",,https://openreview.net/forum?id=8bk68fodvD5,https://neurips.cc/virtual/2022/poster/52834,https://neurips.cc/virtual/2022/poster/52834,Not relevant,Defence,"['nonstationary', 'dual averaging', 'online fair allocation', 'PACE algorithm', 'competitive equilibrium', 'large-scale systems', 'adversarially-corrupted stochastic input', 'ergodic input', 'block-independent input', 'periodic input', 'fair allocation']",,Not relevant,Not relevant,,,,
861,https://neurips.cc/virtual/2022/poster/52814,ReCo: Retrieve and Co-segment for Zero-shot Transfer,Poster,NeurIPS,2022,"Semantic segmentation has a broad range of applications, but its real-world impact has been significantly limited by the prohibitive annotation costs necessary to enable deployment. Segmentation methods that forgo supervision can side-step these costs, but exhibit the inconvenient requirement to provide labelled examples from the target distribution to assign concept names to predictions. An alternative line of work in language-image pre-training has recently demonstrated the potential to produce models that can both assign names across large vocabularies of concepts and enable zero-shot transfer for classification, but do not demonstrate commensurate segmentation abilities.We leverage the retrieval abilities of one such language-image pre-trained model, CLIP, to dynamically curate training sets from unlabelled images for arbitrary collections of concept names, and leverage the robust correspondences offered by modern image representations to co-segment entities among the resulting collections. The synthetic segment collections are then employed to construct a segmentation model (without requiring pixel labels) whose knowledge of concepts is inherited from the scalable pre-training process of CLIP. We demonstrate that our approach, termed Retrieve and Co-segment (ReCo) performs favourably to conventional unsupervised segmentation approaches while inheriting the convenience of nameable predictions and zero-shot transfer. We also demonstrate ReCo’s ability to generate specialist segmenters for extremely rare objects.",,https://openreview.net/forum?id=8ViFz-5Mnnv,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/52814.png?t=1669334700.4201667,https://neurips.cc/virtual/2022/poster/52814,Not relevant,Defence,"Semantic segmentation, annotation costs, unsupervised segmentation, language-image pre-training, CLIP, image representations, zero-shot transfer, concept names, pixel labels, specialist segmenters, extremely rare objects",,,,,,,
862,https://neurips.cc/virtual/2022/poster/52813,Leveraging the Hints: Adaptive Bidding in Repeated First-Price Auctions,Poster,NeurIPS,2022,"With the advent and increasing consolidation of e-commerce, digital advertising has very recently replaced traditional advertising as the main marketing force in the economy. In the past four years, a particularly important development in the digital advertising industry is the shift from second-price auctions to first-price auctions for online display ads. This shift immediately motivated the intellectually challenging question of how to bid in first-price auctions, because unlike in second-price auctions, bidding one's private value truthfully is no longer optimal. Following a series of recent works in this area, we consider a differentiated setup: we do not make any assumption about other bidders' maximum bid (i.e. it can be adversarial over time), and instead assume that we have access to a hint that serves as a prediction of other bidders' maximum bid, where the prediction is learned through some blackbox machine learning model. We consider two types of hints: one where a single point-prediction is available, and the other where a hint interval (representing a type of confidence region into which others' maximum bid falls) is available. We establish minimax optimal regret bounds for both cases and highlight the quantitatively different behavior between the two settings. We also provide improved regret bounds when the others' maximum bid exhibits the further structure of sparsity. Finally, we complement the theoretical results with demonstrations using real bidding data.",,https://openreview.net/forum?id=hjqTeP05OMB,https://neurips.cc/virtual/2022/poster/52813,https://neurips.cc/virtual/2022/poster/52813,Not relevant,Defence,"['repeated first-price auctions', 'digital advertising', 'bid prediction', 'machine learning', 'minimax regret bounds', 'sparsity']",None,,,,,,
863,https://neurips.cc/virtual/2022/poster/52808,"A Closer Look at Learned Optimization: Stability, Robustness, and Inductive Biases",Poster,NeurIPS,2022,"Learned optimizers---neural networks that are trained to act as optimizers---have the potential to dramatically accelerate training of machine learning models. However, even when meta-trained across thousands of tasks at huge computational expense, blackbox learned optimizers often struggle with stability and generalization when applied to tasks unlike those in their meta-training set. In this paper, we use tools from dynamical systems to investigate the inductive biases and stability properties of optimization algorithms, and apply the resulting insights to designing inductive biases for blackbox optimizers. Our investigation begins with a noisy quadratic model, where we characterize conditions in which optimization is stable, in terms of eigenvalues of the training dynamics. We then introduce simple modifications to a learned optimizer's architecture and meta-training procedure which lead to improved stability, and improve the optimizer's inductive bias. We apply the resulting learned optimizer to a variety of neural network training tasks, where it outperforms the current state of the art learned optimizer---at matched optimizer computational overhead---with regard to optimization performance and meta-training speed, and is capable of generalization to tasks far different from those it was meta-trained on. ",,https://openreview.net/forum?id=cxZEBQFDoFK,https://neurips.cc/virtual/2022/poster/52808,https://neurips.cc/virtual/2022/poster/52808,Robustness,Defence,"['Learned optimizers', 'neural networks', 'machine learning models', 'meta-training', 'dynamical systems', 'optimization algorithms', 'inductive biases', 'blackbox optimizers', 'noisy quadratic model', 'eigenvalues', 'training dynamics', 'architecture', 'meta-training procedure', 'optimization performance', 'generalization']",,,,,,,
869,https://neurips.cc/virtual/2022/poster/53242,Beyond IID: data-driven decision-making in heterogeneous environments,Poster,NeurIPS,2022,"In this work, we study data-driven decision-making and depart from the classical identically and independently distributed (i.i.d.) assumption.  We present a new framework in which  historical samples   are generated from unknown and different distributions, which we dub  \textit{heterogeneous environments}.  These distributions are assumed to lie in a heterogeneity ball with known radius and centered around the (also) unknown future (out-of-sample) distribution on which the performance of a decision will be evaluated. We quantify the asymptotic worst-case regret that is achievable by central data-driven policies such as Sample Average Approximation, but also by rate-optimal ones,   as a function of the radius of the heterogeneity ball. Our work shows that the type of achievable performance varies considerably across different combinations of problem classes and notions of heterogeneity. We demonstrate the versatility of our framework by comparing achievable guarantees for the heterogeneous version of widely studied  data-driven problems such as  pricing, ski-rental, and newsvendor. En route, we establish a new connection between data-driven decision-making and distributionally robust optimization.",,https://openreview.net/forum?id=NI6hB70ajO7,https://neurips.cc/virtual/2022/poster/53242,https://neurips.cc/virtual/2022/poster/53242,Not relevant,Other aspects,"['heterogeneous environments', 'data-driven decision-making', 'worst-case regret', 'Sample Average Approximation', 'rate-optimal', 'heterogeneity ball', 'distributionally robust optimization']",,,,,,,
876,https://neurips.cc/virtual/2022/poster/53204,RISE: Robust Individualized Decision Learning with Sensitive Variables,Poster,NeurIPS,2022,"This paper introduces RISE, a robust individualized decision learning framework with sensitive variables, where sensitive variables are collectible data and important to the intervention decision, but their inclusion in decision making is prohibited due to reasons such as delayed availability or fairness concerns. A naive baseline is to ignore these sensitive variables in learning decision rules, leading to significant uncertainty and bias. To address this, we propose a decision learning framework to incorporate sensitive variables during offline training but not include them in the input of the learned decision rule during model deployment. Specifically, from a causal perspective, the proposed framework intends to improve the worst-case outcomes of individuals caused by sensitive variables that are unavailable at the time of decision. Unlike most existing literature that uses mean-optimal objectives, we propose a robust learning framework by finding a newly defined quantile- or infimum-optimal decision rule. The reliable performance of the proposed method is demonstrated through synthetic experiments and three real-world applications. ",,https://openreview.net/forum?id=-IHPcl1ZhF5,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53204.png?t=1669174857.9160004,https://neurips.cc/virtual/2022/poster/53204,Robustness,Defence,"['Robust Individualized Decision Learning', 'Sensitive Variables', 'Decision Learning Framework', 'Causal perspective', 'Quantile-optimal decision rule', 'Infimum-optimal decision rule']",['Real-world applications'],,,,,,
884,https://neurips.cc/virtual/2022/poster/53072,Maximizing Revenue under Market Shrinkage and Market Uncertainty,Poster,NeurIPS,2022,"A shrinking market is a ubiquitous challenge faced by various industries. In this paper we formulate the first formal model of shrinking markets in multi-item settings, and study how mechanism design and machine learning can help preserve revenue in an uncertain, shrinking market. Via a sample-based learning mechanism, we prove the first guarantees on how much revenue can be preserved by truthful multi-item, multi-bidder auctions (for limited supply) when only a random unknown fraction of the population participates in the market. We first present a general reduction that converts any sufficiently rich auction class into a randomized auction robust to market shrinkage. Our main technique is a novel combinatorial construction called a winner diagram that concisely represents all possible executions of an auction on an uncertain set of bidders. Via a probabilistic analysis of winner diagrams, we derive a general possibility result: a sufficiently rich class of auctions always contains an auction that is robust to market shrinkage and market uncertainty. Our result has applications to important practically-constrained settings such as auctions with a limited number of winners. We then show how to efficiently learn an auction that is robust to market shrinkage by leveraging practically-efficient routines for solving the winner determination problem.",,https://openreview.net/forum?id=Ry9iNlpUy1-,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53072.png?t=1669147797.691364,https://neurips.cc/media/neurips-2022/Slides/53072_yScmg8K.pdf,Not relevant,Other aspects,"['mechanism design', 'machine learning', 'multi-item settings', 'revenue preservation', 'truthful multi-item', 'multi-bidder auctions', 'limited supply', 'random unknown fraction', 'population participation', 'market shrinkage', 'market uncertainty', 'combinatorial construction', 'winner diagram', 'probabilistic analysis', 'possibility result', 'limited number of winners', 'efficiently learn an auction']",['Industries'],,,,,,
887,https://neurips.cc/virtual/2022/poster/53579,Differentially Private Learning Needs Hidden State (Or Much Faster Convergence),Poster,NeurIPS,2022,"Prior work on differential privacy analysis of randomized SGD algorithms relies on composition theorems, where the implicit (unrealistic) assumption is that the internal state of the iterative algorithm is revealed to the adversary. As a result, the R\'enyi DP bounds derived by such composition-based analyses linearly grow with the number of training epochs. When the internal state of the algorithm is hidden, we prove a converging privacy bound for noisy stochastic gradient descent (on strongly convex smooth loss functions). We show how to take advantage of privacy amplification by sub-sampling and randomized post-processing, and prove the dynamics of privacy bound for ",,https://openreview.net/forum?id=ipAz7H8pPnI,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53579.png?t=1669392238.0785866,https://neurips.cc/virtual/2022/poster/53579,Other attack,Defence,"['Differential privacy', 'Randomized SGD', 'Composition theorems', 'Renyi DP bounds', 'Noisy stochastic gradient descent', 'Privacy amplification', 'Sub-sampling', 'Randomized post-processing']",,,,,,,
889,https://neurips.cc/virtual/2022/poster/54762,Training Spiking Neural Networks with Local Tandem Learning,Poster,NeurIPS,2022,"Spiking neural networks (SNNs) are shown to be more biologically plausible and energy efficient over their predecessors. However, there is a lack of an efficient and generalized training method for deep SNNs, especially for deployment on analog computing substrates. In this paper, we put forward a generalized learning rule, termed Local Tandem Learning (LTL). The LTL rule follows the teacher-student learning approach by mimicking the intermediate feature representations of a pre-trained ANN. By decoupling the learning of network layers and leveraging highly informative supervisor signals, we demonstrate rapid network convergence within five training epochs on the CIFAR-10 dataset while having low computational complexity. Our experimental results have also shown that the SNNs thus trained can achieve comparable accuracies to their teacher ANNs on CIFAR-10, CIFAR-100, and Tiny ImageNet datasets. Moreover, the proposed LTL rule is hardware friendly. It can be easily implemented on-chip to perform fast parameter calibration and provide robustness against the notorious device non-ideality issues. It, therefore, opens up a myriad of opportunities for training and deployment of SNN on ultra-low-power mixed-signal neuromorphic computing chips.",,https://openreview.net/forum?id=nC8VC8gVGPo,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54762.png?t=1669007524.5257347,https://neurips.cc/virtual/2022/poster/54762,Not relevant,Defence,"['Spiking neural networks', 'Local Tandem Learning', 'Teacher-student learning', 'CIFAR-10', 'CIFAR-100', 'Tiny ImageNet', 'Neuromorphic computing chips']","['Analog computing substrates', 'Ultra-low-power mixed-signal']",,,,,,
894,https://neurips.cc/virtual/2022/poster/53024,Sketching based Representations for Robust Image Classification with Provable Guarantees,Poster,NeurIPS,2022,"How do we provably represent images succinctly so that their essential latent attributes are correctly captured by the representation to as high level of detail as possible? While today's deep networks (such as CNNs)  produce image embeddings they do not have any provable properties and seem to work in mysterious non-interpretable ways. In this work we theoretically study synthetic images that are composed of a union or intersection of several mathematically specified shapes using thresholded polynomial functions (for e.g. ellipses, rectangles).  We show how to produce a succinct sketch of such an image so that the sketch “smoothly” maps to the latent-coefficients producing the different shapes in the image.  We prove several important properties  such as: easy reconstruction of the image from the sketch, similarity preservation (similar shapes produce similar sketches), being able to index sketches so that other similar images and parts of other images can be retrieved,  being able to store the sketches into a dictionary of concepts and shapes so parts of the same or different images that refer to the same shape can point to the same entry in this dictionary of common shape attributes.",,https://openreview.net/forum?id=fDWNnSiHeka,https://neurips.cc/virtual/2022/poster/53024,https://neurips.cc/virtual/2022/poster/53024,Not relevant,Not relevant,"['Sketching', 'Representations', 'Robust Image Classification', 'Provable Guarantees', 'Synthetic Images', 'Thresholded Polynomial Functions', 'Ellipses', 'Rectangles', 'Latent Attributes', 'Similarity Preservation', 'Indexing', 'Concepts', 'Shapes']",[],,,,,,
915,https://neurips.cc/virtual/2022/poster/55212,Learning Equivariant Segmentation with Instance-Unique Querying,Poster,NeurIPS,2022,"Prevalent state-of-the-art instance segmentation methods fall into a query-based scheme, in which instance masks are derived by querying the image feature using a set of instance-aware embeddings. In this work, we devise a new training framework that boosts query-based models through discriminative query embedding learning. It explores two essential properties, namely dataset-level uniqueness and transformation equivariance, of the relation between queries and instances. First, our algorithm uses the queries to retrieve the corresponding instances from the whole training dataset, instead of only searching within individual scenes. As querying instances across scenes is more challenging, the segmenters are forced to learn more discriminative queries for effective instance separation. Second, our algorithm encourages both image (instance) representations and queries to be equivariant against geometric transformations, leading to more robust, instance-query matching. On top of four famous, query-based models (i.e., CondInst, SOLOv2, SOTR, and Mask2Former), our training algorithm provides significant performance gains (e.g., +1.6 – 3.2 AP) on COCO dataset. In addition, our algorithm promotes the performance of SOLOv2 by 2.7 AP, on LVISv1 dataset.",,https://openreview.net/forum?id=q0XxMcbaZH9,https://neurips.cc/virtual/2022/poster/55212,https://neurips.cc/virtual/2022/poster/55212,Not relevant,Defence,"['Instance segmentation', 'Embedding learning', 'Discriminative query', 'Transformation equivariance', 'Robustness']",Not clear,,,,,,
916,https://neurips.cc/virtual/2022/poster/55174,Synergy-of-Experts: Collaborate to Improve Adversarial Robustness,Poster,NeurIPS,2022,"Learning adversarially robust models require invariant predictions to a small neighborhood of its natural inputs, often encountering insufficient model capacity. There is research showing that learning multiple sub-models in an ensemble could mitigate this insufficiency, further improving the generalization and the robustness. However, the ensemble's voting-based strategy excludes the possibility that the true predictions remain with the minority. Therefore, this paper further improves the ensemble through a collaboration scheme---Synergy-of-Experts (SoE). Compared with the voting-based strategy, the SoE enables the possibility of correct predictions even if there exists a single correct sub-model. In SoE, every sub-model fits its specific vulnerability area and reserves the rest of the sub-models to fit other vulnerability areas, which effectively optimizes the utilization of the model capacity. Empirical experiments verify that SoE outperforms various ensemble methods against white-box and transfer-based adversarial attacks.",,https://openreview.net/forum?id=tuC6teLFZD,https://neurips.cc/virtual/2022/poster/55174,https://neurips.cc/virtual/2022/poster/55174,Evasion,Defence,"['Adversarial robustness', 'Ensemble methods', 'Synergy-of-Experts (SoE)', 'Vulnerability area']",,Defence,Evasion,,,,
922,https://neurips.cc/virtual/2022/poster/54177,MOVE: Unsupervised Movable Object Segmentation and Detection,Poster,NeurIPS,2022,"We introduce MOVE, a novel method to segment objects without any form of supervision. MOVE exploits the fact that foreground objects can be shifted locally relative to their initial position and result in realistic (undistorted) new images. This property allows us to train a segmentation model on a dataset of images without annotation and to achieve state of the art (SotA) performance on several evaluation datasets for unsupervised salient object detection and segmentation. In unsupervised single object discovery, MOVE gives an average CorLoc improvement of 7.2% over the SotA, and in unsupervised class-agnostic object detection it gives a relative AP improvement of 53% on average. Our approach is built on top of self-supervised features (e.g. from DINO or MAE), an inpainting network (based on the Masked AutoEncoder) and adversarial training.",,https://openreview.net/forum?id=-t9FUWW5f3u,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54177.png?t=1669204557.8995426,https://neurips.cc/virtual/2022/poster/54177,Not relevant,Defence,"['Unsupervised', 'Movable object segmentation', 'Detection', 'Foreground objects', 'Salient object detection', 'Self-supervised features', 'Inpainting network', 'Adversarial training']",['None'],Not relevant,Not relevant,,,,
937,https://neurips.cc/virtual/2022/poster/55252,Towards Reasonable Budget Allocation in Untargeted Graph Structure Attacks via Gradient Debias,Poster,NeurIPS,2022,"It has become cognitive inertia to employ cross-entropy loss function in classification related tasks. In the untargeted attacks on graph structure, the gradients derived from the attack objective are the attacker's basis for evaluating a perturbation scheme. Previous methods use negative cross-entropy loss as the attack objective in attacking node-level classification models. However, the suitability of the cross-entropy function for constructing the untargeted attack objective has yet been discussed in previous works. This paper argues about the previous unreasonable attack objective from the perspective of budget allocation. We demonstrate theoretically and empirically that negative cross-entropy tends to produce more significant gradients from nodes with lower confidence in the labeled classes, even if the predicted classes of these nodes have been misled. To free up these inefficient attack budgets, we propose a simple attack model for untargeted attacks on graph structure based on a novel attack objective which generates unweighted gradients on graph structures that are not affected by the node confidence. By conducting experiments in gray-box poisoning attack scenarios, we demonstrate that a reasonable budget allocation can significantly improve the effectiveness of gradient-based edge perturbations without any extra hyper-parameter.",,https://openreview.net/forum?id=vkGk2HI8oOP,https://neurips.cc/virtual/2022/poster/55252,https://neurips.cc/virtual/2022/poster/55252,Other attack,Attack,"['Untargeted Graph Structure Attacks', 'Gradient Debias', 'Cross-entropy loss', 'node-level classification models', 'attack objective', 'budget allocation', 'gray-box poisoning attack']",,,,,,,
940,https://neurips.cc/virtual/2022/poster/56100,[Re] Reproducibility Study of “Counterfactual Generative Networks”,Poster,NeurIPS,2022,"Scope of Reproducibility In this work, we study the reproducibility of the paper Counterfactual Generative Networks (CGN) by Sauer and Geiger to verify their main claims, which state that (i) their proposed model can reliably generate high-quality counterfactual images by disentangling the shape, texture and background of the image into independent mechanisms, (ii) each independent mechanism has to be considered, and jointly optimizing all of them end-to-end is needed for high-quality images, and (iii) despite being synthetic, these counterfactual images can improve out-of-distribution performance of classifiers by making them invariant to spurious signals.
Methodology The authors of the paper provide the implementation of CGN training in PyTorch. However, they did not provide code for all experiments. Consequently, we re-implemented the code for most experiments, and run each experiment on 1080 Ti GPUs. Our reproducibility study comes at a total computational cost of 112 GPU hours.
Results We find that the main claims of the paper of (i) generating high-quality counterfactuals, (ii) utilizing appropriate inductive biases, and (iii) using them to instil invariance in classifiers, do largely hold. However, we found certain experiments that were not directly reproducible due to either inconsistency between the paper and code, or incomplete specification of the necessary hyperparameters. Further, we were unable to reproduce a subset of experiments on a large-scale dataset due to resource constraints, for which we compensate by performing those on a smaller version of the same dataset with our results supporting the general performance trend.
What was easy The original paper provides an extensive appendix with implementation details and hyperparameters. Beyond that, the original code implementation was publicly accessible and well structured. As such, getting started with the experiments proved to be quite straightforward. The implementation included configuration files, download scripts for the pretrained weights and datasets, and clear instructions on how to get started with the framework.
What was difficult Some of the experiments required severe modifications to the provided code. Additionally, some details required for the implementation are not specified in the paper or inconsistent with the specifications in the code. Lastly, in evaluating out-of-distribution robustness, getting the baseline model to work and obtaining numbers similar to those reported in the respective papers was challenging, partly due to baseline model inconsistencies within the literature.
Communication with original authors We have reached out to the original authors to get clarifications regarding the setup of some of the experiments, but unfortunately, we received a late response and only a subset of our questions was answered. ",https://rescience.github.io/bibliography/Bagad_2022.html,,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/2d887bb5b3df1bd25c1438371a3b2689.png?t=1666124103.840415,https://neurips.cc/virtual/2022/poster/56100,Not relevant,Defence,"['Reproducibility', 'Counterfactual Generative Networks', 'CGN', 'High-quality counterfactual images', 'Disentangling', 'Shape', 'Texture', 'Background', 'Independent mechanisms', 'Jointly optimizing', 'Out-of-distribution performance', 'Invariance', 'Classifiers']","['Large-scale dataset', 'Computational cost', '1080 Ti GPUs', 'PyTorch']",,,,,,
942,https://neurips.cc/virtual/2022/poster/55159,Delving into Sequential Patches for Deepfake Detection,Poster,NeurIPS,2022,"Recent advances in face forgery techniques produce nearly visually untraceable deepfake videos, which could be leveraged with malicious intentions. As a result, researchers have been devoted to deepfake detection. Previous studies have identified the importance of local low-level cues and temporal information in pursuit to generalize well across deepfake methods, however, they still suffer from robustness problem against post-processings. In this work, we  propose the Local- & Temporal-aware Transformer-based Deepfake Detection (LTTD) framework, which adopts a local-to-global learning protocol with a particular focus on the valuable temporal information within local sequences. Specifically, we propose a Local Sequence Transformer (LST), which models the temporal consistency on sequences of restricted spatial regions, where low-level information is hierarchically enhanced with shallow layers of learned 3D filters. Based on the local temporal embeddings, we then achieve the final classification in a global contrastive way. Extensive experiments on popular datasets validate that our approach effectively spots local forgery cues and achieves state-of-the-art performance.",,https://openreview.net/forum?id=osPA8Bs4MJB,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55159.png?t=1668956385.2021434,https://neurips.cc/virtual/2022/poster/55159,Evasion,Defence,"['Deepfake detection', 'Local low-level cues', 'Temporal information', 'Post-processings', 'Transformer-based', 'Local Sequence Transformer', '3D filters', 'Contrastive way']",['Video'],,,,,,
943,https://neurips.cc/virtual/2022/poster/54598,Hierarchical Graph Transformer with Adaptive Node Sampling,Poster,NeurIPS,2022,"The Transformer architecture has achieved remarkable success in a number of domains including natural language processing and computer vision. However, when it comes to graph-structured data, transformers have not achieved competitive performance, especially on large graphs. In this paper, we identify the main deficiencies of current graph transformers: (1) Existing node sampling strategies in Graph Transformers are agnostic to the graph characteristics and the training process. (2) Most sampling strategies only focus on local neighbors and neglect the long-range dependencies in the graph. We conduct experimental investigations on synthetic datasets to show that existing sampling strategies are sub-optimal. To tackle the aforementioned problems, we formulate the optimization strategies of node sampling in Graph Transformer as an adversary bandit problem, where the rewards are related to the attention weights and can vary in the training procedure. Meanwhile, we propose a hierarchical attention scheme with graph coarsening to capture the long-range interactions while reducing computational complexity. Finally, we conduct extensive experiments on real-world datasets to demonstrate the superiority of our method over existing graph transformers and popular GNNs.",,https://openreview.net/forum?id=x3JsaghSj0v,https://neurips.cc/virtual/2022/poster/54598,https://neurips.cc/virtual/2022/poster/54598,Not relevant,Defence,"['Transformer architecture', 'Graph-structured data', 'Node sampling', 'Graph transformers', 'Graph neural networks', 'Graph coarsening', 'Attention weights', 'Adversary bandit problem']",Not clear,Not relevant,Not relevant,,,,
950,https://neurips.cc/virtual/2022/poster/54080,CyCLIP: Cyclic Contrastive Language-Image Pretraining,Poster,NeurIPS,2022,"Recent advances in contrastive representation learning over paired image-text data have led to models such as CLIP that achieve state-of-the-art performance for zero-shot classification and distributional robustness. Such models typically require joint reasoning in the image and text representation spaces for downstream inference tasks. Contrary to prior beliefs, we demonstrate that the image and text representations learned via a standard contrastive objective are not interchangeable and can lead to inconsistent downstream predictions. To mitigate this issue, we formalize consistency and propose CyCLIP, a framework for contrastive representation learning that explicitly optimizes for the learned representations to be geometrically consistent in the image and text space. In particular, we show that consistent representations can be learned by explicitly symmetrizing (a) the similarity between the two mismatched image-text pairs (cross-modal consistency); and (b) the similarity between the image-image pair and the text-text pair (in-modal consistency). Empirically, we show that the improved consistency in CyCLIP translates to significant gains over CLIP, with gains ranging from 10%-24% for zero-shot classification on standard benchmarks (CIFAR-10, CIFAR-100, ImageNet1K) and 10%-27% for robustness to various natural distribution shifts.",,https://openreview.net/forum?id=I-6yh2-dkyD,https://neurips.cc/virtual/2022/poster/54080,https://neurips.cc/virtual/2022/poster/54080,Not relevant,Defence,"['CyCLIP', 'contrastive representation learning', 'paired image-text data', 'CLIP', 'zero-shot classification', 'distributional robustness', 'joint reasoning', 'image and text representation spaces', 'downstream inference tasks', 'consistency', 'geometrically consistent', 'image and text space', 'cross-modal consistency', 'in-modal consistency', 'zero-shot classification benchmarks', 'CIFAR-10', 'CIFAR-100', 'ImageNet1K', 'robustness', 'natural distribution shifts']",[],,,,,,
954,https://neurips.cc/virtual/2022/poster/54795,TREC: Transient Redundancy Elimination-based Convolution,Poster,NeurIPS,2022,"The intensive computations in convolutional neural networks (CNNs) pose challenges for resource-constrained devices; eliminating redundant computations from convolution is essential. This paper gives a principled method to detect and avoid transient redundancy, a type of redundancy existing in input data or activation maps and hence changing across inferences. By introducing a new form of convolution (TREC), this new method makes transient redundancy detection and avoidance an inherent part of the CNN architecture, and the determination of the best configurations for redundancy elimination part of CNN backward propagation. We provide a rigorous proof of the robustness and convergence of TREC-equipped CNNs. TREC removes over 96% computations and achieves 3.51x average speedups on microcontrollers with minimal (about 0.7%) accuracy loss.",,https://openreview.net/forum?id=FNzLe2-ppRO,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/4ad13f04ef4373992c9d3046200aa350.png?t=1667781209.7236667,https://neurips.cc/virtual/2022/poster/54795,Robustness,Defence,"['transient redundancy elimination', 'convolutional neural networks', 'CNN', 'TREC', 'resource-constrained devices', 'redundancy detection', 'redundancy avoidance', 'CNN architecture', 'backward propagation']",,,,,,,
958,https://neurips.cc/virtual/2022/poster/54635,CalFAT: Calibrated Federated Adversarial Training with Label Skewness,Poster,NeurIPS,2022,"Recent studies have shown that, like traditional machine learning, federated learning (FL) is also vulnerable to adversarial attacks.To improve the adversarial robustness of FL, federated adversarial training (FAT) methods have been proposed to apply adversarial training locally before global aggregation. Although these methods demonstrate promising results on independent identically distributed (IID) data, they suffer from training instability on non-IID data with label skewness, resulting in degraded natural accuracy. This tends to hinder the application of FAT in real-world applications where the label distribution across the clients is often skewed. In this paper, we study the problem of FAT under label skewness, and reveal one root cause of the training instability and natural accuracy degradation issues: skewed labels lead to non-identical class probabilities and heterogeneous local models. We then propose a Calibrated FAT (CalFAT) approach to tackle the instability issue by calibrating the logits adaptively to balance the classes. We show both theoretically and empirically that the optimization of CalFAT leads to homogeneous local models across the clients and better convergence points.",,https://openreview.net/forum?id=8N1NDRGQSQ,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/4f9c76cf97f84048c5990dd4ef842ea2.png?t=1667183565.7494476,https://neurips.cc/virtual/2022/poster/54635,Evasion,Defence,"['Federated Learning', 'Adversarial Robustness', 'Federated Adversarial Training', 'Label Skewness', 'Training Instability', 'Calibrated Federated Adversarial Training']",['Real-world applications'],,,,,,
959,https://neurips.cc/virtual/2022/poster/52899,Prompt Certified Machine Unlearning with Randomized Gradient Smoothing and Quantization,Poster,NeurIPS,2022,"The right to be forgotten calls for efficient machine unlearning techniques that make trained machine learning models forget a cohort of data. The combination of training and unlearning operations in traditional machine unlearning methods often leads to the expensive computational cost on large-scale data. This paper presents a prompt certified machine unlearning algorithm, PCMU, which executes one-time operation of simultaneous training and unlearning in advance for a series of machine unlearning requests, without the knowledge of the removed/forgotten data. First, we establish a connection between randomized smoothing for certified robustness on classification and randomized smoothing for certified machine unlearning on gradient quantization. Second, we propose a prompt certified machine unlearning model based on randomized data smoothing and gradient quantization. We theoretically derive the certified radius R regarding the data change before and after data removals and the certified budget of data removals about R. Last but not least, we present another practical framework of randomized gradient smoothing and quantization, due to the dilemma of producing high confidence certificates in the first framework. We theoretically demonstrate the certified radius R' regarding the gradient change, the correlation between two types of certified radii, and the certified budget of data removals about R'. ",,https://openreview.net/forum?id=ue4gP8ZKiWb,https://neurips.cc/virtual/2022/poster/52899,https://neurips.cc/virtual/2022/poster/52899,Other attack,Defence,"['machine unlearning', 'Randomized Gradient Smoothing', 'Quantization', 'Certified Machine Unlearning', 'Certified robustness', 'classification', 'Certified radius', 'data change', 'data removals', 'Certified budget']",['Right to be forgotten'],,,,,,
972,https://neurips.cc/virtual/2022/poster/53351,Byzantine Spectral Ranking,Poster,NeurIPS,2022,"We study the problem of rank aggregation where the goal is to obtain a global ranking by aggregating pair-wise comparisons of voters over a set of items. We consider an adversarial setting where the voters are partitioned into two sets. The first set votes in a stochastic manner according to the popular score-based Bradley-Terry-Luce (BTL) model for pairwise comparisons. The second set comprises malicious Byzantine voters trying to deteriorate the ranking. We consider a strongly-adversarial scenario where the Byzantine voters know the BTL scores, the votes of the good voters, the algorithm, and can collude with each other. We first show that the popular spectral ranking based Rank-Centrality algorithm, though optimal for the BTL model, does not perform well even when a small constant fraction of the voters are Byzantine.We introduce the Byzantine Spectral Ranking Algorithm (and a faster variant of it), which produces a reliable ranking when the number of good voters exceeds the number of Byzantine voters. We show that no algorithm can produce a satisfactory ranking with probability > 1/2 for all BTL weights when there are more Byzantine voters than good voters, showing that our algorithm works for all possible population fractions. We support our theoretical results with experimental results on synthetic and real datasets to demonstrate the failure of the Rank-Centrality algorithm under several adversarial scenarios and how the proposed Byzantine Spectral Ranking algorithm is robust in obtaining good rankings.",,https://openreview.net/forum?id=_D4cE66L9x3,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53351.png?t=1668487928.3532188,https://neurips.cc/media/neurips-2022/Slides/53351.pdf,Evasion,Defence,"['Byzantine Spectral Ranking', 'Rank-Centrality', ' Bradley-Terry-Luce (BTL) model', ' pairwise comparisons', ' malicious Byzantine voters', 'Rank-Centrality algorithm', 'Byzantine Spectral Ranking Algorithm']",,Not relevant,Not relevant,Defence,Poisoning,Arguable,
975,https://neurips.cc/virtual/2022/poster/53907,What Can the Neural Tangent Kernel Tell Us About Adversarial Robustness?,Poster,NeurIPS,2022,"The adversarial vulnerability of neural nets, and subsequent techniques to create robust models have attracted significant attention; yet we still lack a full understanding of this phenomenon. Here, we study adversarial examples of trained neural networks through analytical tools afforded by recent theory advances connecting neural networks and kernel methods, namely the Neural Tangent Kernel (NTK), following a growing body of work that leverages the NTK approximation to successfully analyze important deep learning phenomena and design algorithms for new applications. We show how NTKs allow to generate adversarial examples in a ",,https://openreview.net/forum?id=KBUgVv8z7OA,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53907.png?t=1669581784.016881,https://neurips.cc/virtual/2022/poster/53907,Evasion,Defence,"['Neural Tangent Kernel', 'Adversarial robustness', 'Neural networks', 'Kernel methods', 'Deep learning']",Not clear,Other aspects,Evasion,,,,Wrong
979,https://neurips.cc/virtual/2022/poster/54131,On Translation and Reconstruction Guarantees of the Cycle-Consistent Generative Adversarial Networks,Poster,NeurIPS,2022,"The task of unpaired image-to-image translation has witnessed a revolution with the introduction of the cycle-consistency loss to Generative Adversarial Networks (GANs). Numerous variants, with Cycle-Consistent Adversarial Network (CycleGAN) at their forefront, have shown remarkable empirical performance. The involvement of two unalike data spaces and the existence of multiple solution maps between them are some of the facets that make such architectures unique. In this study, we investigate the statistical properties of such unpaired data translator networks between distinct spaces, bearing the additional responsibility of cycle-consistency. In a density estimation setup, we derive sharp non-asymptotic bounds on the translation errors under suitably characterized models. This, in turn, points out sufficient regularity conditions that maps must obey to carry out successful translations. We further show that cycle-consistency is achieved as a consequence of the data being successfully generated in each space based on observations from the other. In a first-of-its-kind attempt, we also provide deterministic bounds on the cumulative reconstruction error. In the process, we establish tolerable upper bounds on the discrepancy responsible for ill-posedness in such networks.",,https://openreview.net/forum?id=aPgQdvSAuw,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54131.png?t=1669303324.114694,https://neurips.cc/virtual/2022/poster/54131,Not relevant,Defence,"['Unpaired image-to-image translation', 'Generative Adversarial Networks (GANs)', 'Cycle-consistency loss', 'Cycle-Consistent Adversarial Network (CycleGAN)', 'Density estimation', 'Translation errors', 'Regularization conditions', 'Cycle-consistency', 'Reconstruction error', 'Discrepancy']",,,,,,,
986,https://neurips.cc/virtual/2022/poster/53454,MAtt: A Manifold Attention Network for EEG Decoding,Poster,NeurIPS,2022,"Recognition of electroencephalographic (EEG) signals highly affect the efficiency of non-invasive brain-computer interfaces (BCIs). While recent advances of deep-learning (DL)-based EEG decoders offer improved performances, the development of geometric learning (GL) has attracted much attention for offering exceptional robustness in decoding noisy EEG data. However, there is a lack of studies on the merged use of deep neural networks (DNNs) and geometric learning for EEG decoding. We herein propose a manifold attention network (mAtt), a novel geometric deep learning (GDL)-based model, featuring a manifold attention mechanism that characterizes spatiotemporal representations of EEG data fully on a Riemannian symmetric positive definite (SPD). The evaluation of the proposed mAtt on both time-synchronous and -asyncronous EEG datasets suggests its superiority over other leading DL methods for general EEG decoding. Furthermore, analysis of model interpretation reveals the capability of mAtt in capturing informative EEG features and handling the non-stationarity of brain dynamics.",,https://openreview.net/forum?id=YG4Dg7xtETg,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53454.png?t=1669299459.8196595,https://neurips.cc/virtual/2022/poster/53454,Not relevant,Defence,"['Manifold Attention Network', 'EEG Decoding', 'Deep-learning', 'Geometric Learning', 'Brain-computer interfaces', 'Riemannian symmetric positive definite', 'SPD', 'EEG features', 'non-stationarity of brain dynamics']",None,,,,,,
988,https://neurips.cc/virtual/2022/poster/52937,"Expected Frequency Matrices of Elections: Computation, Geometry, and Preference Learning",Poster,NeurIPS,2022,"We use the ""map of elections"" approach of Szufa et al. (AAMAS 2020) to analyze several well-known vote distributions. For each of them, we give an explicit formula or an efficient algorithm for computing its frequency matrix, which captures the probability that a given candidate appears in a given position in a sampled vote. We use these matrices to draw the ""skeleton map"" of distributions, evaluate its robustness, and analyze its properties. We further develop a general and unified framework for learning the distribution of real-world preferences using the frequency matrices of established vote distributions.",,https://openreview.net/forum?id=X3RuacCx1R,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/52937.png?t=1669584126.4178581,https://neurips.cc/virtual/2022/poster/52937,Not relevant,Other aspects,"['map of elections', 'vote distributions', 'frequency matrix', 'skeleton map', 'robustness', 'preference learning']",,,,,,,
992,https://neurips.cc/virtual/2022/poster/54139,Learning Predictions for Algorithms with Predictions,Poster,NeurIPS,2022,"A burgeoning paradigm in algorithm design is the field of algorithms with predictions, in which algorithms can take advantage of a possibly-imperfect prediction of some aspect of the problem. While much work has focused on using predictions to improve competitive ratios, running times, or other performance measures, less effort has been devoted to the question of how to obtain the predictions themselves, especially in the critical online setting. We introduce a general design approach for algorithms that learn predictors: (1) identify a functional dependence of the performance measure on the prediction quality and (2) apply techniques from online learning to learn predictors, tune robustness-consistency trade-offs, and bound the sample complexity. We demonstrate the effectiveness of our approach by applying it to bipartite matching, ski-rental, page migration, and job scheduling. In several settings we improve upon multiple existing results while utilizing a much simpler analysis, while in the others we provide the first learning-theoretic guarantees.",,https://openreview.net/forum?id=5OLcPQaYTVg,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54139.png?t=1669003972.4046855,https://neurips.cc/virtual/2022/poster/54139,Not relevant,Defence,"['algorithm design', 'prediction', 'online learning', 'bipartite matching', 'ski-rental', 'page migration', 'job scheduling']",,,,,,,
995,https://neurips.cc/virtual/2022/poster/52904,Hyperparameter Sensitivity in Deep Outlier Detection: Analysis and a Scalable Hyper-Ensemble Solution,Poster,NeurIPS,2022,"Outlier detection (OD) literature exhibits numerous algorithms as it applies to diverse domains. However, given a new detection task, it is unclear how to choose an algorithm to use, nor how to set its hyperparameter(s) (HPs) in unsupervised settings. HP tuning is an ever-growing problem with the arrival of many new detectors based on deep learning, which usually come with a long list of HPs. Surprisingly, the issue of model selection in the outlier mining literature has been “the elephant in the room”; a significant factor in unlocking the utmost potential of deep methods, yet little said or done to systematically tackle the issue. In the first part of this paper, we conduct the first large-scale analysis on the HP sensitivity of deep OD methods, and through more than 35,000 trained models, quantitatively demonstrate that model selection is inevitable. Next, we design a HP-robust and scalable deep hyper-ensemble model called ROBOD that assembles models with varying HP configurations, bypassing the choice paralysis. Importantly, we introduce novel strategies to speed up ensemble training, such as parameter sharing, batch/simultaneous training, and data subsampling, that allow us to train fewer models with fewer parameters. Extensive experiments on both image and tabular datasets show that ROBOD achieves and retains robust, state-of-the-art detection performance as compared to its modern counterparts, while taking only 2-10% of the time by the naïve hyper-ensemble with independent training.",,https://openreview.net/forum?id=cUY5OkP3VR,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/52904.png?t=1668833543.5772862,https://neurips.cc/virtual/2022/poster/52904,Not relevant,Not relevant,"['Outlier detection', 'Hyperparameter sensitivity', 'Deep learning', 'Model selection', 'Hyper-ensemble', 'Parameter sharing', 'Batch/simultaneous training', 'Data subsampling']","['Image datasets', 'Tabular datasets']",,,,,,
1001,https://neurips.cc/virtual/2022/poster/53550,MEMO: Test Time Robustness via Adaptation and Augmentation,Poster,NeurIPS,2022,"While deep neural networks can attain good accuracy on in-distribution test points, many applications require robustness even in the face of unexpected perturbations in the input, changes in the domain, or other sources of distribution shift. We study the problem of test time robustification, i.e., using the test input to improve model robustness. Recent prior works have proposed methods for test time adaptation, however, they each introduce additional assumptions, such as access to multiple test points, that prevent widespread adoption. In this work, we aim to study and devise methods that make no assumptions about the model training process and are broadly applicable at test time. We propose a simple approach that can be used in any test setting where the model is probabilistic and adaptable: when presented with a test example, perform different data augmentations on the data point, and then adapt (all of) the model parameters by minimizing the entropy of the model's average, or marginal, output distribution across the augmentations. Intuitively, this objective encourages the model to make the same prediction across different augmentations, thus enforcing the invariances encoded in these augmentations, while also maintaining confidence in its predictions. In our experiments, we evaluate two baseline ResNet models, two robust ResNet-50 models, and a robust vision transformer model, and we demonstrate that this approach achieves accuracy gains of 1-8% over standard model evaluation and also generally outperforms prior augmentation and adaptation strategies. For the setting in which only one test point is available, we achieve state-of-the-art results on the ImageNet-C, ImageNet-R, and, among ResNet-50 models, ImageNet-A distribution shift benchmarks.",,https://openreview.net/forum?id=XrGEkCOREX2,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/ae95296e27d7f695f891cd26b4f37078.png?t=1666497682.7617989,https://neurips.cc/virtual/2022/poster/53550,Robustness,Defence,"['Test time robustification', 'Test time adaptation', 'Data augmentation', 'Invariances', 'Neural networks', 'Distribution shift', 'Robustness']","['ImageNet-C', 'ImageNet-R', 'ImageNet-A']",,,,,,
1002,https://neurips.cc/virtual/2022/poster/53611,DASCO: Dual-Generator Adversarial Support Constrained Offline Reinforcement Learning,Poster,NeurIPS,2022,"In offline RL, constraining the learned policy to remain close to the data is essential to prevent the policy from outputting out-of-distribution (OOD) actions with erroneously overestimated values. In principle, generative adversarial networks (GAN) can provide an elegant solution to do so, with the discriminator directly providing a probability that quantifies distributional shift. However, in practice, GAN-based offline RL methods have not outperformed alternative approaches, perhaps because the generator is trained to both fool the discriminator and maximize return - two objectives that are often at odds with each other. In this paper, we show that the issue of conflicting objectives can be resolved by training two generators: one that maximizes return, with the other capturing the ""remainder"" of the data distribution in the offline dataset, such that the mixture of the two is close to the behavior policy. We show that not only does having two generators enable an effective GAN-based offline RL method, but also approximates a support constraint, where the policy does not need to match the entire data distribution, but only the slice of the data that leads to high long term performance. We name our method DASCO, for Dual-Generator Adversarial Support Constrained Offline RL. On benchmark tasks that require learning from sub-optimal data, DASCO significantly outperforms prior methods that enforce distribution constraint.",,https://openreview.net/forum?id=jBTQGGy9qA-,https://neurips.cc/virtual/2022/poster/53611,https://neurips.cc/virtual/2022/poster/53611,Robustness,Defence,"['offline RL', 'generative adversarial networks (GAN)', 'distributional shift', 'support constraint', 'DASCO', 'Dual-Generator Adversarial Support Constrained Offline RL']",,Not relevant,Not relevant,,,Correct,Arguable
1012,https://neurips.cc/virtual/2022/poster/54961,Optimizing Relevance Maps of Vision Transformers Improves Robustness,Poster,NeurIPS,2022,"It has been observed that visual classification models often rely mostly on spurious cues such as the image background, which hurts their robustness to distribution changes.  To alleviate this shortcoming, we propose to monitor the model's relevancy signal and direct the model to base its prediction on the foreground object.This is done as a finetuning step, involving relatively few samples consisting of pairs of images and their associated foreground masks. Specifically, we encourage the model's relevancy map (i) to assign lower relevance to background regions, (ii) to consider as much information as possible from the foreground, and (iii) we encourage the decisions to have high confidence. When applied to Vision Transformer (ViT) models, a marked improvement in robustness to domain-shifts is observed. Moreover, the foreground masks can be obtained automatically, from a self-supervised variant of the ViT model itself; therefore no additional supervision is required. Our code is available at: https://github.com/hila-chefer/RobustViT.",,https://openreview.net/forum?id=upuYKQiyxa_,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54961.png?t=1669219815.2141235,https://neurips.cc/virtual/2022/poster/54961,Robustness,Defence,"['Robustness', 'Visual classification models', 'Relevancy signal', 'Foreground object', 'Foreground mask', 'Vision Transformer', 'domain-shifts']",,,,,,,
1037,https://neurips.cc/virtual/2022/poster/54787,Boosting the Transferability of Adversarial Attacks with Reverse Adversarial Perturbation,Poster,NeurIPS,2022,"Deep neural networks (DNNs) have been shown to be vulnerable to adversarial examples, which can produce erroneous predictions by injecting imperceptible perturbations. In this work, we study the transferability of adversarial examples, which is significant due to its threat to real-world applications where model architecture or parameters are usually unknown. Many existing works reveal that the adversarial examples are likely to overfit the surrogate model that they are generated from, limiting its transfer attack performance against different target models. To mitigate the overfitting of the surrogate model, we propose a novel attack method, dubbed reverse adversarial perturbation (RAP). Specifically, instead of minimizing the loss of a single adversarial point, we advocate seeking adversarial example located at a region with unified low loss value, by injecting the worst-case perturbation (the reverse adversarial perturbation) for each step of the optimization procedure. The adversarial attack with RAP is formulated as a min-max bi-level optimization problem.  By integrating RAP into the iterative process for attacks, our method can find more stable adversarial examples which are less sensitive to the changes of decision boundary, mitigating the overfitting of the surrogate model.  Comprehensive experimental comparisons demonstrate that RAP can significantly boost adversarial transferability. Furthermore, RAP can be naturally combined with many existing black-box attack techniques, to further boost the transferability. When attacking a real-world image recognition system, Google Cloud Vision API, we obtain 22% performance improvement of targeted attacks over the compared method. Our codes are available at https://github.com/SCLBD/Transfer",,https://openreview.net/forum?id=k5uFiFLWv3X,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/f7cfdde9db36af8e0d9a6d123d5c385e.png?t=1666409849.4429383,https://neurips.cc/media/neurips-2022/Slides/54787.pdf,Evasion,Attack,"['Adversarial examples', 'Transferability of adversarial examples', 'Deep neural networks', 'Reverse adversarial perturbation', 'Min-max bi-level optimization problem', 'Surrogate model', 'Overfitting', 'Image recognition system']",['Google Cloud Vision API'],,,,,,
1038,https://neurips.cc/virtual/2022/poster/53652,Skills Regularized Task Decomposition for Multi-task Offline Reinforcement Learning,Poster,NeurIPS,2022,"Reinforcement learning (RL) with diverse offline datasets can have the advantage of leveraging the relation of multiple tasks and the common skills learned across those tasks, hence allowing us to deal with real-world complex problems efficiently in a data-driven way.  In offline RL where only offline data is used and online interaction with the environment is restricted, it is yet difficult to achieve the optimal policy for multiple tasks, especially when the data quality varies for the tasks. In this paper, we present a skill-based multi-task RL technique on heterogeneous datasets that are generated by behavior policies of different quality. To learn the shareable knowledge across those datasets effectively, we employ a task decomposition method for which common skills are jointly learned and used as guidance to reformulate a task in shared and achievable subtasks. In this joint learning, we use Wasserstein Auto-Encoder (WAE) to represent both skills and tasks on the same latent space and use the quality-weighted loss as a regularization term to induce tasks to be decomposed into subtasks that are more consistent with high-quality skills than others. To improve the performance of offline RL agents learned on the latent space, we also augment datasets with imaginary trajectories relevant to high-quality skills for each task. Through experiments, we show that our multi-task offline RL approach is robust to different-quality datasets and it outperforms other state-of-the-art algorithms for several robotic manipulation tasks and drone navigation tasks.",,https://openreview.net/forum?id=uuaMrewU9Kk,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53652.png?t=1669356702.9311469,https://neurips.cc/virtual/2022/poster/53652,Not relevant,Defence,"['Reinforcement learning', 'Multi-task', 'Offline', 'Data-driven', 'Wasserstein Auto-Encoder', 'Latent space', 'Robotic manipulation', 'Drone navigation']","['Robotic manipulation', 'Drone navigation']",,,,,,
1042,https://neurips.cc/virtual/2022/poster/54620,AD-DROP: Attribution-Driven Dropout for Robust Language Model Fine-Tuning,Poster,NeurIPS,2022,"Fine-tuning large pre-trained language models on downstream tasks is apt to suffer from overfitting when limited training data is available. While dropout proves to be an effective antidote by randomly dropping a proportion of units, existing research has not examined its effect on the self-attention mechanism. In this paper, we investigate this problem through self-attention attribution and find that dropping attention positions with low attribution scores can accelerate training and increase the risk of overfitting. Motivated by this observation, we propose Attribution-Driven Dropout (AD-DROP), which randomly discards some high-attribution positions to encourage the model to make predictions by relying more on low-attribution positions to reduce overfitting. We also develop a cross-tuning strategy to alternate fine-tuning and AD-DROP to avoid dropping high-attribution positions excessively. Extensive experiments on various benchmarks show that AD-DROP yields consistent improvements over baselines. Analysis further confirms that AD-DROP serves as a strategic regularizer to prevent overfitting during fine-tuning.",,https://openreview.net/forum?id=XYDXL9_2P4,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/ab2481c9f93d0ed3033a3281d865ccb2.png?t=1666675473.9594047,https://neurips.cc/media/neurips-2022/Slides/54620_dmW6bnh.pdf,Robustness,Defence,"['Language Model Fine-Tuning', 'Dropout', 'Self-attention', 'Attribution-Driven Dropout', 'Overfitting', 'Fine-tuning', 'Regularization']",['Language Understanding'],,,,,,
1050,https://neurips.cc/virtual/2022/poster/55135,Decentralized Local Stochastic Extra-Gradient for Variational Inequalities,Poster,NeurIPS,2022,"We consider distributed stochastic variational inequalities (VIs) on unbounded domains with the problem data that is heterogeneous (non-IID) and distributed across many devices. We make a very general assumption on the computational network that, in particular, covers the settings of fully decentralized calculations with time-varying networks and centralized topologies commonly used in Federated Learning. Moreover, multiple local updates on the workers can be made for reducing the communication frequency between the workers.We extend the stochastic extragradient method to this very general setting and theoretically analyze its convergence rate in the strongly-monotone, monotone, and non-monotone (when a Minty solution exists) settings. The provided rates explicitly exhibit the dependence on network characteristics (e.g., mixing time), iteration counter, data heterogeneity, variance, number of devices, and other standard parameters. As a special case, our method and analysis apply to distributed stochastic saddle-point problems (SPP), e.g., to the training of Deep Generative Adversarial Networks (GANs) for which decentralized training has been reported to be extremely challenging. In experiments for the decentralized training of GANs we demonstrate the effectiveness of our proposed approach.",,https://openreview.net/forum?id=Y4vT7m4e3d,https://neurips.cc/virtual/2022/poster/55135,https://neurips.cc/virtual/2022/poster/55135,Not relevant,Defence,"['Distributed', 'Stochastic', 'Variational Inequalities', 'Federated Learning', 'Extragradient', 'Deep Generative Adversarial Networks']",['GANs'],Not relevant,Not relevant,,,,
1053,https://neurips.cc/virtual/2022/poster/56124,Fast and Robust Rank Aggregation against Model Misspecification,Poster,NeurIPS,2022,"In rank aggregation (RA), a collection of preferences from different users are summarized into a total order under the assumption of homogeneity of users. Model misspecification in RA arises since the homogeneity assumption fails to be satisfied in the complex real-world situation. Existing robust RAs usually resort to an augmentation of the ranking model to account for additional noises, where the collected preferences can be treated as a noisy perturbation of idealized preferences. Since the majority of robust RAs rely on certain perturbation assumptions,  they cannot generalize well to agnostic noise-corrupted preferences in the real world. In this paper, we propose CoarsenRank, which possesses robustness against model misspecification. Specifically, the properties of our CoarsenRank are summarized as follows: (1) CoarsenRank is designed for mild model misspecification, which assumes there exist the ideal preferences (consistent with model assumption) that locate in a neighborhood of the actual preferences. (2) CoarsenRank then performs regular RAs over a neighborhood of the preferences instead of the original data set directly. Therefore, CoarsenRank enjoys robustness against model misspecification within a neighborhood. (3) The neighborhood of the data set is defined via their empirical data distributions. Further, we put an exponential prior on the unknown size of the neighborhood and derive a much-simplified posterior formula for CoarsenRank under particular divergence measures. (4) CoarsenRank is further instantiated to Coarsened Thurstone, Coarsened Bradly-Terry, and Coarsened Plackett-Luce with three popular probability ranking models. Meanwhile, tractable optimization strategies are introduced with regards to each instantiation respectively. In the end, we apply CoarsenRank on four real-world data sets. Experiments show that CoarsenRank is fast and robust, achieving consistent improvements over baseline methods.",https://www.jmlr.org/papers/v23/20-315.html,,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/56124.png?t=1669563730.5734596,https://neurips.cc/virtual/2022/poster/56124,Not relevant,Defence,"['rank aggregation', 'model misspecification', 'robustness', 'CoarsenRank', 'neighborhood', 'empirical data distributions', 'Thurstone', 'Bradly-Terry', 'Plackett-Luce', 'probability ranking models', 'optimization']",['real-world data sets'],,,,,,
1060,https://neurips.cc/virtual/2022/poster/53219,Archimedes Meets Privacy: On Privately Estimating Quantiles in High Dimensions Under Minimal Assumptions,Poster,NeurIPS,2022,"The last few years have seen a surge of work on high dimensional statistics under privacy constraints, mostly following two main lines of work: the ""worst case"" line, which does not make any distributional assumptions on the input data; and the ""strong assumptions"" line, which assumes that the data is generated from specific families, e.g., subgaussian distributions.In this work we take a middle ground, obtaining new differentially private algorithms with polynomial sample complexity for estimating quantiles in high-dimensions, as well as estimating and sampling points of high Tukey depth, all working under very mild distributional assumptions. From the technical perspective, our work relies upon fundamental robustness results in the convex geometry literature, demonstrating how such results can be used in a private context. Our main object of interest is the (convex) floating body (FB), a notion going back to Archimedes, which is a robust and well studied high-dimensional analogue of the interquantile range of a distribution.  We show how one can privately, and with polynomially many samples, (a) output an approximate interior point of the FB -- e.g., ""a typical user"" in a high-dimensional database -- by leveraging the robustness of the Steiner point of the FB; and at the expense of polynomially many more samples, (b) produce an approximate uniform sample from the FB, by constructing a private noisy projection oracle.",,https://openreview.net/forum?id=ArZWGF0Ifl7,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53219.png?t=1669603012.7511764,https://neurips.cc/virtual/2022/poster/53219,Not relevant,Other aspects,"['privacy', 'quantiles', 'high dimension', 'differentially private algorithms', 'convex geometry', 'robustness', 'Archimedes', 'floating body', 'Steiner point', 'noisy projection oracle']",['None'],,,,,,
1069,https://neurips.cc/virtual/2022/poster/53629,Conditional Diffusion Process for Inverse Halftoning,Poster,NeurIPS,2022,"Inverse halftoning is a technique used to recover realistic images from ancient prints (\textit{e.g.}, photographs, newspapers, books). The rise of deep learning has led to the gradual incorporation of neural network designs into inverse halftoning methods. Most of existing inverse halftoning approaches adopt the U-net architecture, which uses an encoder to encode halftone prints, followed by a decoder for image reconstruction. However, the mainstream supervised learning paradigm with element-wise regression commonly adopted in U-net based methods has poor generalization ability in practical applications. Specifically, when there is a large gap between the dithering patterns of the training and test halftones, the reconstructed continuous-tone images have obvious artifacts. This is an important issue in practical applications, since the algorithms for generating halftones are ever-evolving. Even for the same algorithm, different parameter choices will result in different halftone dithering patterns. In this paper, we propose the first generative halftoning method in the literature, which regards the black pixels in halftones as physically moving particles, and makes the randomly distributed particles move under some certain guidance through reverse diffusion process, so as to obtain desired halftone patterns. In particular, we propose a Conditional Diffusion model for image Halftoning (CDH), which consists of a halftone dithering process and an inverse halftoning process. By changing the initial state of the diffusion model, our method can generate visually plausible halftones with different dithering patterns under the condition of image gray level and Laplacian prior. To avoid introducing redundant patterns and undesired artifacts, we propose a meta-halftone guided network to incorporate blue noise guidance in the diffusion process. In this way, halftone images subject to more diverse distributions are fed into the inverse halftoning model, which helps the model to learn a more robust mapping from halftone distributions to continuous-tone distributions, thereby improving the generalization ability to unseen samples. Quantitative and qualitative experimental results demonstrate that the proposed method achieves state-of-the-art results.",,https://openreview.net/forum?id=EEcFW47sktI,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53629.png?t=1668844371.1157162,https://neurips.cc/virtual/2022/poster/53629,Not relevant,Defence,"['Inverse halftoning', 'Deep learning', 'U-net architecture', 'Supervised learning', 'Generative halftoning', 'Diffusion process', 'Halftone dithering', 'Gray level', 'Laplacian prior', 'Blue noise guidance']","['Ancient prints', 'Photographs', 'Newspapers', 'Books']",,,,,,
1077,https://neurips.cc/virtual/2022/poster/55065,Temporal Effective Batch Normalization in Spiking Neural Networks,Poster,NeurIPS,2022,"Spiking Neural Networks (SNNs) are promising in neuromorphic hardware owing to utilizing spatio-temporal information and sparse event-driven signal processing. However, it is challenging to train SNNs due to the non-differentiable nature of the binary firing function. The surrogate gradients alleviate the training problem and make SNNs obtain comparable performance as Artificial Neural Networks (ANNs) with the same structure. Unfortunately, batch normalization, contributing to the success of ANNs, does not play a prominent role in SNNs because of the additional temporal dimension. To this end, we propose an effective normalization method called temporal effective batch normalization (TEBN). By rescaling the presynaptic inputs with different weights at every time-step, temporal distributions become smoother and uniform. Theoretical analysis shows that TEBN can be viewed as a smoother of SNN's optimization landscape and could help stabilize the gradient norm. Experimental results on both static and neuromorphic datasets show that SNNs with TEBN outperform the state-of-the-art accuracy with fewer time-steps, and achieve better robustness to hyper-parameters than other normalizations.",,https://openreview.net/forum?id=fLIgyyQiJqz,https://neurips.cc/virtual/2022/poster/55065,https://neurips.cc/virtual/2022/poster/55065,Not relevant,Defence,"['Spiking Neural Networks', 'Non-differentiable nature', 'Binary firing function', 'Surrogate gradients', 'Artificial Neural Networks', 'Temporal dimension', 'Temporal effective batch normalization', 'Optimization landscape', 'Neuromorphic datasets']",['Neuromorphic hardware'],,,,,,
1081,https://neurips.cc/virtual/2022/poster/52838,Best of Both Worlds Model Selection,Poster,NeurIPS,2022,"We study the problem of model selection in bandit scenarios in the presence of nested policy classes, with the goal of obtaining simultaneous adversarial and stochastic (``best of both worlds"") high-probability regret guarantees. Our approach requires that each base learner comes with a candidate regret bound that may or may not hold, while our meta algorithm plays each base learner according to a schedule that keeps the base learner's candidate regret bounds balanced until they are detected to violate their guarantees. We develop careful mis-specification tests specifically designed to blend the above model selection criterion with the ability to leverage the (potentially benign) nature of the environment. We recover the model selection guarantees of the CORRAL algorithm for adversarial environments, but with the additional benefit of achieving high probability regret bounds. More importantly, our model selection results also hold simultaneously in stochastic environments under gap assumptions. These are the first theoretical results that achieve best-of-both world (stochastic and adversarial) guarantees while performing model selection in contextual bandit scenarios.",,https://openreview.net/forum?id=9-vs8BucEoo,https://neurips.cc/virtual/2022/poster/52838,https://neurips.cc/virtual/2022/poster/52838,Other attack,Defence,"['Model selection', 'Bandit scenarios', 'Nested policy classes', 'Adversarial and stochastic high-probability regret guarantees', 'Base learner', 'Candidate regret bound', 'Meta algorithm', 'Mis-specification tests', 'CORRAL algorithm', 'Contextual bandit scenarios']",,Other aspects,Evasion,,,Arguable,Arguable
1086,https://neurips.cc/virtual/2022/poster/55061,Scaling & Shifting Your Features: A New Baseline for Efficient Model Tuning,Poster,NeurIPS,2022,"Existing fine-tuning methods either tune all parameters of the pre-trained model (full fine-tuning), which is not efficient, or only tune the last linear layer (linear probing), which suffers a significant accuracy drop compared to the full fine-tuning. In this paper, we propose a new parameter-efficient fine-tuning method termed as SSF, representing that researchers only need to Scale and Shift the deep Features extracted by a pre-trained model to catch up with the performance of full fine-tuning. In this way, SSF also surprisingly outperforms other parameter-efficient fine-tuning approaches even with a smaller number of tunable parameters. Furthermore, different from some existing parameter-efficient fine-tuning methods (e.g., Adapter or VPT) that introduce the extra parameters and computational cost in the training and inference stages, SSF only adds learnable parameters during the training stage, and these additional parameters can be merged into the original pre-trained model weights via re-parameterization in the inference phase. With the proposed SSF, our model obtains 2.46% (90.72% vs. 88.54%) and 11.48% (73.10% vs. 65.57%) performance improvement on FGVC and VTAB-1k in terms of Top-1 accuracy compared to the full fine-tuning but only fine-tuning about 0.3M parameters. We also conduct amounts of experiments in various model families (CNNs, Transformers, and MLPs) and datasets. Results on 26 image classification datasets in total and 3 robustness & out-of-distribution datasets show the effectiveness of SSF. Code is available at https://github.com/dongzelian/SSF. ",,https://openreview.net/forum?id=XtyeppctGgc,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55061.png?t=1668872261.526599,https://neurips.cc/virtual/2022/poster/55061,Not relevant,Defence,"['Fine-tuning', 'Parameter-efficient fine-tuning', 'Pre-trained model', 'Deep Features', 'Scale and Shift', 'Re-parameterization', 'Robustness', 'Out-of-distribution']","['Image classification', 'FGVC', 'VTAB-1k']",,,,,,
1102,https://neurips.cc/virtual/2022/poster/55164,DISCO: Adversarial Defense with Local Implicit Functions,Poster,NeurIPS,2022,"The problem of adversarial defenses for image classification, where the goal is to robustify a classifier against adversarial examples, is considered. Inspired by the hypothesis that these examples lie beyond the natural image manifold, a novel aDversarIal defenSe with local impliCit functiOns (DISCO) is proposed to remove adversarial perturbations by localized manifold projections. DISCO consumes an adversarial image and a query pixel location and outputs a clean RGB value at the location. It is implemented with an encoder and a local implicit module, where the former produces per-pixel deep features and the latter uses the features in the neighborhood of query pixel for predicting the clean RGB value. Extensive experiments demonstrate that both DISCO and its cascade version outperform prior defenses, regardless of whether the defense is known to the attacker. DISCO is also shown to be data and parameter efficient and to mount defenses that transfers across datasets, classifiers and attacks.",,https://openreview.net/forum?id=vgIz0emVTAd,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55164.png?t=1667951219.857774,https://neurips.cc/virtual/2022/poster/55164,Evasion,Defence,"['Adversarial defense', 'Image classification', 'Manifold projections', 'Implicit functions', 'Deep features', 'RGB value', 'Data efficiency', 'Transferability']",,Defence,Evasion,,,,
1118,https://neurips.cc/virtual/2022/poster/55188,GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images,Poster,NeurIPS,2022,"As several industries are moving towards modeling massive 3D virtual worlds, the need for content creation tools that can scale in terms of the quantity, quality, and diversity of 3D content is becoming evident. In our work, we aim to train performant 3D generative models that synthesize textured meshes which can be directly consumed by 3D rendering engines, thus immediately usable in downstream applications. Prior works on 3D generative modeling either lack geometric details, are limited in the mesh topology they can produce, typically do not support textures, or utilize neural renderers in the synthesis process, which makes their use in common 3D software non-trivial. In this work, we introduce GET3D, a Generative model that directly generates Explicit Textured 3D meshes with complex topology, rich geometric details, and high fidelity textures. We bridge recent success in the differentiable surface modeling, differentiable rendering as well as 2D Generative Adversarial Networks to train our model from 2D image collections. GET3D is able to generate high-quality 3D textured meshes, ranging from cars, chairs, animals, motorbikes and human characters to buildings, achieving significant improvements over previous methods.",,https://openreview.net/forum?id=GAUwreODU5L,https://neurips.cc/virtual/2022/poster/55188,https://neurips.cc/virtual/2022/poster/55188,Not relevant,Defence,"['Adversarial defense', 'Image classification', 'Manifold projections', 'Deep features', 'Local implicit module', 'Data and parameter efficiency', 'Transfer learning', '3D generative models', 'Textured meshes', '3D rendering engines', 'Differentiable surface modeling', 'Differentiable rendering', '2D Generative Adversarial Networks']","['Industries', '3D virtual worlds', 'Content creation', '3D software', 'Downstream applications']",,,,,,
1125,https://neurips.cc/virtual/2022/poster/55190,INRAS: Implicit Neural Representation for Audio Scenes,Poster,NeurIPS,2022,"The spatial acoustic information of a scene, i.e., how sounds emitted from a particular location in the scene are perceived in another location, is key for immersive scene modeling. Robust representation of scene's acoustics can be formulated through a continuous field formulation along with impulse responses varied by emitter-listener locations. The impulse responses are then used to render sounds perceived by the listener. While such representation is advantageous, parameterization of impulse responses for generic scenes presents itself as a challenge. Indeed, traditional pre-computation methods have only implemented parameterization at discrete probe points and require large storage, while other existing methods such as geometry-based sound simulations still suffer from inability to simulate all wave-based sound effects. In this work, we introduce a novel neural network for light-weight Implicit Neural Representation for Audio Scenes (INRAS), which can render a high fidelity time-domain impulse responses at any arbitrary emitter-listener positions by learning a continuous implicit function. INRAS disentangles scene’s geometry features with three modules to generate independent features for the emitter, the geometry of the scene, and the listener respectively. These lead to an efficient reuse of scene-dependent features and support effective multi-condition training for multiple scenes.  Our experimental results show that INRAS outperforms existing approaches for representation and rendering of sounds for varying emitter-listener locations in all aspects, including the impulse response quality, inference speed, and storage requirements. ",,https://openreview.net/forum?id=7KBzV5IL7W,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55190.png?t=1669238480.1259675,https://neurips.cc/virtual/2022/poster/55190,Not relevant,Other aspects,"['Implicit Neural Representation for Audio Scenes (INRAS)', 'spatial acoustic information', 'immersive scene modeling', 'impulse responses', 'neural network', 'emitter-listener positions', 'geometry features', 'multi-condition training', 'impulse response quality', 'inference speed', 'storage requirements']",['audio'],,,,,,
1134,https://neurips.cc/virtual/2022/poster/54558,AniFaceGAN: Animatable 3D-Aware Face Image Generation for Video Avatars,Poster,NeurIPS,2022,"Although 2D generative models have made great progress in face image generation and animation, they often suffer from undesirable artifacts such as 3D inconsistency when rendering images from different camera viewpoints. This prevents them from synthesizing video animations indistinguishable from real ones. Recently, 3D-aware GANs extend 2D GANs for explicit disentanglement of camera pose by leveraging 3D scene representations. These methods can well preserve the 3D consistency of the generated images across different views, yet they cannot achieve fine-grained control over other attributes, among which facial expression control is arguably the most useful and desirable for face animation. In this paper, we propose an animatable 3D-aware GAN for multiview consistent face animation generation. The key idea is to decompose the 3D representation of the 3D-aware GAN into a template field and a deformation field, where the former represents different identities with a canonical expression, and the latter characterizes expression variations of each identity. To achieve meaningful control over facial expressions via deformation, we propose a 3D-level imitative learning scheme between the generator and a parametric 3D face model during adversarial training of the 3D-aware GAN. This helps our method achieve high-quality animatable face image generation with strong visual 3D consistency, even though trained with only unstructured 2D images. Extensive experiments demonstrate our superior performance over prior works. Project page: \url{https://yuewuhkust.github.io/AniFaceGAN/",,https://openreview.net/forum?id=LfHwpvDPGpx,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54558.png?t=1668673145.948451,https://neurips.cc/virtual/2022/poster/54558,Not relevant,Defence,"['Animatable 3D-Aware Face Image Generation', 'Video Avatars', '2D generative models', '3D inconsistency', '3D-aware GANs', 'facial expression control', '3D level imitative learning scheme', 'adversarial training']",,Not relevant,Not relevant,,,,
1143,https://neurips.cc/virtual/2022/poster/55354,Roadblocks for Temporarily Disabling Shortcuts and Learning New Knowledge,Poster,NeurIPS,2022,"Deep learning models have been found with a tendency of relying on shortcuts, i.e., decision rules that perform well on standard benchmarks but fail when transferred to more challenging testing conditions. Such reliance may hinder deep learning models from learning other task-related features and seriously affect their performance and robustness. Although recent studies have shown some characteristics of shortcuts, there are few investigations on how to help the deep learning models to solve shortcut problems. This paper proposes a framework to address this issue by setting up roadblocks on shortcuts. Specifically, roadblocks are placed when the model is urged to learn to complete a gently modified task to ensure that the learned knowledge, including shortcuts, is insufficient the complete the task. Therefore, the model trained on the modified task will no longer over-rely on shortcuts. Extensive experiments demonstrate that the proposed framework significantly improves the training of networks on both synthetic and real-world datasets in terms of both classification accuracy and feature diversity. Moreover, the visualization results show that the mechanism behind the proposed our method is consistent with our expectations. In summary, our approach can effectively disable the shortcuts and thus learn more robust features.",,https://openreview.net/forum?id=QjurhjyTAb,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55354.png?t=1668847167.3669684,https://neurips.cc/virtual/2022/poster/55354,Robustness,Defence,"['shortcuts', 'deep learning', 'task-related features', 'performance', 'robustness', 'modified task', 'classification accuracy', 'feature diversity']",,,,,,,
1157,https://neurips.cc/virtual/2022/poster/54186,Tracking Functional Changes in Nonstationary Signals with Evolutionary Ensemble Bayesian Model for Robust Neural Decoding,Poster,NeurIPS,2022,"Neural signals are typical nonstationary data where the functional mapping between neural activities and the intentions (such as the velocity of movements) can occasionally change. Existing studies mostly use a fixed neural decoder, thus suffering from an unstable performance given neural functional changes. We propose a novel evolutionary ensemble framework (EvoEnsemble) to dynamically cope with changes in neural signals by evolving the decoder model accordingly. EvoEnsemble integrates evolutionary computation algorithms in a Bayesian framework where the fitness of models can be sequentially computed with their likelihoods according to the incoming data at each time slot, which enables online tracking of time-varying functions. Two strategies of evolve-at-changes and history-model-archive are designed to further improve efficiency and stability. Experiments with simulations and neural signals demonstrate that EvoEnsemble can track the changes in functions effectively thus improving the accuracy and robustness of neural decoding. The improvement is most significant in neural signals with functional changes.",,https://openreview.net/forum?id=7fU8UPo875w,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/73983c01982794632e0270cd0006d407.png?t=1666180661.2425115,https://neurips.cc/virtual/2022/poster/54186,Robustness,Defence,"['nonstationary signals', 'neural signals', 'evolutionary ensemble', 'Bayesian model', 'robust neural decoding', 'evolving the decoder model', 'online tracking', 'time-varying functions', 'evolve-at-changes', 'history-model-archive']",,,,,,,
1167,https://neurips.cc/virtual/2022/poster/54563,Adversarial Task Up-sampling for Meta-learning,Poster,NeurIPS,2022,"The success of meta-learning on existing benchmarks is predicated on the assumption that the distribution of meta-training tasks covers meta-testing tasks. Frequent violation of the assumption in applications with either insufficient tasks or a very narrow meta-training task distribution leads to memorization or learner overfitting. Recent solutions have pursued augmentation of meta-training tasks, while it is still an open question to generate both correct and sufficiently imaginary tasks. In this paper, we seek an approach that up-samples meta-training tasks from the task representation via a task up-sampling network. Besides, the resulting approach named Adversarial Task Up-sampling (ATU) suffices to generate tasks that can maximally contribute to the latest meta-learner by maximizing an adversarial loss. On few-shot sine regression and image classification datasets, we empirically validate the marked improvement of ATU over state-of-the-art task augmentation strategies in the meta-testing performance and also the quality of up-sampled tasks.",,https://openreview.net/forum?id=pFqgUJxXXz,https://neurips.cc/virtual/2022/poster/54563,https://neurips.cc/virtual/2022/poster/54563,Other attack,Defence,"['meta-learning', 'Adversarial Task Up-sampling', 'task up-sampling network', 'few-shot', 'sine regression', 'image classification', 'meta-testing performance', 'quality of up-sampled tasks']",,,,,,,
1168,https://neurips.cc/virtual/2022/poster/54791,Wasserstein Logistic Regression with Mixed Features,Poster,NeurIPS,2022,"Recent work has leveraged the popular distributionally robust optimization paradigm to combat overfitting in classical logistic regression. While the resulting classification scheme displays a promising performance in numerical experiments, it is inherently limited to numerical features. In this paper, we show that distributionally robust logistic regression with mixed (\emph{i.e.}, numerical and categorical) features, despite amounting to an optimization problem of exponential size, admits a polynomial-time solution scheme. We subsequently develop a practically efficient cutting plane approach that solves the problem as a sequence of polynomial-time solvable exponential conic programs. Our method retains many of the desirable theoretical features of previous works, but---in contrast to the literature---it does not admit an equivalent representation as a regularized logistic regression, that is, it represents a genuinely novel variant of the logistic regression problem. We show that our method outperforms both the unregularized and the regularized logistic regression on categorical as well as mixed-feature benchmark instances.",,https://openreview.net/forum?id=U-RsnLYHcKa,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54791.png?t=1669390989.3379688,https://neurips.cc/virtual/2022/poster/54791,Not relevant,Defence,"['distributionally robust optimization', 'overfitting', 'logistic regression', 'mixed features', 'cutting plane approach', 'exponential conic programs']",Not clear,,,,,,
1176,https://neurips.cc/virtual/2022/poster/55307,Towards Practical Control of Singular Values of Convolutional Layers,Poster,NeurIPS,2022,"In general, convolutional neural networks (CNNs) are easy to train, but their essential properties, such as generalization error and adversarial robustness, are hard to control. Recent research demonstrated that singular values of convolutional layers significantly affect such elusive properties and offered several methods for controlling them. Nevertheless, these methods present an intractable computational challenge or resort to coarse approximations. In this paper, we offer a principled approach to alleviating constraints of the prior art at the expense of an insignificant reduction in layer expressivity. Our method is based on the tensor-train decomposition; it retains control over the actual singular values of convolutional mappings while providing structurally sparse and hardware-friendly representation. We demonstrate the improved properties of modern CNNs with our method and analyze its impact on the model performance, calibration, and adversarial robustness. The source code is available at: https://github.com/WhiteTeaDragon/practical",,https://openreview.net/forum?id=T5TtjbhlAZH,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55307.png?t=1669914344.6531692,https://neurips.cc/virtual/2022/poster/55307,Robustness,Defence,"['convolutional neural networks', 'generalization error', 'adversarial robustness', 'singular values', 'convolutional layers', 'tensor-train decomposition', 'structurally sparse', 'hardware-friendly representation', 'model performance', 'calibration', 'adversarial robustness']",Not clear,,,,,,
1178,https://neurips.cc/virtual/2022/poster/56090,[Re] Exacerbating Algorithmic Bias through Fairness Attacks,Poster,NeurIPS,2022,"We conducted a reproducibility study of the paper 'Exacerbating Algorithmic Bias through Fairness Attacks'. According to the paper, current research on adversarial attacks is primarily focused on targeting model performance, which motivates the need for adversarial attacks on fairness. To that end, the authors propose two novel data poisoning adversarial attacks, the influence attack on fairness and the anchoring attack. We aim to verify the main claims of the paper, namely that: a) the proposed methods indeed affect a model's fairness and outperform existing attacks, b) the anchoring attack hardly affects performance, while impacting fairness, and c) the influence attack on fairness provides a controllable trade-off between performance and fairness degradation.",https://rescience.github.io/bibliography/Nalmpantis_2022.html,,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/0ae0e7479c8934e660a1f21174e16baa.png?t=1666119245.1932914,https://neurips.cc/media/neurips-2022/Slides/56090.pdf,Robustness,Attack,"['Adversarial attacks', 'Fairness', 'Data poisoning', 'Algorithmic bias', 'Reproducibility study']",,,,,,,
1180,https://neurips.cc/virtual/2022/poster/53433,Distilling Representations from GAN Generator via Squeeze and Span,Poster,NeurIPS,2022,"In recent years, generative adversarial networks (GANs) have been an actively studied topic and shown to successfully produce high-quality realistic images in various domains. The controllable synthesis ability of GAN generators suggests that they maintain informative, disentangled, and explainable image representations, but leveraging and transferring their representations to downstream tasks is largely unexplored. In this paper, we propose to distill knowledge from GAN generators by squeezing and spanning their representations. We \emph{squeeze} the generator features into representations that are invariant to semantic-preserving transformations through a network before they are distilled into the student network. We \emph{span} the distilled representation of the synthetic domain to the real domain by also using real training data to remedy the mode collapse of GANs and boost the student network performance in a real domain. Experiments justify the efficacy of our method and reveal its great significance in self-supervised representation learning. Code is available at https://github.com/yangyu12/squeeze-and-span.",,https://openreview.net/forum?id=_P4JCoz83Mb,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53433.png?t=1668307252.8761988,https://neurips.cc/media/neurips-2022/Slides/53433.pdf,Not relevant,Defence,"['generative adversarial networks (GANs)', 'representations', 'distill', 'squeeze', 'span', 'invariant', 'semantic-preserving transformations', 'self-supervised representation learning']",['image'],,,,,,
1185,https://neurips.cc/virtual/2022/poster/56123,Fairness-Aware PAC Learning from Corrupted Data,Poster,NeurIPS,2022,"Addressing fairness concerns about machine learning models is a crucial step towards their long-term adoption in real-world automated systems. While many approaches have been developed for training fair models from data, little is known about the robustness of these methods to data corruption. In this work we consider fairness-aware learning under worst-case data manipulations. We show that an adversary can in some situations force any learner to return an overly biased classifier, regardless of the sample size and with or without degrading accuracy, and that the strength of the excess bias increases for learning problems with underrepresented protected groups in the data. We also prove that our hardness results are tight up to constant factors. To this end, we study two natural learning algorithms that optimize for both accuracy and fairness and show that these algorithms enjoy guarantees that are order-optimal in terms of the corruption ratio and the protected groups frequencies in the large data limit.",https://jmlr.org/papers/v23/21-1189.html,,https://neurips.cc/virtual/2022/poster/56123,https://neurips.cc/virtual/2022/poster/56123,Poisoning,Defence,"['fairness-aware', 'PAC learning', 'corrupted data', 'adversary', 'biased classifier', 'underrepresented protected groups', 'corruption ratio', 'protected groups frequencies']",,Both,Evasion,Other aspects,,Arguable,Arguable
1204,https://neurips.cc/virtual/2022/poster/54802,SizeShiftReg: a Regularization Method for Improving Size-Generalization in Graph Neural Networks,Poster,NeurIPS,2022,"In the past few years, graph neural networks (GNNs) have become the de facto model of choice for graph classification. While, from the theoretical viewpoint, most GNNs can operate on graphs of any size, it is empirically observed that their classification performance degrades when they are applied on graphs with sizes that differ from those in the training data. Previous works have tried to tackle this issue in graph classification by providing the model with inductive biases derived from assumptions on the generative process of the graphs, or by requiring access to graphs from the test domain. The first strategy is tied to the quality of the assumptions made for the generative process, and requires the use of specific models designed after the explicit definition of the generative process of the data, leaving open the question of how to improve the performance of generic GNN models in general settings. On the other hand, the second strategy can be applied to any GNN, but requires access to information that is not always easy to obtain. In this work we consider the scenario in which we only have access to the training data, and we propose a regularization strategy that can be applied to any GNN to improve its generalization capabilities from smaller to larger graphs without requiring access to the test data. Our regularization is based on the idea of simulating a shift in the size of the training graphs using coarsening techniques, and enforcing the model to be robust to such a shift. Experimental results on standard datasets show that popular GNN models, trained on the 50% smallest graphs in the dataset and tested on the 10% largest graphs, obtain performance improvements of up to 30% when trained with our regularization strategy.",,https://openreview.net/forum?id=wOI0AUAq9BR,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/6dfe08eda761bd321f8a9b239f6f4ec3.png?t=1666977389.2487452,https://neurips.cc/virtual/2022/poster/54802,Robustness,Defence,"['graph neural networks', 'graph classification', 'generalization capabilities', 'regularization strategy', 'coarsening techniques']",,,,,,,
1208,https://neurips.cc/virtual/2022/poster/52945,Data-Driven Conditional Robust Optimization,Poster,NeurIPS,2022,"In this paper, we study a novel approach for data-driven decision-making under uncertainty in the presence of contextual information. Specifically, we solve this problem from a Conditional Robust Optimization (CRO) point of view. We propose an integrated framework that designs the conditional uncertainty set by jointly learning the partitions in the covariate data space and simultaneously constructing partition specific deep uncertainty sets for the random vector that perturbs the CRO problem. We also provide  theoretical guarantees for the coverage of the uncertainty sets and value at risk performances obtained using the proposed CRO approach. Finally, we use the simulated and real world data to show the implementation of our approach and compare it against two non-contextual benchmark approaches to demonstrate the value of exploiting contextual information in robust optimization.",,https://openreview.net/forum?id=rUb6iKYrgXQ,https://neurips.cc/virtual/2022/poster/52945,https://neurips.cc/virtual/2022/poster/52945,Robustness,Defence,"['Conditional Robust Optimization', 'Deep uncertainty sets', 'Covariate data space', 'Value at Risk', 'Contextual information']",['Data-driven decision-making'],,,,,,
1212,https://neurips.cc/virtual/2022/poster/53215,Recovering Private Text in Federated Learning of Language Models,Poster,NeurIPS,2022,"Federated learning allows distributed users to collaboratively train a model while keeping each user’s data private. Recently, a growing body of work has demonstrated that an eavesdropping attacker can effectively recover image data from gradients transmitted during federated learning. However, little progress has been made in recovering text data. In this paper, we present a novel attack method FILM for federated learning of language models (LMs). For the first time, we show the feasibility of recovering text from large batch sizes of up to 128 sentences. Unlike image-recovery methods that are optimized to match gradients, we take a distinct approach that first identifies a set of words from gradients and then directly reconstructs sentences based on beam search and a prior-based reordering strategy. We conduct the FILM attack on several large-scale datasets and show that it can successfully reconstruct single sentences with high fidelity for large batch sizes and even multiple sentences if applied iteratively.We evaluate three defense methods: gradient pruning, DPSGD, and a simple approach to freeze word embeddings that we propose.  We show that both gradient pruning and DPSGD lead to a significant drop in utility. However, if we fine-tune a public pre-trained LM on private text without updating word embeddings, it can effectively defend the attack with minimal data utility loss. Together, we hope that our results can encourage the community to rethink the privacy concerns of LM training and its standard practices in the future. Our code is publicly available at https://github.com/Princeton-SysML/FILM .",,https://openreview.net/forum?id=dqgzfhHd2-,https://neurips.cc/virtual/2022/poster/53215,https://neurips.cc/virtual/2022/poster/53215,Data Extraction,Attack,"['Federated Learning', 'Language Models', 'Privacy', 'Eavesdropping', 'Gradient attack', 'Recovering Private text', 'FILM attack', 'Defense methods', 'Gradient pruning', 'DPSGD', 'Word Embeddings']",['Language Modeling'],,,,,,
1232,https://neurips.cc/virtual/2022/poster/54496,Reconstructing Training Data From Trained Neural Networks,Poster,NeurIPS,2022,"Understanding to what extent neural networks memorize training data is an intriguing question with practical and theoretical implications. In this paper we show that in some cases a significant fraction of the training data can in fact be reconstructed from the parameters of a trained neural network classifier.We propose a novel reconstruction scheme that stems from recent theoretical results about the implicit bias in training neural networks with gradient-based methods.To the best of our knowledge, our results are the first to show that reconstructing a large portion of the actual training samples from a trained neural network classifier is generally possible.This has negative implications on privacy, as it can be used as an attack for revealing sensitive training data. We demonstrate our method for binary MLP classifiers on a few standard computer vision datasets.",,https://openreview.net/forum?id=Sxk8Bse3RKO,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54496.png?t=1669360559.9534528,https://neurips.cc/virtual/2022/poster/54496,Data Extraction,Attack,"['Neural networks', 'Training data', 'Reconstruction', 'Implicit bias', 'Gradient-based methods', 'Privacy', 'Attack', 'Computer vision', 'MLP classifiers']",,,,,,,
1269,https://neurips.cc/virtual/2022/poster/53225,Diversity vs. Recognizability: Human-like generalization in one-shot generative models,Poster,NeurIPS,2022,"Robust generalization to new concepts has long remained a distinctive feature of human intelligence. However, recent progress in deep generative models has now led to neural architectures capable of synthesizing novel instances of unknown visual concepts from a single training example. Yet, a more precise comparison between these models and humans is not possible because existing performance metrics for generative models (i.e., FID, IS, likelihood) are not appropriate for the one-shot generation scenario. Here, we propose a new framework to evaluate one-shot generative models along two axes: sample recognizability vs. diversity  (i.e., intra-class variability). Using this framework, we perform a systematic evaluation of representative one-shot generative models on the Omniglot handwritten dataset. We first show that GAN-like and VAE-like models fall on opposite ends of the diversity-recognizability space. Extensive analyses of the effect of key model parameters further revealed that spatial attention and context integration have a linear contribution to the diversity-recognizability trade-off. In contrast, disentanglement transports the model along a parabolic curve that could be used to maximize recognizability. Using the diversity-recognizability framework, we were able to identify models and parameters that closely approximate human data.",,https://openreview.net/forum?id=DVfZKXSFW5m,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53225.png?t=1669025377.912575,https://neurips.cc/virtual/2022/poster/53225,Not relevant,Defence,"['One-shot generative models', 'Human-like generalization', 'FID', 'IS', 'Likelihood', 'Diversity', 'Recognizability', 'GAN-like models', 'VAE-like models', 'Spatial attention', 'Context integration', 'Disentanglement', 'Omniglot dataset']",,,,,,,
1277,https://neurips.cc/virtual/2022/poster/54182,Offline Multi-Agent Reinforcement Learning with Knowledge Distillation,Poster,NeurIPS,2022,"We introduce an offline multi-agent reinforcement learning ( offline MARL) framework that utilizes previously collected data without additional online data collection. Our method reformulates offline MARL as a sequence modeling problem and thus builds on top of the simplicity and scalability of the Transformer architecture. In the fashion of centralized training and decentralized execution, we propose to first train a teacher policy as if the MARL dataset is generated by a single agent. After the teacher policy has identified and recombined the ""good"" behavior in the dataset, we create separate student policies and distill not only the teacher policy's features but also its structural relations among different agents' features to student policies. Despite its simplicity, the proposed method outperforms state-of-the-art model-free offline MARL baselines while being more robust to demonstration's quality on several environments.",,https://openreview.net/forum?id=yipUuqxveCy,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54182.png?t=1669142254.82625,https://neurips.cc/virtual/2022/poster/54182,Not relevant,Defence,"['Offline Multi-Agent Reinforcement Learning', 'Knowledge Distillation', 'Transformer architecture', 'centralized training', 'decentralized execution', 'teacher policy', 'student policies', 'distill', 'outperforms', 'state-of-the-art model-free offline MARL baselines', ""robust to demonstration's quality"", 'environments']",,,,,,,
1278,https://neurips.cc/virtual/2022/poster/53088,Visual correspondence-based explanations improve AI robustness and human-AI team accuracy,Poster,NeurIPS,2022,"Explaining artificial intelligence (AI) predictions is increasingly important and even imperative in many high-stake applications where humans are the ultimate decision-makers. In this work, we propose two novel architectures of explainable image classifiers that first explain, and then predict (as opposed to post-hoc explanation methods). Our models first rank the training-set images by their distance with the query in an image-level deep feature space. And then, we re-rank the top-50 shortlisted candidates using patch-wise similarity of 5 highest-similarity pairs of patches between the query and every candidate. On ImageNet, our models improve (by 1-4 points) the out-of-distribution accuracy on several datasets including Adversarial Patch and ImageNet-R while performing marginally worse (by 1-2 points) on ImageNet to the baselines (ResNet-50 pre-trained ImageNet). A consistent trend is observed on CUB. Via a large-scale, human study (~60 users per method per dataset) on ImageNet and CUB, we find our proposed correspondence-based explanations led to human-alone image classification accuracy and human-AI team accuracy that are consistently better than those of k-NN. Our correspondence-based explanations help users better correctly reject AI's wrong decisions than all other tested methods.Interestingly, for the first time, we show that it is possible to achieve complementary human-AI team accuracy (i.e. that is higher than either AI-alone or human-alone), in both image classification tasks.",,https://openreview.net/forum?id=UavQ9HYye6n,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/481d66d7006b307451e463d71d2fc53f.png?t=1666209625.6906343,https://neurips.cc/virtual/2022/poster/53088,Not relevant,Defence,"['explainable AI', 'image classifiers', 'out-of-distribution accuracy', 'adversarial patch', 'human-AI team accuracy', 'correspondence-based explanations', 'image classification', 'deep feature space']","['ImageNet', 'CUB']",,,,,,
1286,https://neurips.cc/virtual/2022/poster/54937,Quo Vadis: Is Trajectory Forecasting the Key Towards Long-Term Multi-Object Tracking?,Poster,NeurIPS,2022,"Recent developments in monocular multi-object tracking have been very successful in tracking visible objects and bridging short occlusion gaps, mainly relying on data-driven appearance models. While we have significantly advanced short-term tracking performance, bridging longer occlusion gaps remains elusive: state-of-the-art object trackers only bridge less than 10% of occlusions longer than three seconds. We suggest that the missing key is reasoning about future trajectories over a longer time horizon. Intuitively, the longer the occlusion gap, the larger the search space for possible associations. In this paper, we show that even a small yet diverse set of trajectory predictions for moving agents will significantly reduce this search space and thus improve long-term tracking robustness. Our experiments suggest that the crucial components of our approach are reasoning in a bird's-eye view space and generating a small yet diverse set of forecasts while accounting for their localization uncertainty. This way, we can advance state-of-the-art trackers on the MOTChallenge dataset and significantly improve their long-term tracking performance. This paper's source code and experimental data are available at https://github.com/dendorferpatrick/QuoVadis.",,https://openreview.net/forum?id=3r0yLLCo4fF,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54937.png?t=1669639120.69344,https://neurips.cc/virtual/2022/poster/54937,Not relevant,Defence,"['multi-object tracking', 'trajectory forecasting', 'occlusion gaps', 'appearance models', 'birds-eye view space', 'localization uncertainty']",,,,,,,
1302,https://neurips.cc/virtual/2022/poster/53127,Provably Adversarially Robust Detection of Out-of-Distribution Data (Almost) for Free,Poster,NeurIPS,2022,"The application of machine learning in safety-critical systems requires a reliable assessment of uncertainty.However, deep neural networks are known to produce highly overconfident predictions on out-of-distribution (OOD) data.Even if trained to be non-confident on OOD data, one can still adversarially manipulate OOD data so that the classifier again assigns high confidence to the manipulated samples.We show that two previously published defenses can be broken by better adapted attacks, highlighting the importance of robustness guarantees around OOD data.Since the existing method for this task is hard to train and significantly limits accuracy, we construct a classifier that can simultaneously achieve provably adversarially robust OOD detection and high clean accuracy.Moreover, by slightly modifying the classifier's architecture our method provably avoids the asymptotic overconfidence problem of standard neural networks.We provide code for all our experiments.",,https://openreview.net/forum?id=9ZWgrozGP0,https://neurips.cc/virtual/2022/poster/53127,https://neurips.cc/virtual/2022/poster/53127,Robustness,Defence,"['adversarially robust', 'OOD detection', 'overconfidence problem', 'provable', 'neural networks', 'classifier']",,Both,Evasion,Other aspects,Robustness,,Arguable
1307,https://neurips.cc/virtual/2022/poster/52970,Introspective Learning : A Two-Stage approach for Inference in Neural Networks,Poster,NeurIPS,2022,"In this paper, we advocate for two stages in a neural network's decision making process. The first is the existing feed-forward inference framework where patterns in given data are sensed and associated with previously learned patterns. The second stage is a slower reflection stage where we ask the network to reflect on its feed-forward decision by considering and evaluating all available choices. Together, we term the two stages as introspective learning. We use gradients of trained neural networks as a measurement of this reflection. A simple three-layered Multi Layer Perceptron is used as the second stage that predicts based on all extracted gradient features. We perceptually visualize the post-hoc explanations from both stages to provide a visual grounding to introspection. For the application of recognition, we show that an introspective network is 4% more robust and 42% less prone to calibration errors when generalizing to noisy data. We also illustrate the value of introspective networks in downstream tasks that require generalizability and calibration including active learning, out-of-distribution detection, and uncertainty estimation. Finally, we ground the proposed machine introspection to human introspection for the application of image quality assessment.",,https://openreview.net/forum?id=ok-SB1kz67Z,https://neurips.cc/virtual/2022/poster/52970,https://neurips.cc/virtual/2022/poster/52970,Robustness,Defence,"['Introspective learning', 'Neural networks', 'Inference', 'Decision making', 'Reflection', 'Gradients', 'Multi Layer Perceptron', 'Post-hoc explanations', 'Recognition', 'Robustness', 'Calibration errors', 'Generalizability', 'Uncertainty estimation', 'Human introspection', 'Image quality assessment']",Not clear,,,,,,
1326,https://neurips.cc/virtual/2022/poster/56139,The Importance of Being Correlated: Implications of Dependence in Joint Spectral Inference across Multiple Networks,Poster,NeurIPS,2022,"Spectral inference on multiple networks is a rapidly-developing subfield of graph statistics. Recent work has demonstrated that joint, or simultaneous, spectral embedding of multiple independent networks can deliver more accurate estimation than individual spectral decompositions of those same networks. Such inference procedures typically rely heavily on independence assumptions across the multiple network realizations, and even in this case, little attention has been paid to the induced network correlation that can be a consequence of such joint embeddings. In this paper, we present a generalized omnibus embedding methodology and we provide a detailed analysis of this embedding across both independent and correlated networks, the latter of which significantly extends the reach of such procedures, and we describe how this omnibus embedding can itself induce correlation. This leads us to distinguish betwee inherent correlation---that is, the correlation that arises naturally in multisample network data---and induced correlation, which is an artifice of the joint embedding methodology. We show that the generalized omnibus embedding procedure is flexible and robust, and we prove both consistency and a central limit theorem for the embedded points. We examine how induced and inherent correlation can impact inference for network time series data, and we provide network analogues of classical questions such as the effective sample size for more generally correlated data. Further, we show how an appropriately calibrated generalized omnibus embedding can detect changes in real biological networks that previous embedding procedures could not discern, confirming that the effect of inherent and induced correlation can be subtle and transformative. By allowing for and deconstructing both forms of correlation, our methodology widens the scope of spectral techniques for network inference, with import in theory and practice.",https://www.jmlr.org/papers/v23/20-944.html,,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/56139.png?t=1669566957.3576543,https://neurips.cc/virtual/2022/poster/56139,Not relevant,Other aspects,"['Spectral inference', 'multiple networks', 'joint spectral embedding', 'induced correlation', 'inherent correlation', 'network time series data', 'effective sample size', 'biological networks']",['Biological'],,,,,,
1329,https://neurips.cc/virtual/2022/poster/53382,Okapi: Generalising Better by Making Statistical Matches Match,Poster,NeurIPS,2022,"We propose Okapi, a simple, efficient, and general method for robust semi-supervised learning based on online statistical matching. Our method uses a nearest-neighbours-based matching procedure to generate cross-domain views for a consistency loss, while eliminating statistical outliers. In order to perform the online matching in a runtime- and memory-efficient way, we draw upon the self-supervised literature and combine a memory bank with a slow-moving momentum encoder. The consistency loss is applied within the feature space, rather than on the predictive distribution, making the method agnostic to both the modality and the task in question. We experiment on the WILDS 2.0 datasets Sagawa et al., which significantly expands the range of modalities, applications, and shifts available for studying and benchmarking real-world unsupervised adaptation. Contrary to Sagawa et al., we show that it is in fact possible to leverage additional unlabelled data to improve upon empirical risk minimisation (ERM) results with the right method. Our method outperforms the baseline methods in terms of out-of-distribution (OOD) generalisation on the iWildCam (a multi-class classification task) and PovertyMap (a regression task) image datasets as well as the CivilComments (a binary classification task) text dataset. Furthermore, from a qualitative perspective, we show the matches obtained from the learned encoder are strongly semantically related. Code for our paper is publicly available at https://github.com/wearepal/okapi/.",,https://openreview.net/forum?id=3wg-rYuo5AN,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53382.png?t=1669372257.9345357,https://neurips.cc/virtual/2022/poster/53382,Robustness,Defence,"['Okapi', 'semi-supervised learning', 'online statistical matching', 'nearest-neighbours-based matching', 'cross-domain views', 'consistency loss', 'statistical outliers', 'memory bank', 'slow-moving momentum encoder', 'feature space', 'modality', 'task', 'WILDS 2.0 datasets', 'Sagawa et al.', 'unsupervised adaptation', 'out-of-distribution', 'generalisation', 'iWildCam', 'PovertyMap', 'CivilComments', 'text dataset', 'empirical risk minimisation', 'ERM', 'code', 'github']","['image', 'text']",,,,,,
1330,https://neurips.cc/virtual/2022/poster/53614,You Can’t Count on Luck: Why Decision Transformers and RvS Fail in Stochastic Environments,Poster,NeurIPS,2022,"Recently, methods such as Decision Transformer that reduce reinforcement learning to a prediction task and solve it via supervised learning (RvS) have become popular due to their simplicity, robustness to hyperparameters, and strong overall performance on offline RL tasks. However, simply conditioning a probabilistic model on a desired return and taking the predicted action can fail dramatically in stochastic environments since trajectories that result in a return may have only achieved that return due to luck. In this work, we describe the limitations of RvS approaches in stochastic environments and propose a solution. Rather than simply conditioning on returns, as is standard practice, our proposed method, ESPER, conditions on learned average returns which are independent from environment stochasticity. Doing so allows ESPER to achieve strong alignment between target return and expected performance in real environments. We demonstrate this in several challenging stochastic offline-RL tasks including the challenging puzzle game 2048, and Connect Four playing against a stochastic opponent. In all tested domains, ESPER achieves significantly better alignment between the target return and achieved return than simply conditioning on returns. ESPER also achieves higher maximum performance than even the value-based baselines.",,https://openreview.net/forum?id=atb3yifRtX,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53614.png?t=1669242594.189282,https://neurips.cc/virtual/2022/poster/53614,Robustness,Defence,"['Decision Transformer', 'reinforcement learning', 'supervised learning', 'RvS', 'stochastic environments', 'ESPER', 'average returns', 'environment stochasticity', 'offline-RL tasks', '2048', 'Connect Four', 'value-based baselines']",['games'],,,,,,
1332,https://neurips.cc/virtual/2022/poster/54939,Robust Models are less Over-Confident,Poster,NeurIPS,2022,"Despite the success of convolutional neural networks (CNNs) in many academic benchmarks for computer vision tasks, their application in the real-world is still facing fundamental challenges. One of these open problems is the inherent lack of robustness, unveiled by the striking effectiveness of adversarial attacks. Current attack methods are able to manipulate the network's prediction by adding specific but small amounts of noise to the input. In turn, adversarial training (AT) aims to achieve robustness against such attacks and ideally a better model generalization ability by including adversarial samples in the trainingset. However, an in-depth analysis of the resulting robust models beyond adversarial robustness is still pending. In this paper, we empirically analyze a variety of adversarially trained models that achieve high robust accuracies when facing state-of-the-art attacks and we show that AT has an interesting side-effect: it leads to models that are significantly less overconfident with their decisions, even on clean data than non-robust models. Further, our analysis of robust models shows that not only AT but also the model's building blocks (like activation functions and pooling) have a strong influence on the models' prediction confidences. Data & Project website: https://github.com/GeJulia/robustness",,https://openreview.net/forum?id=5K3uopkizS,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54939.png?t=1669194228.5235298,https://neurips.cc/virtual/2022/poster/54939,Robustness,Defence,"['convolutional neural networks', 'adversarial attacks', 'adversarial training', 'model generalization ability', 'overconfident', 'activation functions', 'pooling', 'robustness']",['computer vision'],,,,,,
1333,https://neurips.cc/virtual/2022/poster/54574,Can Push-forward Generative Models Fit Multimodal Distributions?,Poster,NeurIPS,2022,"Many generative models synthesize data by transforming a standard Gaussian random variable using a deterministic neural network. Among these models are the Variational Autoencoders and the Generative Adversarial Networks. In this work, we call them ""push-forward"" models and study their expressivity. We formally demonstrate that the Lipschitz constant of these generative networks has to be large in order to fit multimodal distributions. More precisely, we show that the total variation distance and the Kullback-Leibler divergence between the generated and the data distribution are bounded from below by a constant depending on the mode separation and the Lipschitz constant. Since constraining the Lipschitz constants of neural networks is a common way to stabilize generative models, there is a provable trade-off between the ability of push-forward models to approximate multimodal distributions and the stability of their training. We validate our findings on one-dimensional and image datasets and empirically show that the recently introduced diffusion models do not suffer of such limitation.",,https://openreview.net/forum?id=Tsy9WCO_fK1,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54574.png?t=1669653339.1613297,https://neurips.cc/virtual/2022/poster/54574,Not relevant,Other aspects,"['Push-forward generative models', 'Variational Autoencoders', 'Generative Adversarial Networks', 'Lipschitz constant', 'Multimodal distributions', 'Total variation distance', 'Kullback-Leibler divergence', 'Mode separation', 'Diffusion models']",['None'],Not relevant,Not relevant,,,,
1339,https://neurips.cc/virtual/2022/poster/54547,Adversarial Reprogramming Revisited,Poster,NeurIPS,2022,"Adversarial reprogramming, introduced by Elsayed, Goodfellow, and Sohl-Dickstein, seeks to repurpose a neural network to perform a different task, by manipulating its input without modifying its weights.  We prove that two-layer ReLU neural networks with random weights can be adversarially reprogrammed to achieve arbitrarily high accuracy on Bernoulli data models over hypercube vertices, provided the network width is no greater than its input dimension.  We also substantially strengthen a recent result of Phuong and Lampert on directional convergence of gradient flow, and obtain as a corollary that training two-layer ReLU neural networks on orthogonally separable datasets can cause their adversarial reprogramming to fail.  We support these theoretical results by experiments that demonstrate that, as long as batch normalisation layers are suitably initialised, even untrained networks with random weights are susceptible to adversarial reprogramming.  This is in contrast to observations in several recent works that suggested that adversarial reprogramming is not possible for untrained networks to any degree of reliability.",,https://openreview.net/forum?id=F0wPem89q9y,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/9c72e0c8882794b79d65f14776a0a974.png?t=1666269132.3077526,https://neurips.cc/virtual/2022/poster/54547,Evasion,Attack,"['Adversarial reprogramming', 'Evasion attack', 'Two-layer ReLU neural networks', 'Bernoulli data models', 'Hypercube vertices', 'Width', 'Input dimension', 'Directional convergence of gradient flow', 'Orthogonally separable datasets', 'Batch normalisation', 'Untrained networks', 'Random weights']",,Other aspects,Evasion,Not relevant,Not relevant,,Wrong
1343,https://neurips.cc/virtual/2022/poster/53387,A Characterization of Semi-Supervised Adversarially Robust PAC Learnability,Poster,NeurIPS,2022,"We study the problem of learning an adversarially robust predictor to test time attacks in the semi-supervised PAC model.We address the question of how many labeled and unlabeled examples are required to ensure learning.We show that having enough unlabeled data (the size of a labeled sample that a fully-supervised method would require),the labeled sample complexity can be arbitrarily smaller compared to previous works, and is sharply characterized by a different complexity measure. We prove nearly matching upper and lower bounds on this sample complexity.This shows that there is a significant benefit in semi-supervised robust learning even in the worst-case distribution-free model, and establishes a gap between supervised and semi-supervised label complexities which is known not to hold in standard non-robust PAC learning.",,https://openreview.net/forum?id=B7Q2mbIFa6Q,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53387.png?t=1669391438.5181806,https://neurips.cc/media/neurips-2022/Slides/53387_kD9LKIh.pdf,Robustness,Defence,"['Semi-Supervised Adversarially Robust PAC Learnability', 'Adversarially robust predictor', 'labeled and unlabeled examples', 'sample complexity', 'PAC model', 'worst-case distribution-free model']",,,,,,,
1347,https://neurips.cc/virtual/2022/poster/54542,Provably sample-efficient RL with side information about latent dynamics,Poster,NeurIPS,2022,"We study reinforcement learning (RL) in settings where observations are high-dimensional, but where an RL agent has access to abstract knowledge about the structure of the state space, as is the case, for example, when a robot is tasked to go to a specific room in a building using observations from its own camera, while having access to the floor plan. We formalize this setting as transfer reinforcement learning from an ""abstract simulator,"" which we assume is deterministic (such as a simple model of moving around the floor plan), but which is only required to capture the target domain's latent-state dynamics approximately up to unknown (bounded) perturbations (to account for environment stochasticity). Crucially, we assume no prior knowledge about the structure of observations in the target domain except that they can be used to identify the latent states (but the decoding map is unknown). Under these assumptions, we present an algorithm, called TASID, that learns a robust policy in the target domain, with sample complexity that is polynomial in the horizon, and independent of the number of states, which is not possible without access to some prior knowledge. In synthetic experiments, we verify various properties of our algorithm and show that it empirically outperforms transfer RL algorithms that require access to ""full simulators"" (i.e., those that also simulate observations).",,https://openreview.net/forum?id=67NpH8-_h94,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54542.png?t=1669658070.417022,https://neurips.cc/virtual/2022/poster/54542,Robustness,Defence,"['Reinforcement learning', 'Latent dynamics', 'Transfer RL', 'Abstract simulator', 'Robust policy', 'Sample complexity', 'Horizon', 'States', 'TASID']",['Robotics'],,,,,,
1348,https://neurips.cc/virtual/2022/poster/54036,Revisiting Non-Parametric Matching Cost Volumes for  Robust and Generalizable Stereo Matching,Poster,NeurIPS,2022,"Stereo matching is a classic challenging problem in computer vision, which has recently witnessed remarkable progress by Deep Neural Networks (DNNs). This paradigm shift leads to two interesting and entangled questions that have not been addressed well. First, it is unclear whether stereo matching DNNs that are trained from scratch really learn to perform matching well. This paper studies this problem from the lens of white-box adversarial attacks. It presents a method of learning stereo-constrained photometrically-consistent attacks, which by design are weaker adversarial attacks, and yet can cause catastrophic performance drop for those DNNs. This observation suggests that they may not actually learn to perform matching well in the sense that they should otherwise achieve potentially even better after stereo-constrained perturbations are introduced. Second, stereo matching DNNs are typically trained under the simulation-to-real (Sim2Real) pipeline due to the data hungriness of DNNs. Thus, alleviating the impacts of the Sim2Real photometric gap in stereo matching DNNs becomes a pressing need.  Towards joint adversarially robust and domain generalizable stereo matching, this paper proposes to learn DNN-contextualized binary-pattern-driven non-parametric cost-volumes. It leverages the perspective of learning the cost aggregation via DNNs, and presents a simple yet expressive design that is fully end-to-end trainable, without resorting to specific aggregation inductive biases. In experiments, the proposed method is tested in the SceneFlow dataset, the KITTI2015 dataset, and the Middlebury dataset. It significantly improves the adversarial robustness, while retaining accuracy performance comparable to state-of-the-art methods. It also shows a better Sim2Real generalizability. Our code and pretrained models are released at \href{https://github.com/kelkelcheng/AdversariallyRobustStereo}{this Github Repo}.",,https://openreview.net/forum?id=WXdSp8k0TMn,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54036.png?t=1669658022.6411612,https://neurips.cc/virtual/2022/poster/54036,Evasion,Defence,"['Stereo matching', 'Deep Neural Networks', 'Adversarial attacks', 'Photometrically-consistent', 'Simulation-to-real pipeline']","['SceneFlow', 'KITTI2015', 'Middlebury']",Defence,Evasion,,,,
1351,https://neurips.cc/virtual/2022/poster/53962,The Privacy Onion Effect: Memorization is Relative,Poster,NeurIPS,2022,"Machine learning models trained on private datasets have been shown to leak their private data. Recent work has found that the average data point is rarely leaked---it is often the outlier samples that are subject to memorization and, consequently, leakage. We demonstrate and analyze an Onion Effect of memorization: removing the ""layer"" of outlier points that are most vulnerable to a privacy attack exposes a new layer of previously-safe points to the same attack. We perform several experiments that are consistent with this hypothesis. For example, we show that for membership inference attacks, when the layer of easiest-to-attack examples is removed, another layer below becomes easy-to-attack. The existence of this effect has various consequences. For example, it suggests that proposals to defend against memorization without training with rigorous privacy guarantees are unlikely to be effective. Further, it suggests that privacy-enhancing technologies such as machine unlearning could actually harm the privacy of other users.",,https://openreview.net/forum?id=ErUlLrGaVEU,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53962.png?t=1669523146.0520813,https://neurips.cc/virtual/2022/poster/53962,Data Extraction,Defence,"['privacy', 'machine learning', 'memorization', 'outlier samples', 'membership inference attacks', 'privacy-enhancing technologies', 'machine unlearning']",,Other aspects,Data Extraction,,,,Wrong
1353,https://neurips.cc/virtual/2022/poster/54527,Learning to Attack Federated Learning: A Model-based Reinforcement Learning Attack Framework,Poster,NeurIPS,2022,"We propose a model-based reinforcement learning framework to derive untargeted poisoning attacks against federated learning (FL) systems. Our framework first approximates the distribution of the clients' aggregated data using model updates from the server. The learned distribution is then used to build a simulator of the FL environment, which is utilized to learn an adaptive attack policy through reinforcement learning. Our framework is capable of learning strong attacks automatically even when the server adopts a robust aggregation rule. We further derive an upper bound on the attacker's performance loss due to inaccurate distribution estimation. Experimental results on real-world datasets demonstrate that the proposed attack framework significantly outperforms state-of-the-art poisoning attacks. This indicates the importance of developing adaptive defenses for FL systems.",,https://openreview.net/forum?id=4OHRr7gmhd4,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54527.png?t=1668362006.5271556,https://neurips.cc/virtual/2022/poster/54527,Poisoning,Attack,"['Federated learning', 'Poisoning attack', 'Reinforcement learning', 'Adaptive attack', 'Distribution estimation', 'Adaptive defenses']",['none'],,,,,,
1376,https://neurips.cc/virtual/2022/poster/53121,Capturing Graphs with Hypo-Elliptic Diffusions,Poster,NeurIPS,2022,"Convolutional layers within graph neural networks operate by aggregating information about local neighbourhood structures; one common way to encode such substructures is through random walks. The distribution of these random walks evolves according to a diffusion equation defined using the graph Laplacian. We extend this approach by leveraging classic mathematical results about hypo-elliptic diffusions. This results in a novel tensor-valued graph operator, which we call the hypo-elliptic graph Laplacian. We provide theoretical guarantees and efficient low-rank approximation algorithms. In particular, this gives a structured approach to capture long-range dependencies on graphs that is robust to pooling. Besides the attractive theoretical properties, our experiments show that this method competes with graph transformers on datasets requiring long-range reasoning but scales only linearly in the number of edges as opposed to quadratically in nodes.",,https://openreview.net/forum?id=KtDdr1zUE_1,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53121.png?t=1669601276.0831757,https://neurips.cc/virtual/2022/poster/53121,Not relevant,Not relevant,"['graph neural networks', 'random walks', 'diffusion equation', 'graph Laplacian', 'hypo-elliptic diffusions', 'tensor-valued graph operator', 'low-rank approximation algorithms', 'long-range dependencies', 'pooling', 'graph transformers']",[],,,,,,
1385,https://neurips.cc/virtual/2022/poster/53700,Cooperative Distribution Alignment via JSD Upper Bound,Poster,NeurIPS,2022,"Unsupervised distribution alignment estimates a transformation that maps two or more source distributions to a shared aligned distribution given only samples from each distribution. This task has many applications including generative modeling, unsupervised domain adaptation, and socially aware learning. Most prior works use adversarial learning (i.e., min-max optimization), which can be challenging to optimize and evaluate. A few recent works explore non-adversarial flow-based (i.e., invertible) approaches, but they lack a unified perspective and are limited in efficiently aligning multiple distributions. Therefore, we propose to unify and generalize previous flow-based approaches under a single non-adversarial framework, which we prove is equivalent to minimizing an upper bound on the Jensen-Shannon Divergence (JSD). Importantly, our problem reduces to a min-min, i.e., cooperative, problem and can provide a natural evaluation metric for unsupervised distribution alignment. We show empirical results on both simulated and real-world datasets to demonstrate the benefits of our approach. Code is available at https://github.com/inouye-lab/alignment-upper-bound.",,https://openreview.net/forum?id=X82LFUs6g5Z,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53700.png?t=1669411472.2517133,https://neurips.cc/virtual/2022/poster/53700,Not relevant,Other aspects,"['unsupervised distribution alignment', 'generative modeling', 'unsupervised domain adaptation', 'socially aware learning', 'flow-based approaches', 'Jensen-Shannon Divergence (JSD)', 'min-min optimization', 'cooperative problem']",['None'],,,,,,
1400,https://neurips.cc/virtual/2022/poster/53341,Improved techniques for deterministic l2 robustness,Poster,NeurIPS,2022,"Training convolutional neural networks (CNNs) with a strict 1-Lipschitz constraint under the l2 norm is useful for adversarial robustness, interpretable gradients and stable training. 1-Lipschitz CNNs are usually designed by enforcing each layer to have an orthogonal Jacobian matrix (for all inputs) to prevent the gradients from vanishing during backpropagation. However, their performance often significantly lags behind that of heuristic methods to enforce Lipschitz constraints where the resulting CNN is not 	extit{provably} 1-Lipschitz. In this work, we reduce this gap by introducing (a) a procedure to certify robustness of 1-Lipschitz CNNs by replacing the last linear layer with a 1-hidden layer MLP that significantly improves their performance for both standard and provably robust accuracy, (b) a method to significantly reduce the training time per epoch for Skew Orthogonal Convolution (SOC) layers (>30\% reduction for deeper networks) and (c) a class of pooling layers using the mathematical property that the l2 distance of an input to a manifold is 1-Lipschitz. Using these methods, we significantly advance the state-of-the-art for standard and provable robust accuracies on CIFAR-10 (gains of +1.79% and +3.82%) and similarly on CIFAR-100 (+3.78% and +4.75%) across all networks",,https://openreview.net/forum?id=ftKnhsDquqr,https://neurips.cc/virtual/2022/poster/53341,https://neurips.cc/virtual/2022/poster/53341,Robustness,Defence,"['Adversarial robustness', 'Convolutional neural networks', 'Lipschitz constraint', 'Orthogonal Jacobian matrix', 'Gradients', 'Backpropagation', 'Performance', 'Heuristic methods', 'Certify robustness', 'MLP', 'Accuracy', 'Skew Orthogonal Convolution', 'Training time', 'Pooling layers', 'L2 distance', 'Manifold', 'CIFAR-10', 'CIFAR-100']",[],,,,,,
1402,https://neurips.cc/virtual/2022/poster/55271,Inverse Game Theory for Stackelberg Games: the Blessing of Bounded Rationality,Poster,NeurIPS,2022,"Optimizing strategic decisions (a.k.a. computing equilibrium) is key to the success of many non-cooperative multi-agent applications. However, in many real-world situations, we may face the exact opposite of this game-theoretic problem --- instead of prescribing equilibrium of a given game, we may directly observe the agents' equilibrium behaviors but want to infer the underlying parameters of an unknown game. This research question, also known as inverse game theory, has been studied in multiple recent works in the context of Stackelberg games. Unfortunately, existing works exhibit quite negative results, showing statistical hardness and computational hardness, assuming follower's perfectly rational behaviors. Our work relaxes the perfect rationality agent assumption to the classic quantal response model, a more realistic behavior model of bounded rationality. Interestingly, we show that the smooth property brought by such bounded rationality model actually leads to provably more efficient learning of the follower utility parameters in general Stackelberg games. Systematic empirical experiments on synthesized games confirm our theoretical results and further suggest its robustness beyond the strict quantal response model.",,https://openreview.net/forum?id=ymAsTHhrnGm,https://neurips.cc/virtual/2022/poster/55271,https://neurips.cc/virtual/2022/poster/55271,Other attack,Defence,"['Inverse game theory', 'Stackelberg games', 'Bounded rationality', 'Equilibrium', 'Multi-agent', 'Quantal response model', 'Robustness']",,,,,,,
1404,https://neurips.cc/virtual/2022/poster/53456,Revisiting Active Sets for Gaussian Process Decoders,Poster,NeurIPS,2022,"Decoders built on Gaussian processes (GPs) are enticing due to the marginalisation over the non-linear function space. Such models (also known as GP-LVMs) are often expensive and notoriously difficult to train in practice, but can be scaled using variational inference and inducing points. In this paper, we revisit active set approximations. We develop a new stochastic estimate of the log-marginal likelihood based on recently discovered links to cross-validation, and we propose a computationally efficient approximation thereof. We demonstrate that the resulting stochastic active sets (SAS) approximation significantly improves the robustness of GP decoder training, while reducing computational cost. The SAS-GP obtains more structure in the latent space, scales to many datapoints, and learns better representations than variational autoencoders, which is rarely the case for GP decoders.",,https://openreview.net/forum?id=rAVqc7KSGDa,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/a2790947391a51d18dc235eea344d981.png?t=1666447191.6738708,https://neurips.cc/virtual/2022/poster/53456,Not relevant,Defence,"['Active sets', 'Gaussian processes', 'Decoders', 'Variational inference', 'Inducing points', 'Stochastic estimate', 'Log-marginal likelihood', 'Cross-validation']",,,,,,,
1408,https://neurips.cc/virtual/2022/poster/54294,FedPop: A Bayesian Approach for Personalised Federated Learning,Poster,NeurIPS,2022,"Personalised federated learning (FL) approaches aim at collaboratively learning a machine learning model taylored for each client. Albeit promising advances have been made in this direction, most of existing personalised FL works do not allow for uncertainty quantification which is crucial in many applications. In addition, personalisation in the cross-device setting still involves important issues, especially for new clients or those having small data sets. This paper aims at filling this gap. To this end, we propose a novel methodology coined FedPop by recasting personalised FL into the population modeling paradigm where clients' models involve fixed common population parameters and random individual ones, aiming at explaining data heterogeneity. To derive convergence guarantees for our scheme, we introduce a new class of federated stochastic optimisation algorithms which relies on Markov chain Monte Carlo methods. Compared to existing personalised FL methods, the proposed methodology has important benefits: it is robust to client drift, practical for inference on new clients, and above all, enables uncertainty quantification under mild computational and memory overheads. We provide non-asymptotic convergence guarantees for the proposed algorithms and illustrate their performances on various personalised federated learning tasks.",,https://openreview.net/forum?id=KETwimTQexH,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54294.png?t=1669406983.094083,https://neurips.cc/virtual/2022/poster/54294,Not relevant,Defence,"['Federated Learning', 'Bayesian Approach', 'Personalised', 'Uncertainty Quantification', 'Population Modeling', 'Markov Chain Monte Carlo Methods', 'Client Drift', 'Inference']",['Machine learning'],,,,,,
1427,https://neurips.cc/virtual/2022/poster/55301,Dataset Inference for Self-Supervised Models,Poster,NeurIPS,2022,"Self-supervised models are increasingly prevalent in machine learning (ML) since they reduce the need for expensively labeled data. Because of their versatility in downstream applications, they are increasingly used as a service exposed via public APIs. At the same time, these encoder models are particularly vulnerable to model stealing attacks due to the high dimensionality of vector representations they output. Yet, encoders remain undefended: existing mitigation strategies for stealing attacks focus on supervised learning. We introduce a new dataset inference defense, which uses the private training set of the victim encoder model to attribute its ownership in the event of stealing. The intuition is that the log-likelihood of an encoder's output representations is higher on the victim's training data than on test data if it is stolen from the victim, but not if it is independently trained. We compute this log-likelihood using density estimation models. As part of our evaluation, we also propose measuring the fidelity of stolen encoders and quantifying the effectiveness of the theft detection without involving downstream tasks; instead, we leverage mutual information and distance measurements. Our extensive empirical results in the vision domain demonstrate that dataset inference is a promising direction for defending self-supervised models against model stealing.",,https://openreview.net/forum?id=CCBJf9xJo2X,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/51ef186e18dc00c2d31982567235c559.png?t=1666294804.8355207,https://neurips.cc/virtual/2022/poster/55301,Model Extraction,Defence,"['Self-supervised models', 'Machine learning', 'Model stealing attack', 'Encoder', 'Mitigation strategies', 'Density estimation models', 'Theft detection', 'Mutual information', 'Distance measurements', 'Vision domain']",['vision'],Defence,Model Extraction,,,,
1430,https://neurips.cc/virtual/2022/poster/56084,[Re] Exacerbating Algorithmic Bias through Fairness Attacks,Poster,NeurIPS,2022,"The presented study evaluates ''Exacerbating Algorithmic Bias through Fairness Attacks'' by Mehrabi et al. (2021) within the scope of the ML Reproducibility Challenge 2021. We find it not possible to reproduce the original results from sole use of the paper, and difficult even in possession of the provided codebase. Yet, we managed to obtain similar findings that supported three out of the five main claims of the publication, albeit using partial re-implementations and numerous assumptions. On top of the reproducibility study, we also extend the work of the authors by implementing a different stopping method, which changes the effectiveness of the proposed attacks.",https://doi.org/10.5281/zenodo.6574669,,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/56084.png?t=1669032290.7711475,https://neurips.cc/media/neurips-2022/Slides/56084.pdf,Other attack,Attack,"['Algorithmic Bias', 'Fairness Attacks', 'ML Reproducibility Challenge', 'reproduce results', 're-implementations', 'stopping method', 'attack effectiveness']",['None'],,,,,,
1434,https://neurips.cc/virtual/2022/poster/54849,BagFlip: A Certified Defense Against Data Poisoning,Poster,NeurIPS,2022,"Machine learning models are vulnerable to data-poisoning attacks, in which an attacker maliciously modifies the training set to change the prediction of a learned model. In a trigger-less attack, the attacker can modify the training set but not the test inputs, while in a backdoor attack the attacker can also modify test inputs. Existing model-agnostic defense approaches either cannot handle backdoor attacks or do not provide effective certificates (i.e., a proof of a defense). We present BagFlip, a model-agnostic certified approach that can effectively defend against both trigger-less and backdoor attacks. We evaluate BagFlip on image classification and malware detection datasets. BagFlip is equal to or more effective than the state-of-the-art approaches for trigger-less attacks and more effective than the state-of-the-art approaches for backdoor attacks.",,https://openreview.net/forum?id=ZidkM5b92G,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54849.png?t=1668446124.7749634,https://neurips.cc/virtual/2022/poster/54849,Poisoning,Defence,"['data-poisoning attacks', 'trigger-less attack', 'backdoor attack', 'model-agnostic defense', 'certified approach', 'image classification', 'malware detection']",Not clear,,,,,,
1442,https://neurips.cc/virtual/2022/poster/53130,Overparameterization from Computational Constraints,Poster,NeurIPS,2022,"Overparameterized models with millions of parameters have been hugely successful. In this work, we ask:  can the need for large models be, at least in part, due to the \emph{computational} limitations of the learner? Additionally, we ask, is this situation exacerbated for \emph{robust} learning? We show that this indeed could be the case. We show learning tasks for which computationally bounded learners need \emph{significantly more} model parameters than what information-theoretic learners need. Furthermore, we show that even more model parameters could be necessary for robust learning. In particular, for computationally bounded learners, we extend the recent result of Bubeck and Sellke [NeurIPS'2021] which shows that robust models might need more parameters, to the computational regime and show that bounded learners could provably need an even larger number of parameters. Then, we address the following related question: can we hope to remedy the situation for robust computationally bounded learning by restricting \emph{adversaries} to also be computationally bounded for sake of obtaining models with fewer parameters? Here again, we show that this could be possible. Specifically, building on the work of Garg, Jha, Mahloujifar, and Mahmoody [ALT'2020], we demonstrate a learning task that can be learned efficiently and robustly against a computationally bounded attacker, while to be robust against an information-theoretic attacker requires the learner to utilize significantly more parameters.",,https://openreview.net/forum?id=7uIGl1AB_M_,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53130.png?t=1669527414.9600084,https://neurips.cc/virtual/2022/poster/53130,Robustness,Defence,"['Overparameterization', 'Computational Constraints', 'Robust learning', 'Information-theoretic learners', 'Bounded learners', 'Adversaries', 'Learning task']",,,,,,,
1443,https://neurips.cc/virtual/2022/poster/56095,[Re] Transparent Object Tracking Benchmark,Poster,NeurIPS,2022,"Scope of Reproducibility In the article, the authors of the Transparent Object Tracking Benchmark compare the performance of 25 state-of-the-art tracking algorithms, evaluated on the TOTB dataset, with a new proposed algorithm for tracking transparent objects called TransATOM. Authors claim that it outperforms all other state-of-the-art algorithms. They highlight the effectiveness and advantage of transparency feature for transparent object tracking. They also do a qualitative evaluation of each tracking algorithm on various typical challenges such as rotation, scale variation etc.
Methodology In addition to the TransAtom tracker, we chose ten, best performing on TOTB dataset, state-of-the-art tracking algorithms to evaluate on the TOTB dataset using a set of standard evaluation tools. On different sequences, we performed a qualitative evaluation of each tracking algorithm and thoroughly compared the ATOM tracker to the TransATOM tracker. We did not implement the trackers from scratch, but instead used GitHub implementations. TOTB dataset had to be integrated into some of the standard evaluation tools. We used an internal server with an Ubuntu 18.04 operating system and a TITAN X graphics card to reproduce the results.
Results The tracking performance was reproduced in terms of success, precision, and normalized precision, and the reported value is in the 95 percent confidence interval, which supports the paper's conclusion that TransATOM significantly outperforms other state-of-the-art algorithms on TOTB database. Also, it supports a claim that including a transparency feature in the tracker improves performance when tracking transparent objects. However, we refuted the claim that TransATOM well handles all challenges for robust target localization.
What was easy The evaluation of the tracking results and comparison of different trackers with each other was a simple part of the reproduction because the implementation in Matlab is very robust and works for different formats of tracker results.
What was difficult The most difficult aspect of the replication was integrating the TOTB dataset into various standard evaluation tools and running all trackers on this dataset. The reason for this is that each tool requires its own dataset format, and it was also difficult to set up so many different tracker environments. It also took a long time to run all of the trackers because some of them are quite slow and the TOTB dataset is quite large. The deprecation of different packages was also a problem for some trackers, necessitating extensive debugging.
Communication with original authors We communicated with the author via email. The author provided us with feedback that helped us reproduce the results more accurately.",https://rescience.github.io/bibliography/Trojer_2022.html,,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/9517da358c0cd7ceeceb27e06a10b8c7.png?t=1667759722.6760821,https://neurips.cc/virtual/2022/poster/56095,Not relevant,Defence,"['Transparent object tracking', 'Transparent object tracking benchmark', 'Transparent object tracking algorithm', 'Transparent object tracking dataset', 'Transparent object tracking performance', 'State-of-the-art tracking algorithms', 'TransATOM', 'Transparent object tracking evaluation', 'Transparent object tracking comparison', 'Transparent object tracking challenges', 'Transparent object tracking robust target localization']",['Computer Vision'],,,,,,
1452,https://neurips.cc/virtual/2022/poster/52876,Adversarial training for high-stakes reliability,Poster,NeurIPS,2022,"In the future, powerful AI systems may be deployed in high-stakes settings, where a single failure could be catastrophic. One technique for improving AI safety in high-stakes settings is adversarial training, which uses an adversary to generate examples to train on in order to achieve better worst-case performance.In this work, we used a safe language generation task (``avoid injuries'') as a testbed for achieving high reliability through adversarial training. We created a series of adversarial training techniques---including a tool that assists human adversaries---to find and eliminate failures in a classifier that filters text completions suggested by a generator. In our task, we determined that we can set very conservative classifier thresholds without significantly impacting the quality of the filtered outputs.  We found that adversarial training significantly increased robustness to the adversarial attacks that we trained on--- tripling the time to find adversarial examples without tools and doubling the time with our tool (from 13 to 26 minutes)---without affecting in-distribution performance.  We hope to see further work in the high-stakes reliability setting, including more powerful tools for enhancing human adversaries and better ways to measure high levels of reliability, until we can confidently rule out the possibility of catastrophic deployment-time failures of powerful models.",,https://openreview.net/forum?id=NtJyGXo0nF,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/2f3680790ac607007e3443a317871dd5.png?t=1667283757.4115686,https://neurips.cc/media/neurips-2022/Slides/52876.pdf,Evasion,Defence,"['Adversarial training', 'high-stakes reliability', 'AI safety', 'AI systems', 'catastrophic', 'safe language generation task', 'avoid injuries', 'testbed', 'classifier', 'filters text completions', 'generator', 'conservative classifier thresholds', 'robustness', 'adversarial attacks', 'tools', 'measure high levels of reliability']",['None'],,,,,,
1459,https://neurips.cc/virtual/2022/poster/54738,SInGE: Sparsity via Integrated Gradients Estimation of Neuron Relevance,Poster,NeurIPS,2022,"The leap in performance in state-of-the-art computer vision methods is attributed to the development of deep neural networks. However it often comes at a computational price which may hinder their deployment. To alleviate this limitation, structured pruning is a well known technique which consists in removing channels, neurons or filters, and is commonly applied in order to produce more compact models. In most cases, the computations to remove are selected based on a relative importance criterion. At the same time, the need for explainable predictive models has risen tremendously and motivated the development of robust attribution methods that highlight the relative importance of pixels of an input image or feature map. In this work, we discuss the limitations of existing pruning heuristics, among which magnitude and gradient-based methods. We draw inspiration from attribution methods to design a novel integrated gradient pruning criterion, in which the relevance of each neuron is defined as the integral of the gradient variation on a path towards this neuron removal. Furthermore, We propose an entwined DNN pruning and fine-tuning flowchart to better preserve DNN accuracy while removing parameters. We show through extensive validation on several datasets, architectures as well as pruning scenarios that the proposed method, dubbed SInGE, significantly outperforms existing state-of-the-art DNN pruning methods.",,https://openreview.net/forum?id=oQIJsMlyaW_,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/416849da96fb73bee793e2bf65ae43ac.png?t=1667568021.7199216,https://neurips.cc/virtual/2022/poster/54738,Robustness,Defence,"['Structured pruning', 'Deep Neural Networks', 'Explainable predictive models', 'Attribution methods', 'Integrated gradient pruning criterion', 'Neuron relevance', 'DNN pruning', 'Fine-tuning flowchart', 'DNN accuracy']",['Computer Vision'],,,,,,
1463,https://neurips.cc/virtual/2022/poster/55342,Towards Robust Blind Face Restoration with Codebook Lookup Transformer,Poster,NeurIPS,2022,"Blind face restoration is a highly ill-posed problem that often requires auxiliary guidance to 1) improve the mapping from degraded inputs to desired outputs, or 2) complement high-quality details lost in the inputs. In this paper, we demonstrate that a learned discrete codebook prior in a small proxy space largely reduces the uncertainty and ambiguity of restoration mapping by casting \textit{blind face restoration} as a \textit{code prediction} task, while providing rich visual atoms for generating high-quality faces. Under this paradigm, we propose a Transformer-based prediction network, named \textit{CodeFormer}, to model the global composition and context of the low-quality faces for code prediction, enabling the discovery of natural faces that closely approximate the target faces even when the inputs are severely degraded. To enhance the adaptiveness for different degradation, we also propose a controllable feature transformation module that allows a flexible trade-off between fidelity and quality. Thanks to the expressive codebook prior and global modeling, \textit{CodeFormer} outperforms the state of the arts in both quality and fidelity, showing superior robustness to degradation. Extensive experimental results on synthetic and real-world datasets verify the effectiveness of our method.",,https://openreview.net/forum?id=XdDl3bFUNn5,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55342.png?t=1669794991.6542788,https://neurips.cc/virtual/2022/poster/55342,Other attack,Defence,"['Blind face restoration', 'Codebook prior', 'Transformer-based prediction network', 'Code prediction', 'Low-quality faces', 'Adaptiveness']",['Computer vision'],,,,,,
1465,https://neurips.cc/virtual/2022/poster/55162,ClimbQ: Class Imbalanced Quantization Enabling Robustness on Efficient Inferences,Poster,NeurIPS,2022,"Quantization compresses models to low bits for efficient inferences which has received increasing attentions. However, existing approaches focused on balanced datasets, while imbalanced data is pervasive in the real world. Therefore, in this study, we investigate the realistic problem, quantization on class-imbalanced data. We observe from the analytical results that quantizing imbalanced data tends to obtain a large error due to the differences between separate class distributions, which leads to a significant accuracy loss. To address this issue, we propose a novel quantization framework, Class Imbalanced Quantization (ClimbQ) that focuses on diminishing the inter-class heterogeneity for quantization error reduction. ClimbQ first scales the variance of each class distribution and then projects data through the new distributions to the same space for quantization. To guarantee the homogeneity of class variances after the ClimbQ process, we examine the quantized features and derive that the homogeneity satisfies when data size for each class is restricted (bounded). Accordingly, we design a Homogeneous Variance Loss (HomoVar Loss) which reweights the data losses of each class based on the bounded data sizes to satisfy the homogeneity of class variances. Extensive experiments on class-imbalanced and benchmark balanced datasets reveal that ClimbQ outperforms the state-of-the-art quantization techniques, especially on highly imbalanced data. ",,https://openreview.net/forum?id=F7NQzsl334D,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/46031b3d04dc90994ca317a7c55c4289.png?t=1666752828.3865807,https://neurips.cc/virtual/2022/poster/55162,Robustness,Defence,"['Quantization', 'class-imbalanced data', 'error reduction', 'inter-class heterogeneity', 'homogeneity', 'class variances', 'loss reweighting', 'efficient inferences']",,,,,,,
1482,https://neurips.cc/virtual/2022/poster/54967,Generative Neural Articulated Radiance Fields,Poster,NeurIPS,2022,"Unsupervised learning of 3D-aware generative adversarial networks (GANs) using only collections of single-view 2D photographs has very recently made much progress. These 3D GANs, however, have not been demonstrated for human bodies and the generated radiance fields of existing frameworks are not directly editable, limiting their applicability in downstream tasks. We propose a solution to these challenges by developing a 3D GAN framework that learns to generate radiance fields of human bodies or faces in a canonical pose and warp them using an explicit deformation field into a desired body pose or facial expression. Using our framework, we demonstrate the first high-quality radiance field generation results for human bodies. Moreover, we show that our deformation-aware training procedure significantly improves the quality of generated bodies or faces when editing their poses or facial expressions compared to a 3D GAN that is not trained with explicit deformations.",,https://openreview.net/forum?id=_keb_XuP5oI,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54967.png?t=1669613272.348662,https://neurips.cc/virtual/2022/poster/54967,Not relevant,Attack,"['Generative adversarial networks', '3D-aware', 'Unsupervised learning', 'Radiance fields', 'Human bodies', 'Facial expression', 'Deformation-aware training', '3D GAN', 'Downstream tasks']","['human body', 'facial expression']",,,,,,
1487,https://neurips.cc/virtual/2022/poster/53830,Distributionally Adaptive Meta Reinforcement Learning,Poster,NeurIPS,2022,"Meta-reinforcement learning algorithms provide a data-driven way to acquire policies that quickly adapt to many tasks with varying rewards or dynamics functions. However, learned meta-policies are often effective only on the exact task distribution on which they were trained and struggle in the presence of distribution shift of test-time rewards or transition dynamics. In this work, we develop a framework for meta-RL algorithms that are able to behave appropriately under test-time distribution shifts in the space of tasks. Our framework centers on an adaptive approach to distributional robustness that trains a population of meta-policies to be robust to varying levels of distribution shift. When evaluated on a potentially shifted test-time distribution of tasks, this allows us to choose the meta-policy with the most appropriate level of robustness, and use it to perform fast adaptation. We formally show how our framework allows for improved regret under distribution shift, and empirically show its efficacy on simulated robotics problems under a wide range of distribution shifts.",,https://openreview.net/forum?id=rOimdw0-sx9,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53830.png?t=1669517905.6376574,https://neurips.cc/virtual/2022/poster/53830,Robustness,Defence,"['Meta-reinforcement learning', 'Distributional robustness', 'Adaptive approach', 'Population of meta-policies', 'Test-time distribution shift', 'Regret']",Simulated robotics,,,,,,
1489,https://neurips.cc/virtual/2022/poster/54420,Robust Bayesian Regression via Hard Thresholding,Poster,NeurIPS,2022,"By combining robust regression and prior information, we develop an effective robust regression method that can resist adaptive adversarial attacks. Due to the widespread existence of noise and data corruption, it is necessary to recover the true regression parameters when a certain proportion of the response variables have been corrupted. Methods to overcome this problem often involve robust least-squares regression. However, few methods achieve good performance when dealing with severe adaptive adversarial attacks. Based on the combination of prior information and robust regression via hard thresholding, this paper proposes an algorithm that improves the breakdown point when facing adaptive adversarial attacks. Furthermore, to improve the robustness and reduce the estimation error caused by the inclusion of a prior, the idea of Bayesian reweighting is used to construct a more robust algorithm. We prove the theoretical convergence of proposed algorithms under mild conditions. Extensive experiments show that, under different dataset attacks, our algorithms achieve state-of-the-art results compared with other benchmark algorithms, demonstrating the robustness of the proposed approach.",,https://openreview.net/forum?id=krV1UM7Uw1,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54420.png?t=1669130917.8408,https://neurips.cc/media/neurips-2022/Slides/54420.pdf,Evasion,Defence,"['Robust Bayesian Regression', 'Hard Thresholding', 'Adaptive adversarial attacks', 'Robust least-squares regression', 'Breakdown point', 'Bayesian reweighting', 'Convergence', 'Dataset attacks']",,Defence,Evasion,,,,
1510,https://neurips.cc/virtual/2022/poster/53315,RényiCL: Contrastive Representation Learning with Skew Rényi Divergence,Poster,NeurIPS,2022,"Contrastive representation learning seeks to acquire useful representations by estimating the shared information between multiple views of data. Here, the choice of data augmentation is sensitive to the quality of learned representations: as harder the data augmentations are applied, the views share more task-relevant information, but also task-irrelevant one that can hinder the generalization capability of representation. Motivated by this, we present a new robust contrastive learning scheme, coined RényiCL, which can effectively manage harder augmentations by utilizing Rényi divergence. Our method is built upon the variational lower bound of a Rényi divergence, but a naive usage of a variational method exhibits unstable training due to the large variance. To tackle this challenge, we propose a novel contrastive objective that conducts variational estimation of a skew Renyi divergence and provides a theoretical guarantee on how variational estimation of skew divergence leads to stable training.  We show that Rényi contrastive learning objectives perform innate hard negative sampling and easy positive sampling simultaneously so that it can selectively learn useful features and ignore nuisance features. Through experiments on ImageNet, we show that Rényi contrastive learning with stronger augmentations outperforms other self-supervised methods without extra regularization or computational overhead. Also, we validate our method on various domains such as graph and tabular datasets, showing empirical gain over original contrastive methods. ",,https://openreview.net/forum?id=73h4EZYtSht,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53315.png?t=1669207934.043248,https://neurips.cc/virtual/2022/poster/53315,Not relevant,Defence,"['contrastive representation learning', 'Rényi divergence', 'variational lower bound', 'skew Renyi divergence', 'hard negative sampling', 'easy positive sampling', 'ImageNet', 'graph', 'tabular datasets']",['image classification'],,,,,,
1515,https://neurips.cc/virtual/2022/poster/55158,Sequencer: Deep LSTM for Image Classification,Poster,NeurIPS,2022,"In recent computer vision research, the advent of the Vision Transformer (ViT) has rapidly revolutionized various architectural design efforts: ViT achieved state-of-the-art image classification performance using self-attention found in natural language processing, and MLP-Mixer achieved competitive performance using simple multi-layer perceptrons. In contrast, several studies have also suggested that carefully redesigned convolutional neural networks (CNNs) can achieve advanced performance comparable to ViT without resorting to these new ideas. Against this background, there is growing interest in what inductive bias is suitable for computer vision. Here we propose Sequencer, a novel and competitive architecture alternative to ViT that provides a new perspective on these issues. Unlike ViTs, Sequencer models long-range dependencies using LSTMs rather than self-attention layers. We also propose a two-dimensional version of Sequencer module, where an LSTM is decomposed into vertical and horizontal LSTMs to enhance performance. Despite its simplicity, several experiments demonstrate that Sequencer performs impressively well: Sequencer2D-L, with 54M parameters, realizes 84.6% top-1 accuracy on only ImageNet-1K. Not only that, we show that it has good transferability and the robust resolution adaptability on double resolution-band. solution-band. Our source code is available at https://github.com/okojoalg/sequencer.",,https://openreview.net/forum?id=wlrYnGZ37Wv,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/712a3c9878efeae8ff06d57432016ceb.png?t=1667502434.775794,https://neurips.cc/virtual/2022/poster/55158,Not relevant,Not relevant,"['Image classification', 'Vision Transformer', 'MLP-Mixer', 'Convolutional neural networks', 'Inductive bias', 'Computer vision', 'LSTM', 'Self-attention', 'ImageNet-1K']",['Computer vision'],,,,,,
1517,https://neurips.cc/virtual/2022/poster/55303,GenSDF: Two-Stage Learning of Generalizable Signed Distance Functions,Poster,NeurIPS,2022,"We investigate the generalization capabilities of neural signed distance functions (SDFs) for learning 3D object representations for unseen and unlabeled point clouds. Existing methods can fit SDFs to a handful of object classes and boast fine detail or fast inference speeds, but do not generalize well to unseen shapes. We introduce a two-stage semi-supervised meta-learning approach that transfers shape priors from labeled to unlabeled data to reconstruct unseen object categories. The first stage uses an episodic training scheme to simulate training on unlabeled data and meta-learns initial shape priors. The second stage then introduces unlabeled data with disjoint classes in a semi-supervised scheme to diversify these priors and achieve generalization. We assess our method on both synthetic data and real collected point clouds. Experimental results and analysis validate that our approach outperforms existing neural SDF methods and is capable of robust zero-shot inference on 100+ unseen classes. Code can be found at https://github.com/princeton-computational-imaging/gensdf",,https://openreview.net/forum?id=QK38rpF8RWL,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/ede7e2b6d13a41ddf9f4bdef84fdc737.png?t=1667232711.669387,https://neurips.cc/virtual/2022/poster/55303,Not relevant,Defence,"['signed distance function', 'neural', '3D object representations', 'point clouds', 'generalization', 'meta-learning', 'episodic training', 'semi-supervised', 'shape priors', 'zero-shot inference']",['3D object representation'],,,,,,
1526,https://neurips.cc/virtual/2022/poster/53958,The Neural Testbed: Evaluating Joint Predictions,Poster,NeurIPS,2022,"Predictive distributions quantify uncertainties ignored by point estimates. This paper introduces The Neural Testbed: an open source benchmark for controlled and principled evaluation of agents that generate such predictions. Crucially, the testbed assesses agents not only on the quality of their marginal predictions per input, but also on their joint predictions across many inputs. We evaluate a range of agents using a simple neural network data generating process.Our results indicate that some popular Bayesian deep learning agents do not fare well with joint predictions, even when they can produce accurate marginal predictions. We also show that the quality of joint predictions drives performance in downstream decision tasks. We find these results are robust across choice a wide range of generative models, and highlight the practical importance of joint predictions to the community.",,https://openreview.net/forum?id=JyTT03dqCFD,https://neurips.cc/virtual/2022/poster/53958,https://neurips.cc/virtual/2022/poster/53958,Not relevant,Defence,"['Neural Testbed', 'Evaluating Joint Predictions', 'Predictive distributions', 'Agents', 'Quality of marginal predictions', 'Joint predictions', 'Simple neural network data generating process', 'Bayesian deep learning agents', 'Downstream decision tasks']",[],,,,,,
1531,https://neurips.cc/virtual/2022/poster/53131,Quality Not Quantity: On the Interaction between Dataset Design and Robustness of CLIP,Poster,NeurIPS,2022,"Web-crawled datasets have enabled remarkable generalization capabilities in recent image-text models such as CLIP (Contrastive Language-Image pre-training) or Flamingo, but little is known about the dataset creation processes. In this work, we introduce a testbed of six publicly available data sources---YFCC, LAION, Conceptual Captions, WIT, RedCaps, Shutterstock---to investigate how pre-training distributions induce robustness in CLIP. We find that the performance of the pre-training data varies substantially across distribution shifts, with no single data source dominating. Moreover, we systematically study the interactions between these data sources and find that mixing multiple sources does not necessarily yield better models, but rather dilutes the robustness of the best individual data source. We complement our empirical findings with theoretical insights from a simple setting, where combining the training data also results in diluted robustness. In addition, our theoretical model provides a candidate explanation for the success of the CLIP-based data filtering technique recently employed in the LAION dataset. Overall our results demonstrate that simply gathering a large amount of data from the web is not the most effective way to build a pre-training dataset for robust generalization, necessitating further study into dataset design. Code is available at https://github.com/mlfoundations/clip",,https://openreview.net/forum?id=LTCBavFWp5C,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53131.png?t=1669697425.496368,https://neurips.cc/virtual/2022/poster/53131,Robustness,Defence,"['CLIP', 'Dataset design', 'Robustness', 'Pre-training', 'Generalization', 'Distribution shifts', 'Web-crawled datasets', 'Contrastive Language-Image pre-training', 'Flamingo', 'Data sources', 'YFCC', 'LAION', 'Conceptual Captions', 'WIT', 'RedCaps', 'Shutterstock']",,,,,,,
1533,https://neurips.cc/virtual/2022/invited-talk/56158,The Data-Centric Era: How ML is Becoming an Experimental Science,Invited Talk,NeurIPS,2022,"NeurIPS has been in existence for more than 3 decades, each one marked by a dominant trend. The pioneering years saw the burgeoning of back-prop nets, the coming-of-age years blossomed with convex optimization, regularization, Bayesian methods, boosting, kernel methods, to name a few, and the junior years have been dominated by deep nets and big data. And now, recent analyses conclude that using ever bigger data and deeper networks is not a sustainable way of progressing. Meanwhile, other indicators show that Machine Learning is increasingly reliant upon good data and benchmarks, not only to train more powerful and/or more compact models, but also to soundly evaluate new ideas and to stress test models on their reliability, fairness, and protection against various attacks, including privacy attacks.",,,https://neurips.cc/virtual/2022/invited-talk/56158,https://neurips.cc/virtual/2022/invited-talk/56158,Not relevant,Other aspects,"['Machine Learning', 'Experimental Science', 'Trends', 'NeurIPS', 'Back-prop nets', 'Convex optimization', 'Regularization', 'Bayesian methods', 'Boosting', 'Kernel methods', 'Deep nets', 'Big data', 'Data and benchmarks', 'Reliability', 'Fairness', 'Protection against various attacks', 'Privacy attacks']",,,,,,,
1534,https://neurips.cc/virtual/2022/poster/53027,Subgroup Robustness Grows On Trees: An Empirical Baseline Investigation,Poster,NeurIPS,2022,"Researchers have proposed many methods for fair and robust machine learning, but thorough empirical evaluation of their subgroup robustness is lacking. In this work, we address this gap in the context of tabular data, where sensitive subgroups are clearly-defined, real-world fairness problems abound, and prior works often fail to compare to state-of-the-art tree-based models. We conduct an empirical comparison of several previously-proposed methods for fair and robust learning alongside state-of-the-art tree-based methods and other baselines. Via experiments with more than 340,000 model configurations on eight datasets, we show that tree-based methods have strong subgroup robustness, even when compared to robustness- and fairness-enhancing methods. Moreover, the best tree-based models tend to show good performance over a range of metrics, while robust or group-fair models can show brittleness, with significant performance differences across different metrics for a fixed model. We also demonstrate that tree-based models show less sensitivity to hyperparameter configurations, and are less costly to train. Our work suggests that tree-based ensemble models make an effective baseline for tabular data, and are a sensible default when subgroup robustness is desired.",,https://openreview.net/forum?id=6QvmtRjWNRy,https://neurips.cc/virtual/2022/poster/53027,https://neurips.cc/virtual/2022/poster/53027,Robustness,Defence,"['fairness', 'robustness', 'machine learning', 'tabular data', 'tree-based models', 'ensemble models', 'sensitive subgroups', 'empirical evaluation']",[''],,,,,,
1540,https://neurips.cc/virtual/2022/poster/53626,Sub-exponential time Sum-of-Squares lower bounds for Principal Components Analysis,Poster,NeurIPS,2022,"Principal Components Analysis (PCA) is a dimension-reduction technique widely used in machine learning and statistics. However, due to the dependence of the principal components on all the dimensions, the components are notoriously hard to interpret. Therefore, a variant known as sparse PCA is often preferred. Sparse PCA learns principal components of the data but enforces that such components must be sparse. This has applications in diverse fields such as computational biology and image processing. To learn sparse principal components, it's well known that standard PCA will not work, especially in high dimensions, and therefore algorithms for sparse PCA are often studied as a separate endeavor. Various algorithms have been proposed for Sparse PCA over the years, but given how fundamental it is for applications in science, the limits of efficient algorithms are only partially understood. In this work, we study the limits of the powerful Sum of Squares (SoS) family of algorithms for Sparse PCA. SoS algorithms have recently revolutionized robust statistics, leading to breakthrough algorithms for long-standing open problems in machine learning, such as optimally learning mixtures of gaussians, robust clustering, robust regression, etc. Moreover, it is believed to be the optimal robust algorithm for many statistical problems. Therefore, for sparse PCA, it's plausible that it can beat simpler algorithms such as diagonal thresholding that have been traditionally used. In this work, we show that this is not the case, by exhibiting strong tradeoffs between the number of samples required, the sparsity and the ambient dimension, for which SoS algorithms, even if allowed sub-exponential time, will fail to optimally recover the component. Our results are complemented by known algorithms in literature, thereby painting an almost complete picture of the behavior of efficient algorithms for sparse PCA. Since SoS algorithms encapsulate many algorithmic techniques such as spectral or statistical query algorithms, this solidifies the message that  known algorithms are optimal for sparse PCA. Moreover, our techniques are strong enough to obtain similar tradeoffs for Tensor PCA, another important higher order variant of PCA with applications in topic modeling, video processing, etc.",,https://openreview.net/forum?id=D45iCWZYcff,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/b5200c6107fc3d41d19a2b66835c3974.png?t=1667246918.7899923,https://neurips.cc/virtual/2022/poster/53626,Not relevant,Defence,"['Principal Components Analysis', 'Sparse PCA', 'Sum-of-Squares', 'Robust statistics', 'Machine learning', 'Computational biology', 'Image processing', 'Sparse recovery', 'Tensor PCA', 'Topic modeling', 'Video processing']","['Computational biology', 'Image processing', 'Topic modeling', 'Video processing']",,,,,,
1549,https://neurips.cc/virtual/2022/poster/53802,A General Framework for Auditing Differentially Private Machine Learning,Poster,NeurIPS,2022,"We present a framework to statistically audit the privacy guarantee conferred by a differentially private machine learner in practice. While previous works have taken steps toward evaluating privacy loss through poisoning attacks or membership inference, they have been tailored to specific models or have demonstrated low statistical power. Our work develops a general methodology to empirically evaluate the privacy of differentially private machine learning implementations, combining improved privacy search and verification methods with a toolkit of influence-based poisoning attacks. We demonstrate significantly improved auditing power over previous approaches on a variety of models including logistic regression, Naive Bayes, and random forest. Our method can be used to detect privacy violations due to implementation errors or misuse. When violations are not present, it can aid in understanding the amount of information that can be leaked from a given dataset, algorithm, and privacy specification.",,https://openreview.net/forum?id=AKM3C3tsSx3,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53802.png?t=1669536494.2912822,https://neurips.cc/virtual/2022/poster/53802,Robustness,Defence,"['Differentially private machine learning', 'Statistically audit', 'Empirically evaluate', 'Privacy search', 'Verification methods', 'Influence-based poisoning attacks', 'Logistic regression', 'Naive Bayes', 'Random forest', 'Implementation errors', 'Misuse']",,Other aspects,Poisoning,,,Wrong,Wrong
1551,https://neurips.cc/virtual/2022/poster/53359,Robust Streaming PCA,Poster,NeurIPS,2022,"We consider streaming principal component analysis when the stochastic data-generating model is subject to perturbations. While existing models assume a fixed covariance, we adopt a robust perspective where the covariance matrix belongs to a temporal uncertainty set. Under this setting, we provide fundamental limits on any algorithm recovering principal components. We analyze the convergence of the noisy power method and Oja’s algorithm, both studied for the stationary data generating model, and argue that the noisy power method is rate-optimal in our setting. Finally, we demonstrate the validity of our analysis through numerical experiments. ",,https://openreview.net/forum?id=dAZdQM32IoK,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53359.png?t=1669684570.2344933,https://neurips.cc/media/neurips-2022/Slides/53359_LIqzYY0.pdf,Not relevant,Not relevant,"['Streaming PCA', 'Robustness', 'Principal components', 'Covariance matrix', 'Temporal uncertainty set', 'Noisy power method', ""Oja's algorithm""]",[],,,,,,
1561,https://neurips.cc/virtual/2022/poster/53960,"Deep Ensembles Work, But Are They Necessary?",Poster,NeurIPS,2022,"Ensembling neural networks is an effective way to increase accuracy, and can often match the performance of individual larger models. This observation poses a natural question: given the choice between a deep ensemble and a single neural network with similar accuracy, is one preferable over the other? Recent work suggests that deep ensembles may offer distinct benefits beyond predictive power: namely, uncertainty quantification and robustness to dataset shift. In this work, we demonstrate limitations to these purported benefits, and show that a single (but larger) neural network can replicate these qualities. First, we show that ensemble diversity, by any metric, does not meaningfully contribute to an ensemble's ability to detect out-of-distribution (OOD) data, but is instead highly correlated with the relative improvement of a single larger model. Second, we show that the OOD performance afforded by ensembles is strongly determined by their in-distribution (InD) performance, and - in this sense - is not indicative of any ""effective robustness."" While deep ensembles are a practical way to achieve improvements to predictive power, uncertainty quantification, and robustness, our results show that these improvements can be replicated by a (larger) single model.",,https://openreview.net/forum?id=Wl1ZIgMqLlq,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53960.png?t=1669590045.6035554,https://neurips.cc/virtual/2022/poster/53960,Robustness,Defence,"['Deep ensemble', 'Neural network', 'Accuracy', 'Uncertainty quantification', 'Robustness', 'Dataset shift', 'Out-of-distribution', 'In-distribution', 'Predictive power']",Not clear,,,,,,
1562,https://neurips.cc/virtual/2022/poster/53447,Zero-Sum Stochastic Stackelberg Games,Poster,NeurIPS,2022,"Zero-sum stochastic games have found important applications in a variety of fields, from machine learning to economics. Work on this model has primarily focused on the computation of Nash equilibrium due to its effectiveness in solving adversarial board and video games. Unfortunately, a Nash equilibrium is not guaranteed to exist in zero-sum stochastic games when the payoffs at each state are not convex-concave in the players' actions. A Stackelberg equilibrium, however, is guaranteed to exist. Consequently, in this paper, we study zero-sum stochastic Stackelberg games. Going beyond known existence results for (non-stationary) Stackelberg equilibria, we prove the existence of recursive (i.e., Markov perfect) Stackelberg equilibria (recSE) in these games, provide necessary and sufficient conditions for a policy profile to be a recSE, and show that recSE can be computed in (weakly) polynomial time via value iteration. Finally, we show that zero-sum stochastic Stackelberg games can model the problem of pricing and allocating goods across agents and time. More specifically, we propose a zero-sum stochastic Stackelberg game whose recSE correspond to the recursive competitive equilibria of a large class of stochastic Fisher markets. We close with a series of experiments that showcase how our methodology can be used to solve the consumption-savings problem in stochastic Fisher markets.",,https://openreview.net/forum?id=7TleYo6Tmlo,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53447.png?t=1669692934.7538104,https://neurips.cc/virtual/2022/poster/53447,Not relevant,Defence,"['Zero-sum stochastic games', 'Nash equilibrium', 'Stackelberg equilibrium', 'Machine learning', 'Economics', 'Adversarial board and video games', 'Convex-concave', 'Markov perfect', 'Stochastic Fisher markets', 'Recursive competitive equilibria']","['Economics', 'Stochastic Fisher markets']",Not relevant,Not relevant,,,,
1566,https://neurips.cc/virtual/2022/poster/53452,Collaborative Decision Making Using Action Suggestions,Poster,NeurIPS,2022,"The level of autonomy is increasing in systems spanning multiple domains, but these systems still experience failures. One way to mitigate the risk of failures is to integrate human oversight of the autonomous systems and rely on the human to take control when the autonomy fails. In this work, we formulate a method of collaborative decision making through action suggestions that improves action selection without taking control of the system. Our approach uses each suggestion efficiently by incorporating the implicit information shared through suggestions to modify the agent's belief and achieves better performance with fewer suggestions than naively following the suggested actions. We assume collaborative agents share the same objective and communicate through valid actions. By assuming the suggested action is dependent only on the state, we can incorporate the suggested action as an independent observation of the environment. The assumption of a collaborative environment enables us to use the agent's policy to estimate the distribution over action suggestions. We propose two methods that use suggested actions and demonstrate the approach through simulated experiments. The proposed methodology results in increased performance while also being robust to suboptimal suggestions.",,https://openreview.net/forum?id=DylWBluOgqN,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53452.png?t=1668497893.9315891,https://neurips.cc/virtual/2022/poster/53452,Not relevant,Other aspects,"['Collaborative decision making', 'Action suggestions', 'Autonomy', 'Human oversight', 'Belief', 'Valid actions', 'Environment', 'Policy', 'Simulated experiments', 'Performance', 'Robustness']",['Autonomous systems'],,,,,,
1568,https://neurips.cc/virtual/2022/poster/55153,C-Mixup: Improving Generalization in Regression,Poster,NeurIPS,2022,"Improving the generalization of deep networks is an important open challenge, particularly in domains without plentiful data. The mixup algorithm improves generalization by linearly interpolating a pair of examples and their corresponding labels. These interpolated examples augment the original training set. Mixup has shown promising results in various classification tasks, but systematic analysis of mixup in regression remains underexplored. Using mixup directly on regression labels can result in arbitrarily incorrect labels. In this paper, we propose a simple yet powerful algorithm, C-Mixup, to improve generalization on regression tasks. In contrast with vanilla mixup, which picks training examples for mixing with uniform probability, C-Mixup adjusts the sampling probability based on the similarity of the labels. Our theoretical analysis confirms that C-Mixup with label similarity obtains a smaller mean square error in supervised regression and meta-regression than vanilla mixup and using feature similarity. Another benefit of C-Mixup is that it can improve out-of-distribution robustness, where the test distribution is different from the training distribution. By selectively interpolating examples with similar labels, it mitigates the effects of domain-associated information and yields domain-invariant representations. We evaluate C-Mixup on eleven datasets, ranging from tabular to video data. Compared to the best prior approach, C-Mixup achieves 6.56%, 4.76%, 5.82% improvements in in-distribution generalization, task generalization, and out-of-distribution robustness, respectively. Code is released at https://github.com/huaxiuyao/C-Mixup.",,https://openreview.net/forum?id=BgMz5LHc07R,https://neurips.cc/virtual/2022/poster/55153,https://neurips.cc/virtual/2022/poster/55153,Not relevant,Defence,"['Generalization', 'Regression', 'Mixup', 'Deep networks', 'Label similarity', 'Out-of-distribution robustness', 'Meta-regression', 'Supervised regression', 'Domain-invariant representations']",,,,,,,
1570,https://neurips.cc/virtual/2022/poster/54198,Causal Discovery in Heterogeneous Environments Under the Sparse Mechanism Shift Hypothesis,Poster,NeurIPS,2022,"Machine learning approaches commonly rely on the assumption of independent and identically distributed (i.i.d.) data. In reality, however, this assumption is almost always violated due to distribution shifts between environments. Although valuable learning signals can be provided by heterogeneous data from changing distributions, it is also known that learning under arbitrary (adversarial) changes is impossible. Causality provides a useful framework for modeling distribution shifts, since causal models encode both observational and interventional distributions. In this work, we explore the sparse mechanism shift hypothesis which posits that distribution shifts occur due to a small number of changing causal conditionals. Motivated by this idea, we apply it to learning causal structure from heterogeneous environments, where i.i.d. data only allows for learning an equivalence class of graphs without restrictive assumptions. We propose the Mechanism Shift Score (MSS), a score-based approach amenable to various empirical estimators, which provably identifies the entire causal structure with high probability if the sparse mechanism shifts hypothesis holds. Empirically, we verify behavior predicted by the theory and compare multiple estimators and score functions to identify the best approaches in practice. Compared to other methods, we show how MSS bridges a gap by both being nonparametric as well as explicitly leveraging sparse changes.",,https://openreview.net/forum?id=kFRCvpubDJo,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54198.png?t=1669270633.7432652,https://neurips.cc/virtual/2022/poster/54198,Robustness,Other aspects,"['Causal discovery', 'Heterogeneous environments', 'Sparse mechanism shift hypothesis', 'Machine learning', 'Independent and identically distributed (i.i.d.) data', 'Distribution shifts', 'Causality', 'Observational and interventional distributions', 'Mechanism Shift Score (MSS)', 'Score-based approach', 'Empirical estimators', 'Nonparametric']",,Other aspects,Robustness,Not relevant,Not relevant,,
1573,https://neurips.cc/virtual/2022/poster/55106,Understanding Robust Learning through the Lens of Representation Similarities,Poster,NeurIPS,2022,"Representation learning, \textit{i.e.} the generation of representations useful for downstream applications, is a task of fundamental importance that underlies much of the success of deep neural networks (DNNs). Recently, \emph{robustness to adversarial examples} has emerged as a desirable property for DNNs, spurring the development of robust training methods that account for adversarialexamples. In this paper, we aim to understand how the properties of representations learned by robust training differ from those obtained from standard, non-robust training. This is critical to diagnosing numerous salient pitfalls in robust networks, such as, degradation of performance on benign inputs, poor generalization of robustness, and increase in over-fitting. We utilize a powerful set of tools known as representation similarity metrics, across 3 vision datasets, to obtain layer-wise comparisons between robust and non-robust DNNs with different architectures, training procedures and adversarial constraints. Our experiments highlight hitherto unseen properties of robust representations that we posit underlie the behavioral differences of robust networks. We discover a lack of specialization in robust networks' representations along with a disappearance of `block structure'. We also find overfitting during robust training largely impacts deeper layers. These, along with other findings, suggest ways forward for the design and training of better robust networks.",,https://openreview.net/forum?id=SbAaNa97bzp,https://neurips.cc/virtual/2022/poster/55106,https://neurips.cc/virtual/2022/poster/55106,Robustness,Defence,"['Robust learning', 'Representation Similarities', 'Deep neural networks (DNNs)', 'Adversarial examples', 'Representation learning', 'Robust training', 'Generalization of robustness', 'Overfitting']",,,,,,,
1576,https://neurips.cc/virtual/2022/poster/55117,Coordinates Are NOT Lonely - Codebook Prior Helps Implicit Neural 3D representations,Poster,NeurIPS,2022,"Implicit neural 3D representation has achieved impressive results in surface or scene reconstruction and novel view synthesis, which typically uses the coordinate-based multi-layer perceptrons (MLPs) to learn a continuous scene representation. However, existing approaches, such as Neural Radiance Field (NeRF) and its variants, usually require dense input views (i.e. 50-150) to obtain decent results. To relive the over-dependence on massive calibrated images and enrich the coordinate-based feature representation, we explore injecting the prior information into the coordinate-based network and introduce a novel coordinate-based model, CoCo-INR, for implicit neural 3D representation. The cores of our method are two attention modules: codebook attention and coordinate attention. The former extracts the useful prototypes containing rich geometry and appearance information from the prior codebook, and the latter propagates such prior information into each coordinate and enriches its feature representation for a scene or object surface. With the help of the prior information, our method can render 3D views with more photo-realistic appearance and geometries than the current methods using fewer calibrated images available. Experiments on various scene reconstruction datasets, including DTU and BlendedMVS, and the full 3D head reconstruction dataset, H3DS, demonstrate the robustness under fewer input views and fine detail-preserving capability of our proposed method.",,https://openreview.net/forum?id=oprTuM8F3dt,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55117.png?t=1668772862.5848517,https://neurips.cc/virtual/2022/poster/55117,Not relevant,Defence,"['Implicit neural 3D representation', 'surface reconstruction', 'scene reconstruction', 'novel view synthesis', 'coordinate-based multi-layer perceptrons (MLPs)', 'Neural Radiance Field (NeRF)', 'prior information', 'codebook attention', 'coordinate attention', '3D view rendering', 'photo-realistic appearance', 'geometry', 'fewer input views', 'fine detail-preserving capability']","['Scene reconstruction', '3D head reconstruction']",,,,,,
1583,https://neurips.cc/virtual/2022/poster/53012,Invariance-Aware Randomized Smoothing Certificates,Poster,NeurIPS,2022,"Building models that comply with the invariances inherent to different domains, such as invariance under translation or rotation, is a key aspect of applying machine learning to real world problems like molecular property prediction, medical imaging, protein folding or LiDAR classification. For the first time, we study how the invariances of a model can be leveraged to provably guarantee the robustness of its predictions. We propose a gray-box approach, enhancing the powerful black-box randomized smoothing technique with white-box knowledge about invariances. First, we develop gray-box certificates based on group orbits, which can be applied to arbitrary models with invariance under permutation and Euclidean isometries. Then, we derive provably tight gray-box certificates. We experimentally demonstrate that the provably tight certificates can offer much stronger guarantees, but that in practical scenarios the orbit-based method is a good approximation.",,https://openreview.net/forum?id=5TfqL2gWdV9,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53012.png?t=1668788401.095689,https://neurips.cc/virtual/2022/poster/53012,Robustness,Defence,"['invariance', 'machine learning', 'robustness', 'prediction', 'gray-box', 'randomized smoothing', 'certificates', 'group orbits', 'permutation', 'Euclidean isometries']","['molecular property prediction', 'medical imaging', 'protein folding', 'LiDAR classification']",,,,,,
1584,https://neurips.cc/virtual/2022/poster/54077,Reinforced Genetic Algorithm for Structure-based Drug Design,Poster,NeurIPS,2022,"Structure-based drug design (SBDD) aims to discover drug candidates by finding molecules (ligands) that bind tightly to a disease-related protein (targets), which is the primary approach to computer-aided drug discovery. Recently, applying deep generative models for three-dimensional (3D) molecular design conditioned on protein pockets to solve SBDD has attracted much attention, but their formulation as probabilistic modeling often leads to unsatisfactory optimization performance. On the other hand, traditional combinatorial optimization methods such as genetic algorithms (GA) have demonstrated state-of-the-art performance in various molecular optimization tasks. However, they do not utilize protein target structure to inform design steps but rely on a random-walk-like exploration, which leads to unstable performance and no knowledge transfer between different tasks despite the similar binding physics. To achieve a more stable and efficient SBDD, we propose Reinforced Genetic Algorithm (RGA) that uses neural models to prioritize the profitable design steps and suppress random-walk behavior. The neural models take the 3D structure of the targets and ligands as inputs and are pre-trained using native complex structures to utilize the knowledge of the shared binding physics from different targets and then fine-tuned during optimization. We conduct thorough empirical studies on optimizing binding affinity to various disease targets and show that RGA outperforms the baselines in terms of docking scores and is more robust to random initializations. The ablation study also indicates that the training on different targets helps improve the performance by leveraging the shared underlying physics of the binding processes. The code is available at https://github.com/futianfan/reinforced-genetic-algorithm.",,https://openreview.net/forum?id=Qx6UPW0r9Lf,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/49ef08ad6e7f26d7f200e1b2b9e6e4ac.png?t=1666036372.356775,https://neurips.cc/virtual/2022/poster/54077,Not relevant,Not relevant,"['Structure-based drug design', 'Deep generative models', 'Three-dimensional molecular design', 'Protein pockets', 'Combinatorial optimization', 'Genetic algorithms', 'Neural models', 'Binding affinity', 'Disease targets', 'Docking scores', 'Physics of binding processes']",['Pharmaceuticals'],,,,,,
1601,https://neurips.cc/virtual/2022/poster/53301,Certifying Robust Graph Classification under Orthogonal Gromov-Wasserstein Threats,Poster,NeurIPS,2022,"Graph classifiers are vulnerable to topological attacks. Although certificates of robustness have been recently developed, their threat model only counts local and global edge perturbations, which effectively ignores important graph structures such as isomorphism. To address this issue, we propose measuring the perturbation with the orthogonal Gromov-Wasserstein discrepancy, and building its Fenchel biconjugate to facilitate convex optimization. Our key insight is drawn from the matching loss whose root connects two variables via a monotone operator, and it yields a tight outer convex approximation for resistance distance on graph nodes. When applied to graph classification by graph convolutional networks, both our certificate and attack algorithm are demonstrated effective.",,https://openreview.net/forum?id=qcRgqCXv1o2,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53301.png?t=1669582573.7205842,https://neurips.cc/virtual/2022/poster/53301,Evasion,Defence,"['Robust Graph Classification', 'Orthogonal Gromov-Wasserstein', 'Threats', 'Certifying', 'Graph classifiers', 'Topological attacks', 'Isomorphism', 'Perturbation', 'Fenchel biconjugate', 'Convex optimization', 'Matching loss', 'Resistance distance', 'Graph nodes', 'Graph convolutional networks', 'Certificate', 'Attack algorithm']",,Attack,Evasion,Other aspects,,,Arguable
1614,https://neurips.cc/virtual/2022/poster/53969,Seeing the forest and the tree: Building representations of both individual and collective dynamics with transformers,Poster,NeurIPS,2022,"Complex time-varying systems are often studied by abstracting away from the dynamics of individual components to build a model of the population-level dynamics from the start. However, when building a population-level description, it can be easy to lose sight of each individual and how they contribute to the larger picture. In this paper, we present a novel transformer architecture for learning from time-varying data that builds descriptions of both the individual as well as the collective population dynamics. Rather than combining all of our data into our model at the onset, we develop a separable architecture that operates on individual time-series first before passing them forward; this induces a permutation-invariance property and can be used to transfer across systems of different size and order. After demonstrating that our model can be applied to successfully recover complex interactions and dynamics in many-body systems, we apply our approach to populations of neurons in the nervous system. On neural activity datasets, we show that our model not only yields robust decoding performance, but also provides impressive performance in transfer across recordings of different animals without any neuron-level correspondence. By enabling flexible pre-training that can be transferred to neural recordings of different size and order, our work provides a first step towards creating a foundation model for neural decoding.",,https://openreview.net/forum?id=5aZ8umizItU,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53969.png?t=1669482549.1274962,https://neurips.cc/virtual/2022/poster/53969,Not relevant,Other aspects,"['transformer architecture', 'representations', 'time-varying systems', 'population-level dynamics', 'individual', 'complex interactions', 'many-body systems', 'neurons', 'nervous system', 'neural activity', 'transfer learning', 'pre-training']","['neural decoding', 'many-body systems', 'nervous system']",,,,,,
1616,https://neurips.cc/virtual/2022/poster/53641,Understanding and Improving Robustness of Vision Transformers through Patch-based Negative Augmentation,Poster,NeurIPS,2022,"We investigate the robustness of vision transformers (ViTs) through the lens of their special patch-based architectural structure, i.e., they process an image as a sequence of image patches. We find that ViTs are surprisingly insensitive to patch-based transformations, even when the transformation largely destroys the original semantics and makes the image unrecognizable by humans. This indicates that ViTs heavily use features that survived such transformations but are generally not indicative of the semantic class to humans. Further investigations show that these features are useful but non-robust, as ViTs trained on them can achieve high in-distribution accuracy, but break down under distribution shifts. From this understanding, we ask: can training the model to rely less on these features improve ViT robustness and out-of-distribution performance? We use the images transformed with our patch-based operations as negatively augmented views and offer losses to regularize the training away from using non-robust features. This is a complementary view to existing research that mostly focuses on augmenting inputs with semantic-preserving transformations to enforce models' invariance. We show that patch-based negative augmentation consistently improves robustness of ViTs on ImageNet based robustness benchmarks across 20+ different experimental settings. Furthermore, we find our patch-based negative augmentation are complementary to traditional (positive) data augmentation techniques and batch-based negative examples in contrastive learning. ",,https://openreview.net/forum?id=U138nQxHh3,https://neurips.cc/virtual/2022/poster/53641,https://neurips.cc/virtual/2022/poster/53641,Robustness,Defence,"['Vision transformers', 'patch-based architecture', 'robustness', 'negative augmentation', 'semantic class', 'distribution shifts', 'out-of-distribution performance', 'invariance', 'ImageNet', 'contrastive learning']",['Computer Vision'],,,,,,
1619,https://neurips.cc/virtual/2022/poster/54655,Towards Learning Universal Hyperparameter Optimizers with Transformers,Poster,NeurIPS,2022,"Meta-learning hyperparameter optimization (HPO) algorithms from prior experiments is a promising approach to improve optimization efficiency over objective functions from a similar distribution. However, existing methods are restricted to learning from experiments sharing the same set of hyperparameters. In this paper, we introduce the OptFormer, the first text-based Transformer HPO framework that provides a universal end-to-end interface for jointly learning policy and function prediction when trained on vast tuning data from the wild, such as Google’s Vizier database, one of the world’s largest HPO datasets. Our extensive experiments demonstrate that the OptFormer can simultaneously imitate at least 7 different HPO algorithms, which can be further improved via its function uncertainty estimates. Compared to a Gaussian Process, the OptFormer also learns a robust prior distribution for hyperparameter response functions, and can thereby provide more accurate and better calibrated predictions. This work paves the path to future extensions for training a Transformer-based model as a general HPO optimizer.",,https://openreview.net/forum?id=r-6Z1SJbCpv,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54655.png?t=1669214238.8111463,https://neurips.cc/virtual/2022/poster/54655,Not relevant,Defence,"['Hyperparameter optimization', 'Meta-learning', 'OptFormer', 'Transformer', 'Vizier', 'HPO', 'Hyperparameter response functions']",Not clear,,,,,,
1625,https://neurips.cc/virtual/2022/poster/54879,One-shot Neural Backdoor Erasing via Adversarial Weight Masking,Poster,NeurIPS,2022,"Recent studies show that despite achieving high accuracy on a number of real-world applications, deep neural networks (DNNs) can be backdoored: by injecting triggered data samples into the training dataset, the adversary can mislead the trained model into classifying any test data to the target class as long as the trigger pattern is presented. To nullify such backdoor threats, various methods have been proposed. Particularly, a line of research aims to purify the potentially compromised model. However, one major limitation of this line of work is the requirement to access sufficient original training data: the purifying performance is a lot worse when the available training data is limited. In this work, we propose Adversarial Weight Masking (AWM), a novel method capable of erasing the neural backdoors even in the one-shot setting. The key idea behind our method is to formulate this into a min-max optimization problem: first, adversarially recover the non-robust perturbation patterns and then (soft) mask the network weights that are sensitive to the recovered patterns. Comprehensive evaluations of several benchmark datasets suggest that AWM can largely improve the purifying effects over other state-of-the-art methods on various available training dataset sizes. ",,https://openreview.net/forum?id=Yb3dRKY170h,https://neurips.cc/virtual/2022/poster/54879,https://neurips.cc/virtual/2022/poster/54879,Not relevant,Defence,"['deep neural networks', 'backdoored', 'data samples', 'training dataset', 'adversary', 'target class', 'test data', 'trigger pattern', 'nullify', 'backdoor threats', 'purifying', 'compromised model', 'Adversarial Weight Masking', 'min-max optimization', 'non-robust perturbation patterns', 'mask', 'network weights', 'sensitive', 'recovered patterns', 'benchmark datasets', 'purifying effects', 'state-of-the-art methods', 'available training dataset sizes']",,Defence,Poisoning,,,Wrong,Wrong
1641,https://neurips.cc/virtual/2022/poster/53542,A time-resolved theory of information encoding in recurrent neural networks,Poster,NeurIPS,2022,"Information transmission in neural circuits depends on how well time-varying stimuli are encoded by neural populations.A dynamic balance of externally incoming currents by strong recurrent inhibition was previously proposed as a mechanism to accurately and robustly encode the information in a time-varying stimulus, but a full theory was missing. Here, we develop a non-stationary dynamic mean-field theory that transparently explains how a tight balance of excitatory currents by recurrent inhibition improves information encoding. We demonstrate that the mutual information rate of a time-varying input increases linearly with the tightness of balance, both in the presence of additive noise and with recurrently generated chaotic network fluctuations. We corroborated our findings in numerical experiments and demonstrated that recurrent networks with positive firing rates trained to transmit a time-varying stimulus generically use recurrent inhibition to increase the information rate. We also found that networks trained to transmit multiple independent time-varying signals spontaneously form multiple local inhibitory clusters, one for each input channel.Our findings suggest that feedforward excitatory input and local recurrent inhibition--as observed in many biological circuits--is a generic circuit motif for encoding and transmitting time-varying information in recurrent neural circuits.",,https://openreview.net/forum?id=r6_zHM2POTd,https://neurips.cc/virtual/2022/poster/53542,https://neurips.cc/virtual/2022/poster/53542,Not relevant,Defence,"['Information encoding', 'Recurrent neural networks', 'Time-varying stimuli', 'Excitatory currents', 'Recurrent inhibition', 'Mutual information rate', 'Additive noise', 'Chaotic network fluctuations', 'Positive firing rates', 'Feedforward excitatory input', 'Local recurrent inhibition']",,,,,,,
1646,https://neurips.cc/virtual/2022/poster/53983,What You See is What You Get: Principled Deep Learning via Distributional Generalization,Poster,NeurIPS,2022,"Having similar behavior at training time and test time—what we call a “What You See Is What You Get” (WYSIWYG) property—is desirable in machine learning. Models trained with standard stochastic gradient descent (SGD), however, do not necessarily have this property, as their complex behaviors such as robustness or subgroup performance can differ drastically between training and test time. In contrast, we show that Differentially-Private (DP) training provably ensures the high-level WYSIWYG property, which we quantify using a notion of distributional generalization. Applying this connection, we introduce new conceptual tools for designing deep-learning methods by reducing generalization concerns to optimization ones: to mitigate unwanted behavior at test time, it is provably sufficient to mitigate this behavior on the training data. By applying this novel design principle, which bypasses “pathologies” of SGD, we construct simple algorithms that are competitive with SOTA in several distributional-robustness applications, significantly improve the privacy vs. disparate impact trade-off of DP-SGD, and mitigate robust overfitting in adversarial training. Finally, we also improve on theoretical bounds relating DP, stability, and distributional generalization.",,https://openreview.net/forum?id=g05fHAvNeXx,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53983.png?t=1669670325.640289,https://neurips.cc/virtual/2022/poster/53983,Robustness,Defence,"['What You See Is What You Get (WYSIWYG)', 'Stochastic gradient descent (SGD)', 'Differentially-Private (DP) training', 'Distributional generalization', 'Deep learning', 'Optimization', 'Distributional-robustness applications', 'Privacy vs. Disparate impact trade-off', 'Adversarial training', 'Stability', 'Theoretical bounds']",,Other aspects,Robustness,,,,Wrong
1647,https://neurips.cc/virtual/2022/poster/52911,Multitasking Models are Robust to Structural Failure: A Neural Model for Bilingual Cognitive Reserve,Poster,NeurIPS,2022,"We find a surprising connection between multitask learning and robustness to neuron failures. Our experiments show that bilingual language models retain higher performance under various neuron perturbations, such as random deletions, magnitude pruning and weight noise. Our study is motivated by research in cognitive science showing that symptoms of dementia and cognitive decline appear later in bilingual speakers compared to monolingual patients with similar brain damage, a phenomenon called bilingual cognitive reserve. Our language model experiments replicate this phenomenon on bilingual GPT-2 and other models.We provide a theoretical justification of this robustness by mathematically analyzing linear representation learning and showing that multitasking creates more robust representations. We open-source our code and models in the following URL: https://github.com/giannisdaras/multilingual_robustness.",,https://openreview.net/forum?id=SyD-b2m2meG,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/52911.png?t=1669600606.7930393,https://neurips.cc/virtual/2022/poster/52911,Robustness,Defence,"['multitask learning', 'robustness', 'neuron failures', 'bilingual language models', 'cognitive science', 'dementia', 'cognitive decline', 'bilingual cognitive reserve', 'GPT-2', 'linear representation learning', 'mathematically analyzing']",['Language model'],,,,,,
1656,https://neurips.cc/virtual/2022/poster/55205,Learning Generalizable Part-based Feature Representation for 3D Point Clouds,Poster,NeurIPS,2022,"Deep networks on 3D point clouds have achieved remarkable success in 3D classification, while they are vulnerable to geometry variations caused by inconsistent data acquisition procedures. This results in a challenging 3D domain generalization (3DDG) problem, that is to generalize a model trained on source domain to an unseen target domain. Based on the observation that local geometric structures are more generalizable than the whole shape, we propose to reduce the geometry shift by a generalizable part-based feature representation and design a novel part-based domain generalization network (PDG) for 3D point cloud classification. Specifically, we build a part-template feature space shared by source and target domains. Shapes from distinct domains are first organized to part-level features and then represented by part-template features. The transformed part-level features, dubbed aligned part-based representations, are then aggregated by a part-based feature aggregation module. To improve the robustness of the part-based representations, we further propose a contrastive learning framework upon part-based shape representation. Experiments and ablation studies on 3DDA and 3DDG benchmarks justify the efficacy of the proposed approach for domain generalization, compared with the previous state-of-the-art methods. Our code will be available on http://github.com/weixmath/PDG.",,https://openreview.net/forum?id=V03mpOjCwtg,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/46771d1f432b42343f56f791422a4991.png?t=1667055720.7435734,https://neurips.cc/virtual/2022/poster/55205,Robustness,Defence,"['3D point clouds', '3D classification', 'domain generalization', 'part-based feature representation', 'part-template feature space', 'part-level features', 'part-based feature aggregation module', 'contrastive learning framework', '3DDA', '3DDG', 'domain generalization']",['3D'],,,,,,
1668,https://neurips.cc/virtual/2022/poster/54202,Mining Multi-Label Samples from Single Positive Labels,Poster,NeurIPS,2022,"Conditional generative adversarial networks (cGANs) have shown superior results in class-conditional generation tasks. To simultaneously control multiple conditions, cGANs require multi-label training datasets, where multiple labels can be assigned to each data instance. Nevertheless, the tremendous annotation cost limits the accessibility of multi-label datasets in real-world scenarios. Therefore, in this study we explore the practical setting called the single positive setting, where each data instance is annotated by only one positive label with no explicit negative labels. To generate multi-label data in the single positive setting, we propose a novel sampling approach called single-to-multi-label (S2M) sampling, based on the Markov chain Monte Carlo method. As a widely applicable “add-on” method, our proposed S2M sampling method enables existing unconditional and conditional GANs to draw high-quality multi-label data with a minimal annotation cost. Extensive experiments on real image datasets verify the effectiveness and correctness of our method, even when compared to a model trained with fully annotated datasets.",,https://openreview.net/forum?id=R5pVDJ4FNoc,https://neurips.cc/virtual/2022/poster/54202,https://neurips.cc/media/neurips-2022/Slides/54202.pdf,Not relevant,Defence,"['class-conditional generation', 'conditional generative adversarial networks', 'multi-label training datasets', 'single positive setting', 'single-to-multi-label (S2M) sampling', 'Markov chain Monte Carlo method', 'unconditional and conditional GANs']","['real-world scenarios', 'real image datasets']",,,,,,
1677,https://neurips.cc/virtual/2022/poster/55361,Learn what matters: cross-domain imitation learning with task-relevant embeddings,Poster,NeurIPS,2022,"We study how an autonomous agent learns to perform a task from demonstrations in a different domain, such as a different environment or different agent. Such cross-domain imitation learning is required to, for example, train an artificial agent from demonstrations of a human expert. We propose a scalable framework that enables cross-domain imitation learning without access to additional demonstrations or further domain knowledge. We jointly train the learner agent's policy and learn a mapping between the learner and expert domains with adversarial training. We effect this by using a mutual information criterion to find an embedding of the expert's state space that contains task-relevant information and is invariant to domain specifics. This step significantly simplifies estimating the mapping between the learner and expert domains and hence facilitates end-to-end learning. We demonstrate successful transfer of policies between considerably different domains, without extra supervision such as additional demonstrations, and in situations where other methods fail.",,https://openreview.net/forum?id=_w-ivKc1cj,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55361.png?t=1669748315.220398,https://neurips.cc/virtual/2022/poster/55361,Not relevant,Other aspects,"['Cross-domain imitation learning', 'Adversarial training', 'Mutual information criterion', 'Embedding', 'Policy transfer', 'Task-relevant information']",['Autonomous agent'],,,,,,
1678,https://neurips.cc/virtual/2022/poster/54221,ViewFool: Evaluating the Robustness of Visual Recognition to Adversarial Viewpoints,Poster,NeurIPS,2022,"Recent studies have demonstrated that visual recognition models lack robustness to distribution shift. However, current work mainly considers model robustness to 2D image transformations, leaving viewpoint changes in the 3D world less explored. In general, viewpoint changes are prevalent in various real-world applications (e.g., autonomous driving), making it imperative to evaluate viewpoint robustness. In this paper, we propose a novel method called ViewFool to find adversarial viewpoints that mislead visual recognition models. By encoding real-world objects as neural radiance fields (NeRF), ViewFool characterizes a distribution of diverse adversarial viewpoints under an entropic regularizer, which helps to handle the fluctuations of the real camera pose and mitigate the reality gap between the real objects and their neural representations. Experiments validate that the common image classifiers are extremely vulnerable to the generated adversarial viewpoints, which also exhibit high cross-model transferability. Based on ViewFool, we introduce ImageNet-V, a new out-of-distribution dataset for benchmarking viewpoint robustness of image classifiers. Evaluation results on 40 classifiers with diverse architectures, objective functions, and data augmentations reveal a significant drop in model performance when tested on ImageNet-V, which provides a possibility to leverage ViewFool as an effective data augmentation strategy to improve viewpoint robustness.",,https://openreview.net/forum?id=X0m9q0IcsmX,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54221.png?t=1669806231.7583733,https://neurips.cc/virtual/2022/poster/54221,Evasion,Defence,"['Adversarial viewpoint', 'Robustness', 'Visual recognition', 'Neural radiance fields', 'Entropic regularizer', 'Image classifiers', 'Out-of-distribution dataset', 'ViewFool', 'ImageNet-V']",['Autonomous driving'],,,,,,
1690,https://neurips.cc/virtual/2022/poster/54863,3DB: A Framework for Debugging Computer Vision Models,Poster,NeurIPS,2022,"We introduce 3DB: an extendable, unified framework for testing and debugging vision models using photorealistic simulation.  We demonstrate, through a wide range of use cases, that 3DB allows users to discover vulnerabilities in computer vision systems and gain insights into how models make decisions. 3DB captures and generalizes many robustness analyses from prior work, and enables one to study their interplay. Finally, we find that the insights generated by the system transfer to the physical world. 3DB will be released as a library alongside a set of examples and documentation. We attach 3DB to the submission.",,https://openreview.net/forum?id=dRgHxaOJsiV,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54863.png?t=1669226704.1689942,https://neurips.cc/virtual/2022/poster/54863,Robustness,Defence,"['3DB', 'Debugging', 'Computer Vision', 'Models', 'Photorealistic simulation', 'Robustness analyses', 'Physical world']",,,,,,,
1706,https://neurips.cc/virtual/2022/poster/54790,FairVFL: A Fair Vertical Federated Learning Framework with Contrastive Adversarial Learning,Poster,NeurIPS,2022,"Vertical federated learning (VFL) is a privacy-preserving machine learning paradigm that can learn models from features distributed on different platforms in a privacy-preserving way. Since in real-world applications the data may contain bias on fairness-sensitive features (e.g., gender), VFL models may inherit bias from training data and become unfair for some user groups. However, existing fair machine learning methods usually rely on the centralized storage of fairness-sensitive features to achieve model fairness, which are usually inapplicable in federated scenarios. In this paper, we propose a fair vertical federated learning framework (FairVFL), which can improve the fairness of VFL models. The core idea of FairVFL is to learn unified and fair representations of samples based on the decentralized feature fields in a privacy-preserving way. Specifically, each platform with fairness-insensitive features first learns local data representations from local features. Then, these local representations are uploaded to a server and aggregated into a unified representation for the target task. In order to learn a fair unified representation, we send it to each platform storing fairness-sensitive features and apply adversarial learning to remove bias from the unified representation inherited from the biased data. Moreover, for protecting user privacy, we further propose a contrastive adversarial learning method to remove private information from the unified representation in server before sending it to the platforms keeping fairness-sensitive features. Experiments on three real-world datasets validate that our method can effectively improve model fairness with user privacy well-protected.",,https://openreview.net/forum?id=5vVSA_cdRqe,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/d7aab42e6b85c49c0f1d3a115e939c74.png?t=1666273310.6975615,https://neurips.cc/virtual/2022/poster/54790,Not relevant,Not relevant,"['Vertical federated learning', 'Fairness-sensitive features', 'Bias', 'Adversarial learning', 'Privacy-preserving', 'Contrastive adversarial learning']",['None'],Defence,Data Extraction,,,Correct,Correct
1713,https://neurips.cc/virtual/2022/poster/52888,Robust Anytime Learning of Markov Decision Processes,Poster,NeurIPS,2022,"Markov decision processes (MDPs) are formal models commonly used in sequential decision-making. MDPs capture the stochasticity that may arise, for instance, from imprecise actuators via probabilities in the transition function. However, in data-driven applications, deriving precise probabilities from (limited) data introduces statistical errors that may lead to unexpected or undesirable outcomes.Uncertain MDPs (uMDPs) do not require precise probabilities but instead use so-called uncertainty sets in the transitions, accounting for such limited data.Tools from the formal verification community efficiently compute robust policies that provably adhere to formal specifications, like safety constraints, under the worst-case instance in the uncertainty set. We continuously learn the transition probabilities of an MDP in a robust anytime-learning approach that combines a dedicated Bayesian inference scheme with the computation of robust policies. In particular, our method (1) approximates probabilities as intervals, (2) adapts to new data that may be inconsistent with an intermediate model, and (3) may be stopped at any time to compute a robust policy on the uMDP that faithfully captures the data so far. Furthermore, our method is capable of adapting to changes in the environment. We show the effectiveness of our approach and compare it to robust policies computed on uMDPs learned by the UCRL2 reinforcement learning algorithm in an experimental evaluation on several benchmarks.",,https://openreview.net/forum?id=B5qRau1IxjM,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/52888.png?t=1669977825.6554694,https://neurips.cc/virtual/2022/poster/52888,Robustness,Defence,"['Markov decision processes', 'Uncertain MDPs', 'formal verification', 'Bayesian inference', 'robust anytime-learning', 'UCRL2 reinforcement learning']",['sequential decision-making'],,,,,,
1715,https://neurips.cc/virtual/2022/poster/53155,Trimmed Maximum Likelihood Estimation for Robust Generalized Linear Model,Poster,NeurIPS,2022,"We study the problem of learning generalized linear models under adversarial corruptions.We analyze a classical heuristic called the \textit{iterative trimmed maximum likelihood estimator} which is known to be effective against \textit{label corruptions} in practice. Under label corruptions, we prove that this simple estimator achieves minimax near-optimal risk on a wide range of generalized linear models, including Gaussian regression, Poisson regression and Binomial regression. Finally, we extend the estimator to the much more challenging setting of \textit{label and covariate corruptions} and demonstrate its robustness and optimality in that setting as well.",,https://openreview.net/forum?id=VHmdFPy4U_u,https://neurips.cc/virtual/2022/poster/53155,https://neurips.cc/virtual/2022/poster/53155,Robustness,Defence,"['generalized linear models', 'adversarial corruptions', 'iterative trimmed maximum likelihood estimator', 'minimax near-optimal risk', 'Gaussian regression', 'Poisson regression', 'Binomial regression', 'label and covariate corruptions', 'robustness', 'optimality']",,,,,,,
1723,https://neurips.cc/virtual/2022/poster/53266,Robust Imitation of a Few Demonstrations with a Backwards Model,Poster,NeurIPS,2022,"Behavior cloning of expert demonstrations can speed up learning optimal policies in a more sample-efficient way over reinforcement learning. However, the policy cannot extrapolate well to unseen states outside of the demonstration data, creating covariate shift (agent drifting away from demonstrations) and compounding errors. In this work, we tackle this issue by extending the region of attraction around the demonstrations so that the agent can learn how to get back onto the demonstrated trajectories if it veers off-course. We train a generative backwards dynamics model and generate short imagined trajectories from states in the demonstrations. By imitating both demonstrations and these model rollouts, the agent learns the demonstrated paths and how to get back onto these paths. With optimal or near-optimal demonstrations, the learned policy will be both optimal and robust to deviations, with a wider region of attraction. On continuous control domains, we evaluate the robustness when starting from different initial states unseen in the demonstration data. While both our method and other imitation learning baselines can successfully solve the tasks for initial states in the training distribution, our method exhibits considerably more robustness to different initial states.",,https://openreview.net/forum?id=aoWo6iAxGx,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53266.png?t=1669678679.6380723,https://neurips.cc/virtual/2022/poster/53266,Robustness,Defence,"['Imitation learning', 'Backwards model', 'Robustness', 'Covariate shift', 'Extrapolation', 'Generative model', 'Continuous control']",['Continuous control domains'],,,,,,
1747,https://neurips.cc/virtual/2022/poster/54823,Revisiting Sparse Convolutional Model for Visual Recognition,Poster,NeurIPS,2022,"Despite strong empirical performance for image classification, deep neural networks are often regarded as ``black boxes'' and they are difficult to interpret. On the other hand, sparse convolutional models, which assume that a signal can be expressed by a linear combination of a few elements from a convolutional dictionary, are powerful tools for analyzing natural images with good theoretical interpretability and biological plausibility. However, such principled models have not demonstrated competitive performance when compared with empirically designed deep networks. This paper revisits the sparse convolutional modeling for image classification and bridges the gap between good empirical performance (of deep learning) and good interpretability (of sparse convolutional models). Our method uses differentiable optimization layers that are defined from convolutional sparse coding as drop-in replacements of standard convolutional layers in conventional deep neural networks. We show that such models have equally strong empirical performance on CIFAR-10, CIFAR-100 and ImageNet datasets when compared to conventional neural networks. By leveraging stable recovery property of sparse modeling, we further show that such models can be much more robust to input corruptions as well as adversarial perturbations in testing through a simple proper trade-off between sparse regularization and data reconstruction terms. ",,https://openreview.net/forum?id=INzRLBAA4JX,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54823.png?t=1669715451.5791245,https://neurips.cc/virtual/2022/poster/54823,Robustness,Defence,"['Sparse convolutional model', 'Visual recognition', 'Deep neural networks', 'Interpretability', 'Convolutional dictionary', 'Differentiable optimization layers', 'Conventional deep neural networks', 'Sparse coding', 'Input corruptions', 'Adversarial perturbations', 'Sparse regularization', 'Data reconstruction']","['Image classification', 'CIFAR-10', 'CIFAR-100', 'ImageNet']",,,,,,
1754,https://neurips.cc/virtual/2022/poster/53903,Friendly Noise against Adversarial Noise: A Powerful Defense against Data Poisoning Attack,Poster,NeurIPS,2022,"A powerful category of (invisible) data poisoning attacks modify a subset of training examples by small adversarial perturbations to change the prediction of certain test-time data. Existing defense mechanisms are not desirable to deploy in practice, as they ofteneither drastically harm the generalization performance, or are attack-specific, and prohibitively slow to apply. Here, we propose a simple but highly effective approach that unlike existing methods breaks various types of invisible poisoning attacks with the slightest drop in the generalization performance. We make the key observation that attacks introduce local sharp regions of high training loss, which when minimized, results in learning the adversarial perturbations and makes the attack successful. To break poisoning attacks, our key idea is to alleviate the sharp loss regions introduced by poisons. To do so, our approach comprises two components: an optimized friendly noise that is generated to maximally perturb examples without degrading the performance, and a randomly varying noise component. The combination of both components builds a very light-weight but extremely effective defense against the most powerful triggerless targeted and hidden-trigger backdoor poisoning attacks, including Gradient Matching, Bulls-eye Polytope, and Sleeper Agent. We show that our friendly noise is transferable to other architectures, and adaptive attacks cannot break our defense due to its random noise component.",,https://openreview.net/forum?id=DoQElY73YR,https://neurips.cc/virtual/2022/poster/53903,https://neurips.cc/virtual/2022/poster/53903,Poisoning,Defence,"['Adversarial noise', 'Data Poisoning Attack', 'Defense Mechanisms', 'Training examples', 'Prediction', 'Generalization performance', 'Sharp loss regions', 'Optimized friendly noise', 'Randomly varying noise component', 'Backdoor poisoning attacks', 'Adaptive attacks']",,Defence,Poisoning,,,,
1768,https://neurips.cc/virtual/2022/poster/54671,FedRolex: Model-Heterogeneous Federated Learning with Rolling Sub-Model Extraction,Poster,NeurIPS,2022,"Most cross-device federated learning (FL) studies focus on the model-homogeneous setting where the global server model and local client models are identical. However, such constraint not only excludes low-end clients who would otherwise make unique contributions to model training but also restrains clients from training large models due to on-device resource bottlenecks. In this work, we propose FedRolex, a partial training (PT)-based approach that enables model-heterogeneous FL and can train a global server model larger than the largest client model. At its core, FedRolex employs a rolling sub-model extraction scheme that allows different parts of the global server model to be evenly trained, which mitigates the client drift induced by the inconsistency between individual client models and server model architectures. Empirically, we show that FedRolex outperforms state-of-the-art PT-based model-heterogeneous FL methods (e.g. Federated Dropout) and reduces the gap between model-heterogeneous and model-homogeneous FL, especially under the large-model large-dataset regime. In addition, we provide theoretical statistical analysis on its advantage over Federated Dropout. Lastly, we evaluate FedRolex on an emulated real-world device distribution to show that FedRolex can enhance the inclusiveness of FL and boost the performance of low-end devices that would otherwise not benefit from FL. Our code is available at: \href{https://github.com/MSU-MLSys-Lab/FedRolex}{https://github.com/MSU-MLSys-Lab/FedRolex}.",,https://openreview.net/forum?id=OtxyysUdBE,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54671.png?t=1668707630.2906868,https://neurips.cc/virtual/2022/poster/54671,Not relevant,Defence,"['Federated learning', 'Model-heterogeneous', 'Rolling sub-model extraction', 'Partial training', 'Low-end clients', 'Large-model large-dataset regime', 'Theoretical statistical analysis']",['Federated learning'],Not relevant,Not relevant,,,,
1782,https://neurips.cc/virtual/2022/poster/52935,Amortized Inference for Causal Structure Learning,Poster,NeurIPS,2022,"Inferring causal structure poses a combinatorial search problem that typically involves evaluating structures with a score or independence test. The resulting search is costly, and designing suitable scores or tests that capture prior knowledge is difficult. In this work, we propose to amortize causal structure learning. Rather than searching over structures, we train a variational inference model to predict the causal structure from observational or interventional data. This allows us to bypass both the search over graphs and the hand-engineering of suitable score functions. Instead, our inference model acquires domain-specific inductive biases for causal discovery solely from data generated by a simulator. The architecture of our inference model emulates permutation invariances that are crucial for statistical efficiency in structure learning, which facilitates generalization to significantly larger problem instances than seen during training. On synthetic data and semisynthetic gene expression data, our models exhibit robust generalization capabilities when subject to substantial distribution shifts and significantly outperform existing algorithms, especially in the challenging genomics domain. Our code and models are publicly available at: https://github.com/larslorch/avici",,https://openreview.net/forum?id=eV4JI-MMeX,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/f6bc0623a4ab517ae89db46f368c09c4.png?t=1666175021.179898,https://neurips.cc/virtual/2022/poster/52935,Not relevant,Defence,"['Amortized Inference', 'Causal Structure Learning', 'Combinatorial search problem', 'Score or independence test', 'Variational inference model', 'Observational or interventional data', 'Permutation invariances', 'Statistical efficiency', 'Structure learning', 'Synthetic data', 'Semisynthetic gene expression data', 'Generalization capabilities', 'Distribution shifts', 'Genomics domain']",['Genomics'],,,,,,
1786,https://neurips.cc/virtual/2022/poster/54741,Washing The Unwashable : On The (Im)possibility of Fairwashing Detection,Poster,NeurIPS,2022,"The use of black-box models (e.g., deep neural networks) in high-stakes decision-making systems, whose internal logic is complex, raises the need for providing explanations about their decisions. Model explanation techniques mitigate this problem by generating an interpretable and high-fidelity surrogate model (e.g., a logistic regressor or decision tree) to explain the logic of black-box models. In this work, we investigate the issue of fairwashing, in which model explanation techniques are manipulated to rationalize decisions taken by an unfair black-box model using deceptive surrogate models. More precisely, we theoretically characterize and analyze fairwashing, proving that this phenomenon is difficult to avoid due to an irreducible factor---the unfairness of the black-box model. Based on the theory developed, we propose a novel technique, called FRAUD-Detect (FaiRness AUDit Detection), to detect fairwashed models by measuring a divergence over subpopulation-wise fidelity measures of the interpretable model. We empirically demonstrate that this divergence is significantly larger in purposefully fairwashed interpretable models than in honest ones. Furthermore, we show that our detector is robust to an informed adversary trying to bypass our detector. The code implementing FRAUD-Detect is available at https://github.com/cleverhans-lab/FRAUD-Detect.",,https://openreview.net/forum?id=3vmKQUctNy,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54741.png?t=1670445305.7735431,https://neurips.cc/virtual/2022/poster/54741,Robustness,Defence,"['fairwashing', 'black-box models', 'model explanation techniques', 'surrogate model', 'FRAUD-Detect', 'fairness audit detection', 'subpopulation-wise fidelity measures']",['high-stakes decision-making systems'],Other aspects,Other attack,,,Wrong,Arguable
1788,https://neurips.cc/virtual/2022/poster/55076,Debiased Self-Training for Semi-Supervised Learning,Poster,NeurIPS,2022,"Deep neural networks achieve remarkable performances on a wide range of tasks with the aid of large-scale labeled datasets. Yet these datasets are time-consuming and labor-exhaustive to obtain on realistic tasks. To mitigate the requirement for labeled data, self-training is widely used in semi-supervised learning by iteratively assigning pseudo labels to unlabeled samples. Despite its popularity, self-training is well-believed to be unreliable and often leads to training instability. Our experimental studies further reveal that the bias in semi-supervised learning arises from both the problem itself and the inappropriate training with potentially incorrect pseudo labels, which accumulates the error in the iterative self-training process. To reduce the above bias, we propose Debiased Self-Training (DST). First, the generation and utilization of pseudo labels are decoupled by two parameter-independent classifier heads to avoid direct error accumulation. Second, we estimate the worst case of self-training bias, where the pseudo labeling function is accurate on labeled samples, yet makes as many mistakes as possible on unlabeled samples. We then adversarially optimize the representations to improve the quality of pseudo labels by avoiding the worst case. Extensive experiments justify that DST achieves an average improvement of 6.3% against state-of-the-art methods on standard semi-supervised learning benchmark datasets and 18.9% against FixMatch on 13 diverse tasks. Furthermore, DST can be seamlessly adapted to other self-training methods and help stabilize their training and balance performance across classes in both cases of training from scratch and finetuning from pre-trained models.",,https://openreview.net/forum?id=NI7moUOKtc,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/08f90c1a417155361a5c4b8d297e0d78.png?t=1666668047.4378536,https://neurips.cc/media/neurips-2022/Slides/55076.pdf,Not relevant,Defence,"['semi-supervised learning', 'self-training', 'bias', 'pseudo labels', 'adversarial optimization', 'classifier heads']",['none'],,,,,,
1796,https://neurips.cc/virtual/2022/poster/53039,How Sampling Impacts the Robustness of Stochastic Neural Networks,Poster,NeurIPS,2022,"Stochastic neural networks (SNNs) are random functions whose predictions are gained by averaging over multiple realizations. Consequently, a gradient-based adversarial example is calculated based on one set of samples and its classification on another set. In this paper we derive a sufficient condition for such a stochastic prediction to be robust against a given sample-based attack. This allows us to identify the factors that lead to an increased robustness of SNNs and gives theoretical explanations for: (i) the well known observation, that increasing the amount of samples drawn for the estimation of adversarial examples increases the attack's strength,(ii) why increasing the number of samples during an attack can not fully reduce the effect of stochasticity, (iii) why the sample size during inference does not influence the robustness, and(iv) why a higher gradient variance and a shorter expected value of the gradient relates to a higher robustness. Our theoretical findings give a unified view on the mechanisms underlying previously proposed approaches for increasing attack strengths or model robustness and are verified by an extensive empirical analysis.",,https://openreview.net/forum?id=-zBN5sBzdvr,https://neurips.cc/virtual/2022/poster/53039,https://neurips.cc/virtual/2022/poster/53039,Evasion,Defence,"['Stochastic neural networks', 'Adversarial example', 'Gradient-based', 'Sample-based attack', 'Robustness', 'Increasing attack strengths', 'Model robustness', 'Empirical analysis']",,Other aspects,Evasion,,,,Arguable
1797,https://neurips.cc/virtual/2022/poster/52788,Evaluating Latent Space Robustness and Uncertainty of EEG-ML Models under Realistic Distribution Shifts,Poster,NeurIPS,2022,"The recent availability of large datasets in bio-medicine has inspired the development of representation learning methods for multiple healthcare applications. Despite advances in predictive performance, the clinical utility of such methods is limited when exposed to real-world data. This study develops model diagnostic measures to detect potential pitfalls before deployment without assuming access to external data. Specifically, we focus on modeling realistic data shifts in electrophysiological signals (EEGs) via data transforms and extend the conventional task-based evaluations with analyses of a) the model's latent space and b) predictive uncertainty under these transforms. We conduct experiments on multiple EEG feature encoders and two clinically relevant downstream tasks using publicly available large-scale clinical EEGs. Within this experimental setting, our results suggest that measures of latent space integrity and model uncertainty under the proposed data shifts may help anticipate performance degradation during deployment.",,https://openreview.net/forum?id=KRk0lBRPpOC,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/52788.png?t=1669521679.0218084,https://neurips.cc/media/neurips-2022/Slides/52788.pdf,Robustness,Defence,"['Latent space robustness', 'Uncertainty', 'EEG-ML models', 'Realistic distribution shifts', 'Representation learning', 'EEG signals', 'Downstream tasks', 'Clinical utility']",['Healthcare'],,,,,,
1799,https://neurips.cc/virtual/2022/poster/53006,Parameters or Privacy: A Provable Tradeoff Between Overparameterization and Membership Inference,Poster,NeurIPS,2022,"A surprising phenomenon in modern machine learning is the ability of a highly overparameterized model to generalize well (small error on the test data) even when it is trained to memorize the training data (zero error on the training data). This has led to an arms race towards increasingly overparameterized models (c.f., deep learning). In this paper, we study an underexplored hidden cost of overparameterization: the fact that overparameterized models may be more vulnerable to privacy attacks, in particular the membership inference attack that predicts the (potentially sensitive) examples used to train a model. We significantly extend the relatively few empirical results on this problem by theoretically proving for an overparameterized linear regression model in the Gaussian data setting that membership inference vulnerability increases with the number of parameters. Moreover, a range of empirical studies indicates that more complex, nonlinear models exhibit the same behavior. Finally, we extend our analysis towards ridge-regularized linear regression and show in the Gaussian data setting that increased regularization also increases membership inference vulnerability in the overparameterized regime.",,https://openreview.net/forum?id=7nypt7cjNL,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53006.png?t=1669670266.6155415,https://neurips.cc/virtual/2022/poster/53006,Not relevant,Attack,"['overparameterization', 'membership inference attack', 'privacy attacks', 'linear regression', 'ridge regularization', 'Gaussian data setting']",,,,,,,
1804,https://neurips.cc/virtual/2022/poster/54022,"How to talk so AI will learn: Instructions, descriptions, and autonomy",Poster,NeurIPS,2022,"From the earliest years of our lives, humans use language to express our beliefs and desires. Being able to talk to artificial agents about our preferences would thus fulfill a central goal of value alignment. Yet today, we lack computational models explaining such language use. To address this challenge, we formalize learning from language in a contextual bandit setting and ask how a human might communicate preferences over behaviors. We study two distinct types of language: instructions, which provide information about the desired policy, and descriptions, which provide information about the reward function. We show that the agent's degree of autonomy determines which form of language is optimal: instructions are better in low-autonomy settings, but descriptions are better when the agent will need to act independently. We then define a pragmatic listener agent that robustly infers the speaker's reward function by reasoning about how the speaker expresses themselves. We validate our models with a behavioral experiment, demonstrating that (1) our speaker model predicts human behavior, and (2) our pragmatic listener successfully recovers humans' reward functions. Finally, we show that this form of social learning can integrate with and reduce regret in traditional reinforcement learning. We hope these insights facilitate a shift from developing agents that obey language to agents that learn from it.",,https://openreview.net/forum?id=ZLsZmNe1RDb,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54022.png?t=1668560507.9458802,https://neurips.cc/virtual/2022/poster/54022,Not relevant,Other aspects,"['Instructions', 'Descriptions', 'Autonomy', 'Contextual bandit setting', 'Human communication', 'Policy', 'Reward function', 'Social learning', 'Reinforcement learning']",Not clear,,,,,,
1811,https://neurips.cc/virtual/2022/poster/53022,Diffusion Visual Counterfactual Explanations,Poster,NeurIPS,2022,"Visual Counterfactual Explanations (VCEs) are an important tool to understand the decisions of an image classifier. They are “small” but “realistic” semantic changes of the image changing the classifier decision. Current approaches for the generation of VCEs are restricted to adversarially robust models and often contain non-realistic artefacts, or are limited to image classification problems with few classes. In this paper, we overcome this by generating Diffusion Visual Counterfactual Explanations (DVCEs) for arbitrary ImageNet classifiers via a diffusion process. Two modifications to the diffusion process are key for our DVCEs: first, an adaptive parameterization, whose hyperparameters generalize across images and models, together with distance regularization and late start of the diffusion process, allow us to generate images with minimal semantic changes to the original ones but different classification. Second, our cone regularization via an adversarially robust model ensures that the diffusion process does not converge to trivial non-semantic changes, but instead produces realistic images of the target class which achieve high confidence by the classifier.",,https://openreview.net/forum?id=7SEi-ISNni7,https://neurips.cc/virtual/2022/poster/53022,https://neurips.cc/virtual/2022/poster/53022,Evasion,Defence,"['Visual Counterfactual Explanations (VCEs)', 'Image classifier', 'Adversarially robust models', 'Diffusion Visual Counterfactual Explanations (DVCEs)', 'ImageNet classifiers', 'Diffusion process', 'Adaptive parameterization', 'Distance regularization', 'Late start', 'Cone regularization']",Not clear,Other aspects,Robustness,,,Arguable,Wrong
1812,https://neurips.cc/virtual/2022/poster/54151,Semi-Supervised Video Salient Object Detection Based on Uncertainty-Guided Pseudo Labels,Poster,NeurIPS,2022,"Semi-Supervised Video Salient Object Detection (SS-VSOD) is challenging because of the lack of temporal information in video sequences caused by sparse annotations. Most works address this problem by generating pseudo labels for unlabeled data. However, error-prone pseudo labels negatively affect the VOSD model. Therefore, a deeper insight into pseudo labels should be developed. In this work, we aim to explore 1) how to utilize the incorrect predictions in pseudo labels to guide the network to generate more robust pseudo labels and 2) how to further screen out the noise that still exists in the improved pseudo labels. To this end, we propose an Uncertainty-Guided Pseudo Label Generator (UGPLG), which makes full use of inter-frame information to ensure the temporal consistency of the pseudo labels and improves the robustness of the pseudo labels by strengthening the learning of difficult scenarios. Furthermore, we also introduce the adversarial learning to address the noise problems in pseudo labels, guaranteeing the positive guidance of pseudo labels during model training. Experimental results demonstrate that our methods outperform existing semi-supervised method and partial fully-supervised methods across five public benchmarks of DAVIS, FBMS, MCL, ViSal and SegTrack-V2.",,https://openreview.net/forum?id=BOQr80FBX_,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/5bf73bc6c6e6775d472621264309a88b.png?t=1667649186.256586,https://neurips.cc/virtual/2022/poster/54151,Not relevant,Defence,"['Semi-Supervised', 'Video Salient Object Detection', 'Uncertainty-Guided Pseudo Labels', 'Temporal consistency', 'Adversarial learning', 'Robustness', 'Pseudo labels']","['DAVIS', 'FBMS', 'MCL', 'ViSal', 'SegTrack-V2']",,,,,,
1816,https://neurips.cc/virtual/2022/poster/52827,Accelerating Certified Robustness Training via Knowledge Transfer,Poster,NeurIPS,2022,"Training deep neural network classifiers that are certifiably robust against adversarial attacks is critical to ensuring the security and reliability of AI-controlled systems. Although numerous state-of-the-art certified training methods have been developed, they are computationally expensive and scale poorly with respect to both dataset and network complexity. Widespread usage of certified training is further hindered by the fact that periodic retraining is necessary to incorporate new data and network improvements. In this paper, we propose Certified Robustness Transfer (CRT), a general-purpose framework for reducing the computational overhead of any certifiably robust training method through knowledge transfer. Given a robust teacher, our framework uses a novel training loss to transfer the teacher’s robustness to the student. We provide theoretical and empirical validation of CRT. Our experiments on CIFAR-10 show that CRT speeds up certified robustness training by 8× on average across three different architecture generations while achieving comparable robustness to state-of-the-art methods. We also show that CRT can scale to large-scale datasets like ImageNet.",,https://openreview.net/forum?id=QFMw21ZKaa_,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/52827.png?t=1669406430.5415485,https://neurips.cc/virtual/2022/poster/52827,Evasion,Defence,"['Certified robustness training', 'Knowledge transfer', 'Adversarial attack', 'Deep neural network', 'AI-controlled systems', 'Robust teacher', 'Student', 'CIFAR-10', 'ImageNet']",['None'],,,,,,
1832,https://neurips.cc/virtual/2022/poster/53287,Finding Optimal Arms in Non-stochastic Combinatorial Bandits with Semi-bandit Feedback and Finite Budget,Poster,NeurIPS,2022,"We consider the combinatorial bandits problem with semi-bandit feedback under finite sampling budget constraints, in which the learner can carry out its action only for a limited number of times specified by an overall budget. The action is to choose a set of arms, whereupon feedback for each arm in the chosen set is received. Unlike existing works, we study this problem in a non-stochastic setting with subset-dependent feedback, i.e., the semi-bandit feedback received could be generated by an oblivious adversary and also might depend on the chosen set of arms. In addition, we consider a general feedback scenario covering both the numerical-based as well as preference-based case and introduce a sound theoretical framework for this setting guaranteeing sensible notions of optimal arms, which a learner seeks to find. We suggest a generic algorithm suitable to cover the full spectrum of conceivable arm elimination strategies from aggressive to conservative. Theoretical questions about the sufficient and necessary budget of the algorithm to find the best arm are answered and complemented by deriving lower bounds for any learning algorithm for this problem scenario.",,https://openreview.net/forum?id=h37KyWDDC6B,https://neurips.cc/virtual/2022/poster/53287,https://neurips.cc/virtual/2022/poster/53287,Not relevant,Other aspects,"['combinatorial bandits', 'semi-bandit feedback', 'finite budget', 'subset-dependent feedback', 'non-stochastic', 'numerical-based', 'preference-based', 'optimal arms', 'aggressive', 'conservative', 'learning algorithm', 'lower bounds']",,Not relevant,Not relevant,,,,
1835,https://neurips.cc/virtual/2022/poster/53485,Clipped Stochastic Methods for Variational Inequalities with Heavy-Tailed Noise,Poster,NeurIPS,2022,"Stochastic first-order methods such as Stochastic Extragradient (SEG) or Stochastic Gradient Descent-Ascent (SGDA) for solving smooth minimax problems and, more generally, variational inequality problems (VIP) have been gaining a lot of attention in recent years due to the growing popularity of adversarial formulations in machine learning. While high-probability convergence bounds are known to more accurately reflect the actual behavior of stochastic methods, most convergence results are provided in expectation. Moreover, the only known high-probability complexity results have been derived under restrictive sub-Gaussian (light-tailed) noise and bounded domain assumptions [Juditsky et al., 2011]. In this work, we prove the first high-probability complexity results with logarithmic dependence on the confidence level for stochastic methods for solving monotone and structured non-monotone VIPs with non-sub-Gaussian (heavy-tailed) noise and unbounded domains. In the monotone case, our results match the best known ones in the light-tails case [Juditsky et al., 2011], and are novel for structured non-monotone problems such as negative comonotone, quasi-strongly monotone, and/or star-cocoercive ones. We achieve these results by studying SEG and SGDA with clipping. In addition, we numerically validate that the gradient noise of many practical GAN formulations is heavy-tailed and show that clipping improves the performance of SEG/SGDA.",,https://openreview.net/forum?id=S4KGBKBhCPo,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53485.png?t=1669113237.9243963,https://neurips.cc/virtual/2022/poster/53485,Not relevant,Defence,"['Stochastic first-order methods', 'Stochastic Extragradient', 'Stochastic Gradient Descent-Ascent', 'variational inequality problems', 'adversarial formulations', 'machine learning', 'high-probability convergence bounds', 'non-sub-Gaussian noise', 'unbounded domains', 'monotone', 'structured non-monotone', 'negative comonotone', 'quasi-strongly monotone', 'star-cocoercive', 'SEG', 'SGDA', 'clipping', 'GAN formulations']",['none'],Not relevant,Not relevant,,,,
1854,https://neurips.cc/virtual/2022/poster/54423,DigGAN: Discriminator gradIent Gap Regularization for GAN Training with Limited Data,Poster,NeurIPS,2022,"Generative adversarial nets (GANs) have been remarkably successful at learning to sample from distributions specified by a given dataset, particularly if the given dataset is reasonably large compared to its dimensionality. However, given limited data, classical GANs have struggled, and strategies like output-regularization, data-augmentation, use of pre-trained models and pruning have been shown to lead to improvements. Notably, the applicability of these strategies is often constrained to particular settings, e.g., availability of a pretrained GAN, or increases training time, e.g., when using pruning. In contrast, we propose a  Discriminator gradIent Gap regularized GAN (DigGAN) formulation which can be added to any existing GAN. DigGAN augments existing GANs by encouraging to narrow the gap between the norm of the gradient of a discriminator's prediction w.r.t. real images and w.r.t. the generated samples. We observe this formulation to avoid bad attractors within the GAN loss landscape, and we find DigGAN to significantly improve the results of GAN training when limited data is available.",,https://openreview.net/forum?id=azBVn74t_2,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54423.png?t=1669251090.901316,https://neurips.cc/media/neurips-2022/Slides/54423.pdf,Not relevant,Defence,"['generative adversarial nets', 'GANs', 'limited data', 'Discriminator gradIent Gap regularization', 'GAN training']",['None'],Not relevant,Not relevant,,,,
1859,https://neurips.cc/virtual/2022/poster/53891,"Robustness in deep learning: The good (width), the bad (depth), and the ugly (initialization)",Poster,NeurIPS,2022,"We study the average robustness notion in deep neural networks in (selected) wide and narrow, deep and shallow, as well as lazy and non-lazy training settings. We prove that in the under-parameterized setting, width has a negative effect while it improves robustness in the over-parameterized setting. The effect of depth closely depends on the initialization and the training mode. In particular, when initialized with LeCun initialization, depth helps robustness with the lazy training regime. In contrast, when initialized with Neural Tangent Kernel (NTK) and He-initialization, depth hurts the robustness. Moreover, under the non-lazy training regime, we demonstrate how the width of a two-layer ReLU network benefits robustness. Our theoretical developments improve the results by [Huang et al. NeurIPS21; Wu et al. NeurIPS21] and are consistent with [Bubeck and Sellke NeurIPS21; Bubeck et al. COLT21].",,https://openreview.net/forum?id=m8vzptcFKsT,https://neurips.cc/virtual/2022/poster/53891,https://neurips.cc/virtual/2022/poster/53891,Robustness,Defence,"['average robustness', 'deep neural networks', 'under-parameterized setting', 'over-parameterized setting', 'LeCun initialization', 'Neural Tangent Kernel (NTK)', 'He-initialization', 'lazy training regime', 'non-lazy training regime', 'width', 'depth', 'initialization', 'two-layer ReLU network']",,,,,,,
1865,https://neurips.cc/virtual/2022/poster/54478,No-regret learning in games with noisy feedback: Faster rates and adaptivity via learning rate separation,Poster,NeurIPS,2022,"We examine the problem of regret minimization when the learner is involved in a continuous game with other optimizing agents: in this case, if all players follow a no-regret algorithm, it is possible to achieve significantly lower regret relative to fully adversarial environments. We study this problem in the context of variationally stable games (a class of continuous games which includes all convex-concave and monotone games), and when the players only have access to noisy estimates of their individual payoff gradients. If the noise is additive, the game-theoretic and purely adversarial settings enjoy similar regret guarantees; however, if the noise is \emph{multiplicative}, we show that the learners can, in fact, achieve \emph{constant} regret. We achieve this faster rate via an optimistic gradient scheme with \emph{learning rate separation} \textendash\ that is, the method's extrapolation and update steps are tuned to different schedules, depending on the noise profile. Subsequently, to eliminate the need for delicate hyperparameter tuning, we propose a fully adaptive method that smoothly interpolates between worst- and best-case regret guarantees.",,https://openreview.net/forum?id=dpYhDYjl4O,https://neurips.cc/virtual/2022/poster/54478,https://neurips.cc/virtual/2022/poster/54478,Not relevant,Defence,"['No-regret learning', 'games with noisy feedback', 'learning rate separation', 'regret minimization', 'variationally stable games', 'payoff gradients', 'noise', 'multiplicative noise', 'optimistic gradient scheme', 'adaptive method']",,,,,,,
1875,https://neurips.cc/virtual/2022/poster/55009,Tikhonov Regularization is Optimal Transport Robust under Martingale Constraints,Poster,NeurIPS,2022,"Distributionally robust optimization (DRO) has been shown to offer a principled way to regularize learning models. In this paper, we find that Tikhonov regularization is distributionally robust in an optimal transport sense (i.e. if an adversary chooses distributions in a suitable optimal transport neighborhood of the empirical measure), provided that suitable martingale constraints are also imposed. Further, we introduce a relaxation of the martingale constraints which not only provide a unified viewpoint to a class of existing robust methods but also lead to new regularization tools. To realize these novel tools,  provably efficient computational algorithms are proposed. As a byproduct, the strong duality theorem proved in this paper can be potentially applied to other problems of independent interest. ",,https://openreview.net/forum?id=EQgPNPwREa,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/80537a945c7aaa788ccfcdf1b99b5d8f.png?t=1666562991.2833588,https://neurips.cc/virtual/2022/poster/55009,Not relevant,Defence,"['Distributionally robust optimization', 'Tikhonov regularization', 'Optimal transport', 'Martingale constraints', 'Adversary', 'Empirical measure', 'Regularization tools', 'Strong duality theorem']",,,,,,,
1881,https://neurips.cc/virtual/2022/poster/54928,ZIN: When and How to Learn Invariance Without Environment Partition?,Poster,NeurIPS,2022,"It is commonplace to encounter heterogeneous data, of which some aspects of the data distribution may vary  but the underlying causal mechanisms remain constant.  When data are divided into distinct environments according to the heterogeneity, recent invariant learning methods have proposed to learn robust and invariant models using this environment partition. It is hence tempting to utilize the inherent heterogeneity even when environment partition is not provided. Unfortunately, in this work, we show that learning invariant features under this circumstance is fundamentally impossible without further inductive biases or additional information. Then, we propose a framework to jointly learn environment partition and invariant representation, assisted by additional auxiliary information. We derive sufficient and necessary conditions for our framework to provably identify invariant features under a fairly general setting. Experimental results on both synthetic and real world datasets validate our analysis and demonstrate an improved performance of the proposed framework. Our findings also raise the need of making the role of  inductive biases more explicit when learning invariant models without environment partition in future works. Codes are available at https://github.com/linyongver/ZIN_official .",,https://openreview.net/forum?id=pUPFRSxfACD,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54928.png?t=1668945088.818897,https://neurips.cc/virtual/2022/poster/54928,Not relevant,Other aspects,"['invariant learning', 'environment partition', 'heterogeneous data', 'causal mechanisms', 'auxiliary information', 'inductive biases']",[],,,,,,
1883,https://neurips.cc/virtual/2022/poster/54924,Object-Category Aware Reinforcement Learning,Poster,NeurIPS,2022,"Object-oriented reinforcement learning (OORL) is a promising way to improve the sample efficiency and generalization ability over standard RL.  Recent works that try to solve OORL tasks without additional feature engineering mainly focus on learning the object representations and then solving tasks via reasoning based on these object representations. However, none of these works tries to explicitly model the inherent similarity between different object instances of the same category.  Objects of the same category should share similar functionalities; therefore, the category is the most critical property of an object. Following this insight, we propose a novel framework named Object-Category Aware Reinforcement Learning (OCARL), which utilizes the category information of objects to facilitate both perception and reasoning. OCARL consists of three parts: (1) Category-Aware Unsupervised Object Discovery (UOD),  which discovers the objects as well as their corresponding categories; (2) Object-Category Aware Perception, which encodes the category information and is also robust to the incompleteness of (1) at the same time; (3) Object-Centric Modular Reasoning, which adopts multiple independent and object-category-specific networks when reasoning based on objects. Our experiments show that OCARL can improve both the sample efficiency and generalization in the OORL domain.",,https://openreview.net/forum?id=9Qjn_3gWLDc,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54924.png?t=1669775461.6116197,https://neurips.cc/virtual/2022/poster/54924,Not relevant,Defence,"['Object-oriented reinforcement learning', 'Object-Category Aware Reinforcement Learning', 'Object representation', 'Category information', 'Perception', 'Reasoning', 'Sample efficiency', 'Generalization']",['OORL'],,,,,,
1886,https://neurips.cc/virtual/2022/poster/54973,Is this the Right Neighborhood? Accurate and Query Efficient Model Agnostic Explanations,Poster,NeurIPS,2022,"There have been multiple works that try to ascertain explanations for decisions of black box models on particular inputs by perturbing the input or by sampling around it, creating a neighborhood and then fitting a sparse (linear) model (e.g. LIME). Many of these methods are unstable and so more recent work tries to find stable or robust alternatives. However, stable solutions may not accurately represent the behavior of the model around the input. Thus, the question we ask in this paper is are we approximating the local boundary around the input accurately? In particular, are we sampling the right neighborhood so that a linear approximation of the black box is faithful to its true behavior around that input given that the black box can be highly non-linear (viz. deep relu network with many linear pieces). It is difficult to know the correct neighborhood width (or radius) as too small a width can lead to a bad condition number of the inverse covariance matrix of function fitting procedures resulting in unstable predictions, while too large a width may lead to accounting for multiple linear pieces and consequently a poor local approximation. We in this paper propose a simple approach that is robust across neighborhood widths in recovering faithful local explanations. In addition to a naive implementation of our approach which can still be accurate, we propose a novel adaptive neighborhood sampling scheme (ANS) that we formally show can be much more sample and query efficient. We then empirically evaluate our approach on  real data where our explanations are significantly more sample and query efficient than the competitors, while also being faithful and stable across different widths.",,https://openreview.net/forum?id=lJHkZbX6Ic1,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/28b60a16b55fd531047c0c958ce14b95.png?t=1666444062.5097592,https://neurips.cc/media/neurips-2022/Slides/54973.pdf,Not relevant,Other aspects,"['Model agnostic explanations', 'Black box models', 'Local boundary', 'Input', 'Non-linear', 'Deep relu network', 'Linear approximation', 'Neighborhood width', 'Inverse covariance matrix', 'Function fitting procedures', 'Adaptive neighborhood sampling scheme', 'ANS']",[''],,,,,,
1892,https://neurips.cc/virtual/2022/poster/54931,Distributional Reward Estimation for Effective Multi-agent Deep Reinforcement Learning,Poster,NeurIPS,2022,"Multi-agent reinforcement learning has drawn increasing attention in practice, e.g., robotics and automatic driving, as it can explore optimal policies using samples generated by interacting with the environment. However, high reward uncertainty still remains a problem when we want to train a satisfactory model, because obtaining high-quality reward feedback is usually expensive and even infeasible. To handle this issue, previous methods mainly focus on passive reward correction. At the same time, recent active reward estimation methods have proven to be a recipe for reducing the effect of reward uncertainty. In this paper, we propose a novel Distributional Reward Estimation framework for effective Multi-Agent Reinforcement Learning (DRE-MARL). Our main idea is to design the multi-action-branch reward estimation and policy-weighted reward aggregation for stabilized training. Specifically, we design the multi-action-branch reward estimation to model reward distributions on all action branches. Then we utilize reward aggregation to obtain stable updating signals during training. Our intuition is that consideration of all possible consequences of actions could be useful for learning policies. The superiority of the DRE-MARL is demonstrated using benchmark multi-agent scenarios, compared with the SOTA baselines in terms of both effectiveness and robustness.",,https://openreview.net/forum?id=4qR780g2Mg,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/30f8f6b940d1073d8b6a5eebc46dd6e5.png?t=1666772453.7614732,https://neurips.cc/virtual/2022/poster/54931,Not relevant,Defence,"['Multi-agent reinforcement learning', 'Reward estimation', 'Policy-weighted reward aggregation', 'Stabilized training']","['Robotics', 'Automatic driving']",,,,,,
1894,https://neurips.cc/virtual/2022/poster/54918,SageMix: Saliency-Guided Mixup for Point Clouds,Poster,NeurIPS,2022,"Data augmentation is key to improving the generalization ability of deep learning models. Mixup is a simple and widely-used data augmentation technique that has proven effective in alleviating the problems of overfitting and data scarcity. Also, recent studies of saliency-aware Mixup in the image domain show that preserving discriminative parts is beneficial to improving the generalization performance. However, these Mixup-based data augmentations are underexplored in 3D vision, especially in point clouds. In this paper, we propose SageMix, a saliency-guided Mixup for point clouds to preserve salient local structures. Specifically, we extract salient regions from two point clouds and smoothly combine them into one continuous shape. With a simple sequential sampling by re-weighted saliency scores, SageMix preserves the local structure of salient regions. Extensive experiments demonstrate that the proposed method consistently outperforms existing Mixup methods in various benchmark point cloud datasets. With PointNet++, our method achieves an accuracy gain of 2.6% and 4.0% over standard training in ModelNet40 and ScanObjectNN, respectively. In addition to generalization performance, SageMix improves robustness and uncertainty calibration. Moreover, when adopting our method to various tasks including part segmentation and standard image classification, our method achieves competitive performance. Code is available at https://github.com/mlvlab/SageMix.",,https://openreview.net/forum?id=q-FRENiEP_d,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54918.png?t=1668757862.157738,https://neurips.cc/virtual/2022/poster/54918,Not relevant,Defence,"['Point cloud', 'Data augmentation', 'Mixup', 'Saliency-Guided', '3D vision', 'Salient regions', 'Local structure', 'Saliency scores', 'Uncertainty calibration']","['ModelNet40', 'ScanObjectNN', 'part segmentation', 'image classification']",,,,,,
1897,https://neurips.cc/virtual/2022/poster/54905,Adv-Attribute: Inconspicuous and Transferable Adversarial Attack on Face Recognition,Poster,NeurIPS,2022,"Deep learning models have shown their vulnerability when dealing with adversarial attacks. Existing attacks almost perform on low-level instances, such as pixels and super-pixels, and rarely exploit semantic clues. For face recognition attacks, existing methods typically generate the l_p-norm perturbations on pixels, however, resulting in low attack transferability and high vulnerability to denoising defense models. In this work, instead of performing perturbations on the low-level pixels, we propose to generate attacks through perturbing on the high-level semantics to improve attack transferability. Specifically, a unified flexible framework, Adversarial Attributes (Adv-Attribute), is designed to generate inconspicuous and transferable attacks on face recognition, which crafts the adversarial noise and adds it into different attributes based on the guidance of the difference in face recognition features from the target. Moreover, the importance-aware attribute selection and the multi-objective optimization strategy are introduced to further ensure the balance of stealthiness and attacking strength. Extensive experiments on the FFHQ and CelebA-HQ datasets show that the proposed Adv-Attribute method achieves the state-of-the-art attacking success rates while maintaining better visual effects against recent attack methods.",,https://openreview.net/forum?id=d229wqASHOT,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54905.png?t=1668778273.923639,https://neurips.cc/virtual/2022/poster/54905,Evasion,Attack,"['adversarial attack', 'face recognition', 'high-level semantics', 'Adversarial Attributes', 'Adv-attribute', 'attack transferability', 'perturbations', 'low-level pixels', 'multi-objective optimization strategy', 'importance-aware attribute selection', 'stealthiness']",['Face Recognition'],,,,,,
1902,https://neurips.cc/virtual/2022/poster/54887,Noise Attention Learning: Enhancing Noise Robustness by Gradient Scaling,Poster,NeurIPS,2022,"Machine learning has been highly successful in data-driven applications but is often hampered when the data contains noise, especially label noise. When trained on noisy labels, deep neural networks tend to fit all noisy labels, resulting in poor generalization. To handle this problem, a common idea is to force the model to fit only clean samples rather than mislabeled ones. In this paper, we propose a simple yet effective method that automatically distinguishes the mislabeled samples and prevents the model from memorizing them, named Noise Attention Learning. In our method, we introduce an attention branch to produce attention weights based on representations of samples. This attention branch is learned to divide the samples according to the predictive power in their representations. We design the corresponding loss function that incorporates the attention weights for training the model without affecting the original learning direction. Empirical results show that most of the mislabeled samples yield significantly lower weights than the clean ones. Furthermore, our theoretical analysis shows that the gradients of training samples are dynamically scaled by the attention weights, implicitly preventing memorization of the mislabeled samples. Experimental results on two benchmarks (CIFAR-10 and CIFAR-100) with simulated label noise and three real-world noisy datasets (ANIMAL-10N, Clothing1M and Webvision) demonstrate that our approach outperforms state-of-the-art methods. ",,https://openreview.net/forum?id=yfNSUQ3yRo,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/bf2fb7d1825a1df3ca308ad0bf48591e.png?t=1667705535.7633424,https://neurips.cc/virtual/2022/poster/54887,Robustness,Defence,"['Noise robustness', 'Gradient scaling', 'Mislabeled samples', 'Noise Attention Learning', 'Deep neural networks', 'Generalization', 'Label noise', 'Predictive power']","['CIFAR-10', 'CIFAR-100', 'ANIMAL-10N', 'Clothing1M', 'Webvision']",,,,,,
1928,https://neurips.cc/virtual/2022/poster/54815,Information-Theoretic GAN Compression with Variational Energy-based Model,Poster,NeurIPS,2022,"We propose an information-theoretic knowledge distillation approach for the compression of generative adversarial networks, which aims to maximize the mutual information between teacher and student networks via a variational optimization based on an energy-based model. Because the direct computation of the mutual information in continuous domains is intractable, our approach alternatively optimizes the student network by maximizing the variational lower bound of the mutual information. To achieve a tight lower bound, we introduce an energy-based model relying on a deep neural network to represent a flexible variational distribution that deals with high-dimensional images and consider spatial dependencies between pixels, effectively. Since the proposed method is a generic optimization algorithm, it can be conveniently incorporated into arbitrary generative adversarial networks and even dense prediction networks, e.g., image enhancement models. We demonstrate that the proposed algorithm achieves outstanding performance in model compression of generative adversarial networks consistently when combined with several existing models. ",,https://openreview.net/forum?id=sRKNkpUMQNr,https://neurips.cc/virtual/2022/poster/54815,https://neurips.cc/virtual/2022/poster/54815,Not relevant,Defence,"['Information-Theoretic', 'GAN Compression', 'Variational Energy-based Model', 'Knowledge distillation', 'Mutual information', 'Teacher and student networks', 'Variational optimization', 'Energy-based model', 'Deep neural network', 'Variational distribution', 'High-dimensional images', 'Spatial dependencies', 'Model compression', 'Generative adversarial networks', 'Dense prediction networks', 'Image enhancement models']",['None'],,,,,,
1932,https://neurips.cc/virtual/2022/poster/54760,Truncated proposals for scalable and hassle-free simulation-based inference,Poster,NeurIPS,2022,"Simulation-based inference (SBI) solves statistical inverse problems by repeatedly running a stochastic simulator and inferring posterior distributions from model-simulations. To improve simulation efficiency, several inference methods take a sequential approach and iteratively adapt the proposal distributions from which model simulations are generated. However, many of these sequential methods are difficult to use in practice, both because the resulting optimisation problems can be challenging and efficient diagnostic tools are lacking. To overcome these issues, we present Truncated Sequential Neural Posterior Estimation (TSNPE). TSNPE performs sequential inference with truncated proposals, sidestepping the optimisation issues of alternative approaches. In addition, TSNPE allows to efficiently perform coverage tests that can scale to complex models with many parameters. We demonstrate that TSNPE performs on par with previous methods on established benchmark tasks. We then apply TSNPE to two challenging problems from neuroscience and show that TSNPE can successfully obtain the posterior distributions, whereas previous methods fail. Overall, our results demonstrate that TSNPE is an efficient, accurate, and robust inference method that can scale to challenging scientific models.",,https://openreview.net/forum?id=QW98XBAqNRa,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54760.png?t=1669406596.0668702,https://neurips.cc/virtual/2022/poster/54760,Not relevant,Defence,"['Simulation-based inference', 'sequential inference', 'Truncated Sequential Neural Posterior Estimation (TSNPE)', 'coverage tests', 'inference method', 'scientific models']",['Neuroscience'],,,,,,
1935,https://neurips.cc/virtual/2022/poster/54779,ELASTIC: Numerical Reasoning with Adaptive Symbolic Compiler,Poster,NeurIPS,2022,"Numerical reasoning over text is a challenging task of Artificial Intelligence (AI), requiring reading comprehension and numerical reasoning abilities. Previous approaches use numerical reasoning programs to represent the reasoning process. However, most works do not separate the generation of operators and operands, which are key components of a numerical reasoning program, thus limiting their ability to generate such programs for complicated tasks. In this paper, we introduce the numEricaL reASoning with adapTive symbolIc Compiler (ELASTIC) model, which is constituted of the RoBERTa as the Encoder and a Compiler with four modules: Reasoning Manager, Operator Generator, Operands Generator, and Memory Register. ELASTIC is robust when conducting complicated reasoning. Also, it is domain agnostic by supporting the expansion of diverse operators without caring about the number of operands it contains. Experiments show that ELASTIC achieves 68.96 and 65.21 of execution accuracy and program accuracy on the FinQA dataset and 83.00 program accuracy on the MathQA dataset, outperforming previous state-of-the-art models significantly.",,https://openreview.net/forum?id=gd7ZI0X7Q-h,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54779.png?t=1669032573.1058102,https://neurips.cc/virtual/2022/poster/54779,Not relevant,Other aspects,"['Numerical reasoning', 'Adaptive symbolic compiler', 'Artificial Intelligence', 'RoBERTa', 'Encoder', 'Compiler', 'Reasoning Manager', 'Operator Generator', 'Operands Generator', 'Memory Register', 'FinQA dataset', 'MathQA dataset']",['None'],,,,,,
1941,https://neurips.cc/virtual/2022/poster/54776,Randomized Message-Interception Smoothing: Gray-box Certificates for Graph Neural Networks,Poster,NeurIPS,2022,"Randomized smoothing is one of the most promising frameworks for certifying the adversarial robustness of machine learning models, including Graph Neural Networks (GNNs). Yet, existing randomized smoothing certificates for GNNs are overly pessimistic since they treat the model as a black box, ignoring the underlying architecture. To remedy this, we propose novel gray-box certificates that exploit the message-passing principle of GNNs: We randomly intercept messages and carefully analyze the probability that messages from adversarially controlled nodes reach their target nodes. Compared to existing certificates, we certify robustness to much stronger adversaries that control entire nodes in the graph and can arbitrarily manipulate node features. Our certificates provide stronger guarantees for attacks at larger distances, as messages from farther-away nodes are more likely to get intercepted. We demonstrate the effectiveness of our method on various models and datasets. Since our gray-box certificates consider the underlying graph structure, we can significantly improve certifiable robustness by applying graph sparsification.",,https://openreview.net/forum?id=t0VbBTw-o8,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54776.png?t=1669727741.521229,https://neurips.cc/virtual/2022/poster/54776,Evasion,Defence,"['Randomized smoothing', 'Graph Neural Networks (GNNs)', 'Gray-box certificates', 'Message-passing principle', 'Adversarial robustness', 'Node features', 'Graph structure', 'Graph sparsification']",['Graph Neural Networks'],,,,,,
1965,https://neurips.cc/virtual/2022/poster/54665,CHIMLE: Conditional Hierarchical IMLE for Multimodal Conditional Image Synthesis,Poster,NeurIPS,2022,"A persistent challenge in conditional image synthesis has been to generate diverse output images from the same input image despite only one output image being observed per input image. GAN-based methods are prone to mode collapse, which leads to low diversity. To get around this, we leverage Implicit Maximum Likelihood Estimation (IMLE) which can overcome mode collapse fundamentally. IMLE uses the same generator as GANs but trains it with a different, non-adversarial objective which ensures each observed image has a generated sample nearby. Unfortunately, to generate high-fidelity images, prior IMLE-based methods require a large number of samples, which is expensive. In this paper, we propose a new method to get around this limitation, which we dub Conditional Hierarchical IMLE (CHIMLE), which can generate high-fidelity images without requiring many samples. We show CHIMLE significantly outperforms the prior best IMLE, GAN and diffusion-based methods in terms of image fidelity and mode coverage across four tasks, namely night-to-day, 16x single image super-resolution, image colourization and image decompression. Quantitatively, our method improves Fréchet Inception Distance (FID) by 36.9% on average compared to the prior best IMLE-based method, and by 27.5% on average compared to the best non-IMLE-based general-purpose methods. More results and code are available on the project website at https://niopeng.github.io/CHIMLE/.",,https://openreview.net/forum?id=5pvB6IH_9UZ,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54665.png?t=1669343530.4191723,https://neurips.cc/virtual/2022/poster/54665,Not relevant,Defence,"['Conditional image synthesis', 'GAN', 'IMLE', 'Mode collapse', 'Fréchet Inception Distance (FID)', 'High-fidelity images', 'Image Fidelity', 'Mode coverage', 'Night-to-day', '16x single image super-resolution', 'Image colourization', 'Image decompression']",,Not relevant,Not relevant,,,,
1968,https://neurips.cc/virtual/2022/poster/54647,Cluster Randomized Designs for One-Sided Bipartite Experiments,Poster,NeurIPS,2022,"The conclusions of randomized controlled trials may be biased when the outcome of one unit depends on the treatment status of other units, a problem known as \textit{interference}. In this work, we study interference in the setting of one-sided bipartite experiments in which the experimental units---where treatments are randomized and outcomes are measured---do not interact directly. Instead, their interactions are mediated through their connections to \textit{interference units} on the other side of the graph. Examples of this type of interference are common in marketplaces and two-sided platforms. The \textit{cluster-randomized design} is a popular method to mitigate interference when the graph is known, but it has not been well-studied in the one-sided bipartite experiment setting. In this work, we formalize a natural model for interference in one-sided bipartite experiments using the exposure mapping framework. We first exhibit settings under which existing cluster-randomized designs fail to properly mitigate interference under this model. We then show that minimizing the bias of the difference-in-means estimator under our model results in a balanced partitioning clustering objective with a natural interpretation. We further prove that our design is minimax optimal over the class of linear potential outcomes models with bounded interference. We conclude by providing theoretical and experimental evidence of the robustness of our design to a variety of interference graphs and potential outcomes models.",,https://openreview.net/forum?id=hqtSdpAK39W,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54647.png?t=1669250419.9193192,https://neurips.cc/virtual/2022/poster/54647,Not relevant,Defence,"['Cluster randomized designs', 'One-sided bipartite experiments', 'Interference', 'Marketplaces', 'Two-sided platforms', 'Exposure mapping framework', 'Difference-in-means estimator', 'Balanced partitioning clustering', 'Linear potential outcomes models', 'Bounded interference', 'Robustness']","['Marketplaces', 'Two-sided platforms']",,,,,,
1982,https://neurips.cc/virtual/2022/poster/54578,Efficient Submodular Optimization under Noise: Local Search is Robust,Poster,NeurIPS,2022,"The problem of monotone submodular maximization has been studied extensively due to its wide range of applications. However, there are cases where one can only access the objective function in a distorted or noisy form because of the uncertain nature or the errors involved in the evaluation. This paper considers the problem of constrained monotone submodular maximization with noisy oracles introduced by Hassidim and Singer (2017). For a cardinality constraint, we propose an algorithm achieving a near-optimal (1-1/e-O(epsilon))-approximation guarantee (for arbitrary epsilon > 0) with only a polynomial number of queries to the noisy value oracle, which improves the exponential query complexity of Singer and Hassidim (2018). For general matroid constraints, we show the first constant approximation algorithm in the presence of noise. Our main approaches are to design a novel local search framework that can handle the effect of noise and to construct certain smoothing surrogate functions for noise reduction.",,https://openreview.net/forum?id=OlDEMIbCvTl,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54578.png?t=1668498438.914236,https://neurips.cc/virtual/2022/poster/54578,Not relevant,Defence,"['monotone submodular maximization', 'noisy oracles', 'cardinality constraint', 'approximation guarantee', 'noise reduction']",['None'],,,,,,
1993,https://neurips.cc/virtual/2022/poster/54551,First-Order Algorithms for Min-Max Optimization in Geodesic Metric Spaces,Poster,NeurIPS,2022,"From optimal transport to robust dimensionality reduction, many machine learning applicationscan be cast into the min-max optimization problems over Riemannian manifolds. Though manymin-max algorithms have been analyzed in the Euclidean setting, it has been elusive how theseresults translate to the Riemannian case. Zhang et al. (2022) have recently identified that geodesic convexconcave Riemannian problems admit always Sion’s saddle point solutions. Immediately, an importantquestion that arises is if a performance gap between the Riemannian and the optimal Euclidean spaceconvex concave algorithms is necessary. Our work is the first to answer the question in the negative:We prove that the Riemannian corrected extragradient (RCEG) method achieves last-iterate at alinear convergence rate at the geodesically strongly convex concave case, matching the euclidean one.Our results also extend to the stochastic or non-smooth case where RCEG & Riemanian gradientascent descent (RGDA) achieve respectively near-optimal convergence rates up to factors dependingon curvature of the manifold. Finally, we empirically demonstrate the effectiveness of RCEG insolving robust PCA.",,https://openreview.net/forum?id=zJNqte0b-xn,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54551.png?t=1669663619.4642785,https://neurips.cc/virtual/2022/poster/54551,Not relevant,Defence,"['min-max optimization', 'Riemannian manifolds', 'geodesic convex-concave', 'Riemannian corrected extragradient', 'Riemannian gradient ascent-descent', 'curvature of the manifold', 'robust PCA']",['Robust PCA'],,,,,,
1996,https://neurips.cc/virtual/2022/poster/54553,Efficient and Effective Augmentation Strategy for Adversarial Training,Poster,NeurIPS,2022,"Adversarial training of Deep Neural Networks is known to be significantly more data-hungry when compared to standard training. Furthermore, complex data augmentations such as AutoAugment, which have led to substantial gains in standard training of image classifiers, have not been successful with Adversarial Training. We first explain this contrasting behavior by viewing augmentation during training as a problem of domain generalization, and further propose Diverse Augmentation-based Joint Adversarial Training (DAJAT) to use data augmentations effectively in adversarial training. We aim to handle the conflicting goals of enhancing the diversity of the training dataset and training with data that is close to the test distribution by using a combination of simple and complex augmentations with separate batch normalization layers during training. We further utilize the popular Jensen-Shannon divergence loss to encourage the \emph{joint} learning of the \emph{diverse augmentations}, thereby allowing simple augmentations to guide the learning of complex ones. Lastly, to improve the computational efficiency of the proposed method, we propose and utilize a two-step defense, Ascending Constraint Adversarial Training (ACAT), that uses an increasing epsilon schedule and weight-space smoothing to prevent gradient masking. The proposed method DAJAT achieves substantially better robustness-accuracy trade-off when compared to existing methods on the RobustBench Leaderboard on ResNet-18 and WideResNet-34-10. The code for implementing DAJAT is available here: https://github.com/val-iisc/DAJAT",,https://openreview.net/forum?id=ODkBI1d3phW,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54553.png?t=1669375175.6653566,https://neurips.cc/virtual/2022/poster/54553,Evasion,Defence,"['Adversarial training', 'Deep Neural Networks', 'Data augmentations', 'Domain generalization', 'Jensen-Shannon divergence loss', 'Gradient masking', 'Robustness-accuracy trade-off', 'RobustBench Leaderboard']",['None'],,,,,,
2004,https://neurips.cc/virtual/2022/poster/54506,RKHS-SHAP: Shapley Values for Kernel Methods,Poster,NeurIPS,2022,"Feature attribution for kernel methods is often heuristic and not individualised for each prediction. To address this, we turn to the concept of Shapley values (SV), a coalition game theoretical framework that has previously been applied to different machine learning model interpretation tasks, such as linear models, tree ensembles and deep networks. By analysing SVs from a functional perspective, we propose RKHS-SHAP, an attribution method for kernel machines that can efficiently compute both Interventional and Observational Shapley values using kernel mean embeddings of distributions. We show theoretically that our method is robust with respect to local perturbations - a key yet often overlooked desideratum for consistent model interpretation. Further, we propose Shapley regulariser, applicable to a general empirical risk minimisation framework, allowing learning while controlling the level of specific feature's contributions to the model. We demonstrate that the Shapley regulariser enables learning which is robust to covariate shift of a given feature and fair learning which controls the SVs of sensitive features. ",,https://openreview.net/forum?id=gnc2VJHXmsG,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54506.png?t=1668368637.8017378,https://neurips.cc/virtual/2022/poster/54506,Not relevant,Defence,"['Shapley values', 'kernel methods', 'feature attribution', 'Interventional Shapley values', 'Observational Shapley values', 'kernel mean embeddings', 'local perturbations', 'Shapley regularizer', 'empirical risk minimization', 'covariate shift', 'fair learning']",,,,,,,
2005,https://neurips.cc/virtual/2022/poster/54531,Non-stationary Bandits with Knapsacks,Poster,NeurIPS,2022,"In this paper, we study the problem of bandits with knapsacks (BwK) in a non-stationary environment. The BwK problem generalizes the multi-arm bandit (MAB) problem to model the resource consumption associated with playing each arm. At each time, the decision maker/player chooses to play an arm, and s/he will receive a reward and consume certain amount of resource from each of the multiple resource types. The objective is to maximize the cumulative reward over a finite horizon subject to some knapsack constraints on the resources. Existing works study the BwK problem under either a stochastic or adversarial environment. Our paper considers a non-stationary environment which continuously interpolates between these two extremes. We first show that the traditional notion of variation budget is insufficient to characterize the non-stationarity of the BwK problem for a sublinear regret due to the presence of the constraints, and then we propose a new notion of global non-stationarity measure. We employ both non-stationarity measures to derive upper and lower bounds for the problem. Our results are based on a primal-dual analysis of the underlying linear programs and highlight the interplay between the constraints and the non-stationarity. Finally, we also extend the non-stationarity measure to the problem of online convex optimization with constraints and obtain new regret bounds accordingly. ",,https://openreview.net/forum?id=OVb3ZY0fzMk,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54531.png?t=1669731279.5144978,https://neurips.cc/virtual/2022/poster/54531,Not relevant,Defence,"['non-stationary bandits with knapsacks', 'resource consumption', 'multi-arm bandit', 'cumulative reward', 'knapsack constraints', 'stochastic', 'adversarial environment', 'variation budget', 'sublinear regret', 'global non-stationarity measure', 'primal-dual analysis', 'online convex optimization with constraints', 'regret bounds']",,Not relevant,Not relevant,,,,
2008,https://neurips.cc/virtual/2022/poster/54512,Adversarial Unlearning: Reducing Confidence Along Adversarial Directions,Poster,NeurIPS,2022,"Supervised learning methods trained with maximum likelihood objectives often overfit on training data. Most regularizers that prevent overfitting look to increase confidence on additional examples (e.g., data augmentation, adversarial training), or reduce it on training data (e.g., label smoothing). In this work we propose a complementary regularization strategy that reduces confidence on self-generated examples. The method, which we call RCAD (Reducing Confidence along Adversarial Directions), aims to reduce confidence on out-of-distribution examples lying along directions adversarially chosen to increase training loss. In contrast to adversarial training, RCAD does not try to robustify the model to output the original label, but rather regularizes it to have reduced confidence on points generated using much larger perturbations than in conventional adversarial training. RCAD can be easily integrated into training pipelines with a few lines of code. Despite its simplicity, we find on many classification benchmarks that RCAD can be added to existing techniques (e.g., label smoothing, MixUp training) to increase test accuracy by 1-3% in absolute value, with more significant gains in the low data regime. We also provide a theoretical analysis that helps to explain these benefits in simplified settings, showing that RCAD can provably help the model unlearn spurious features in the training data.",,https://openreview.net/forum?id=cJ006qBE8Uv,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54512.png?t=1669635202.5250704,https://neurips.cc/virtual/2022/poster/54512,Other attack,Defence,"['Adversarial unlearning', 'Reducing Confidence', 'Overfitting', 'Regularization', 'Out-of-distribution examples', 'Adversarial Training', 'Test accuracy', 'Spurious features']",,,,,,,
2012,https://neurips.cc/virtual/2022/poster/54500,LIFT: Language-Interfaced Fine-Tuning for Non-language Machine Learning Tasks,Poster,NeurIPS,2022,"Fine-tuning pretrained language models (LMs) without making any architectural changes has become a norm for learning various language downstream tasks. However, for non-language downstream tasks, a common practice is to employ task-specific designs for input, output layers, and loss functions. For instance, it is possible to fine-tune an LM into an MNIST classifier by replacing the word embedding layer with an image patch embedding layer, the word token output layer with a 10-way output layer, and the word prediction loss with a 10-way classification loss, respectively. A natural question arises: Can LM fine-tuning solve non-language downstream tasks without changing the model architecture or loss function? To answer this, we propose Language-Interfaced Fine-Tuning (LIFT) and study its efficacy and limitations by conducting an extensive empirical study on a suite of non-language classification and regression tasks. LIFT does not make any changes to the model architecture or loss function, and it solely relies on the natural language interface, enabling ""no-code machine learning with LMs.""  We find that LIFT performs comparably well across a wide range of low-dimensional classification and regression tasks, matching the performances of the best baselines in many cases, especially for the classification tasks. We also report experimental results on the fundamental properties of LIFT, including inductive bias, robustness, and sample complexity. We also analyze the effect of pretraining on LIFT and a few properties/techniques specific to LIFT, e.g., context-aware learning via appropriate prompting, calibrated predictions, data generation, and two-stage fine-tuning. Our code is available at https://github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning.",,https://openreview.net/forum?id=s_PJMEGIUfa,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54500.png?t=1669498072.6368294,https://neurips.cc/virtual/2022/poster/54500,Not relevant,Defence,"['Fine-tuning', 'Language models', 'Non-language tasks', 'Classification', 'Regression', 'Inductive bias', 'Robustness', 'Sample complexity', 'Pretraining', 'Context-aware learning', 'Prompting', 'Calibrated predictions', 'Data generation', 'Two-stage fine-tuning']",['None'],,,,,,
2023,https://neurips.cc/virtual/2022/poster/54425,GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech,Poster,NeurIPS,2022,"Style transfer for out-of-domain (OOD) speech synthesis aims to generate speech samples with unseen style (e.g., speaker identity, emotion, and prosody) derived from an acoustic reference, while facing the following challenges: 1) The highly dynamic style features in expressive voice are difficult to model and transfer; and 2) the TTS models should be robust enough to handle diverse OOD conditions that differ from the source data. This paper proposes GenerSpeech, a text-to-speech model towards high-fidelity zero-shot style transfer of OOD custom voice. GenerSpeech decomposes the speech variation into the style-agnostic and style-specific parts by introducing two components: 1) a multi-level style adaptor to efficiently model a large range of style conditions, including global speaker and emotion characteristics, and the local (utterance, phoneme, and word-level) fine-grained prosodic representations; and 2) a generalizable content adaptor with Mix-Style Layer Normalization to eliminate style information in the linguistic content representation and thus improve model generalization. Our evaluations on zero-shot style transfer demonstrate that GenerSpeech surpasses the state-of-the-art models in terms of audio quality and style similarity. The extension studies to adaptive style transfer further show that GenerSpeech performs robustly in the few-shot data setting. Audio samples are available at \url{https://GenerSpeech.github.io/}. ",,https://openreview.net/forum?id=dmCyoqxEwHf,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/0f089a3bcf38d052f7882d12b3923a82.png?t=1666427141.8936596,https://neurips.cc/media/neurips-2022/Slides/54425.pdf,Not relevant,Other aspects,"['style transfer', 'out-of-domain', 'speech synthesis', 'voice', 'prosody', 'multi-level style adaptor', 'generalizable content adaptor', 'Mix-Style Layer Normalization', 'linguistic content representation', 'model generalization', 'zero-shot style transfer', 'few-shot data setting']",['Speech'],,,,,,
2040,https://neurips.cc/virtual/2022/poster/53811,Decision-based Black-box Attack Against Vision Transformers via Patch-wise Adversarial Removal,Poster,NeurIPS,2022,"Vision transformers (ViTs) have demonstrated impressive performance and stronger adversarial robustness compared to Convolutional Neural Networks (CNNs). On the one hand, ViTs' focus on global interaction between individual patches reduces the local noise sensitivity of images. On the other hand, the neglect of noise sensitivity differences between image regions by existing decision-based attacks further compromises the efficiency of noise compression, especially for ViTs. Therefore, validating the black-box adversarial robustness of ViTs when the target model can only be queried still remains a challenging problem. In this paper, we theoretically analyze the limitations of existing decision-based attacks from the perspective of noise sensitivity difference between regions of the image, and propose a new decision-based black-box attack against ViTs, termed Patch-wise Adversarial Removal (PAR). PAR divides images into patches through a coarse-to-fine search process and compresses the noise on each patch separately. PAR records the noise magnitude and noise sensitivity of each patch and selects the patch with the highest query value for noise compression. In addition, PAR can be used as a noise initialization method for other decision-based attacks to improve the noise compression efficiency on both ViTs and CNNs without introducing additional calculations. Extensive experiments on three datasets demonstrate that PAR achieves a much lower noise magnitude with the same number of queries.",,https://openreview.net/forum?id=CwQCeJnteii,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53811.png?t=1669040104.286187,https://neurips.cc/media/neurips-2022/Slides/53811.pdf,Evasion,Attack,"['Vision transformers', 'Adversarial robustness', 'Convolutional Neural Networks', 'Decision-based attack', 'Patch-wise Adversarial Removal', 'Noise sensitivity', 'Black-box attack']",,Attack,Evasion,,,,
2051,https://neurips.cc/virtual/2022/poster/53791,VoiceBlock: Privacy through Real-Time Adversarial Attacks with Audio-to-Audio Models,Poster,NeurIPS,2022,"As governments and corporations adopt deep learning systems to collect and analyze user-generated audio data, concerns about security and privacy naturally emerge in areas such as automatic speaker recognition. While audio adversarial examples offer one route to mislead or evade these invasive systems, they are typically crafted through time-intensive offline optimization, limiting their usefulness in streaming contexts. Inspired by architectures for audio-to-audio tasks such as denoising and speech enhancement, we propose a neural network model capable of adversarially modifying a user's audio stream in real-time. Our model learns to apply a time-varying finite impulse response (FIR) filter to outgoing audio, allowing for effective and inconspicuous perturbations on a small fixed delay suitable for streaming tasks. We demonstrate our model is highly effective at de-identifying user speech from speaker recognition and able to transfer to an unseen recognition system. We conduct a perceptual study and find that our method produces perturbations significantly less perceptible than baseline anonymization methods, when controlling for effectiveness. Finally, we provide an implementation of our model capable of running in real-time on a single CPU thread. Audio examples and code can be found at https://interactiveaudiolab.github.io/project/voiceblock.html.",,https://openreview.net/forum?id=8gQEmEgWAkc,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53791.png?t=1669698899.3272662,https://neurips.cc/virtual/2022/poster/53791,Evasion,Attack,"['audio adversarial examples', 'streaming contexts', 'audio-to-audio tasks', 'deep learning systems', 'user-generated audio data', 'security', 'privacy', 'automatic speaker recognition', 'time-varying finite impulse response (FIR) filter', 'speech enhancement', 'de-identifying user speech', 'perceptual study', 'anonymization methods', 'real-time']",['audio'],Attack,Evasion,,,,
2055,https://neurips.cc/virtual/2022/poster/53767,RORL: Robust Offline Reinforcement Learning via Conservative Smoothing,Poster,NeurIPS,2022,"Offline reinforcement learning (RL) provides a promising direction to exploit massive amount of offline data for complex decision-making tasks. Due to the distribution shift issue, current offline RL algorithms are generally designed to be conservative in value estimation and action selection. However, such conservatism can impair the robustness of learned policies when encountering observation deviation under realistic conditions, such as sensor errors and adversarial attacks. To trade off robustness and conservatism, we propose Robust Offline Reinforcement Learning (RORL) with a novel conservative smoothing technique. In RORL, we explicitly introduce regularization on the policy and the value function for states near the dataset, as well as additional conservative value estimation on these states. Theoretically, we show RORL enjoys a tighter suboptimality bound than recent theoretical results in linear MDPs. We demonstrate that RORL can achieve state-of-the-art performance on the general offline RL benchmark and is considerably robust to adversarial observation perturbations.",,https://openreview.net/forum?id=_QzJJGH_KE,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/2d5d4cf93ccf992b3fe617b32b8296a3.png?t=1666434736.688026,https://neurips.cc/media/neurips-2022/Slides/53767.pdf,Robustness,Defence,"['Robust Offline Reinforcement Learning', 'Conservative Smoothing', 'Offline reinforcement learning', 'Value estimation', 'Action selection', 'Sensor errors', 'Adversarial attacks', 'Regularization', 'Policy', 'Value function', 'Dataset', 'Suboptimality bound', 'Linear MDPs']",,,,,,,
2064,https://neurips.cc/virtual/2022/poster/53765,Model Preserving Compression for Neural Networks,Poster,NeurIPS,2022,"After training complex deep learning models, a common task is to compress the model to reduce compute and storage demands. When compressing, it is desirable to preserve the original model's per-example decisions (e.g., to go beyond top-1 accuracy or preserve robustness), maintain the network's structure, automatically determine per-layer compression levels, and eliminate the need for fine tuning. No existing compression methods simultaneously satisfy these criteria---we introduce a principled approach that does by leveraging interpolative decompositions. Our approach simultaneously selects and eliminates channels (analogously, neurons), then constructs an interpolation matrix that propagates a correction into the next layer, preserving the network's structure. Consequently, our method achieves good performance even without fine tuning and admits theoretical analysis. Our theoretical generalization bound for a one layer network lends itself naturally to a heuristic that allows our method to automatically choose per-layer sizes for deep networks. We demonstrate the efficacy of our approach with strong empirical performance on a variety of tasks, models, and datasets---from simple one-hidden-layer networks to deep networks on ImageNet.",,https://openreview.net/forum?id=gt-l9Hu2ndd,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53765.png?t=1669142964.387217,https://neurips.cc/virtual/2022/poster/53765,Not relevant,Defence,"['Model compression', 'Neural networks', 'Interpolative decompositions', 'Channel elimination', 'Theoretical generalization bound', 'Heuristic', 'Deep networks', 'ImageNet']",['Computer Vision'],,,,,,
2085,https://neurips.cc/virtual/2022/poster/56120,All You Need is a Good Functional Prior for Bayesian Deep Learning,Poster,NeurIPS,2022,"The Bayesian treatment of neural networks dictates that a prior distribution is specified over their weight and bias parameters. This poses a challenge because modern neural networks are characterized by a large number of parameters, and the choice of these priors has an uncontrolled effect on the induced functional prior, which is the distribution of the functions obtained by sampling the parameters from their prior distribution. We argue that this is a hugely limiting aspect of Bayesian deep learning, and this work tackles this limitation in a practical and effective way. Our proposal is to reason in terms of functional priors, which are easier to elicit, and to “tune” the priors of neural network parameters in a way that they reflect such functional priors. Gaussian processes offer a rigorous framework to define prior distributions over functions, and we propose a novel and robust framework to match their prior with the functional prior of neural networks based on the minimization of their Wasserstein distance. We provide vast experimental evidence that coupling these priors with scalable Markov chain Monte Carlo sampling offers systematically large performance improvements over alternative choices of priors and state-of-the-art approximate Bayesian deep learning approaches. We consider this work a considerable step in the direction of making the long-standing challenge of carrying out a fully Bayesian treatment of neural networks, including convolutional neural networks, a concrete possibility.",https://www.jmlr.org/papers/v23/20-1340.html,,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/4f87658ef0de194413056248a00ce009.png?t=1666488688.995036,https://neurips.cc/virtual/2022/poster/56120,Not relevant,Other aspects,"['Bayesian deep learning', 'Functional prior', 'Neural networks', 'Wasserstein distance', 'Markov chain Monte Carlo sampling', 'Convolutional neural networks']",['None'],,,,,,
2092,https://neurips.cc/virtual/2022/poster/56091,[Re] Privacy-preserving collaborative learning with automatic transformation search,Poster,NeurIPS,2022,"Scope of Reproduciblity Gao et al. propose to leverage policies consisting of a series of data augmentations for preventing the possibility of reconstruction attacks on the training data of gradients. The goal of this study is to: (1) Verify the findings of the authors about the performance of the found policies and the correlation between the reconstruction metric and provided protection. (2) Explore if the defence generalizes to an attacker that has knowledge about the policy used.
Methodology For the experiments conducted in this research, parts of the code from Gao et al, were refactored to allow for more clear and robust experimentation. Approximately a week of computation time is needed for our experiments on a 1080 Ti GPU.
Results It was possible to verify the results from the original paper within a reasonable margin of error. However, the reproduced results show that the claimed protection does not generalize to an attacker that has knowledge over the augmentations used. Additionally, the results show that the optimal augmentations are often predictable since the policies found by the proposed search algorithm mostly consist of the augmentations that perform best individually.
What was easy The design of the search algorithm allowed for easy iterations of experiments since obtaining the metrics of a single policy can be done in under a minute on an average GPU. It was helpfull that the authors provided the code of their experiments.
What was difficult To obtain the reconstruction score and accuracy of a policy, the architecture needs to be trained for about 10 GPU-hours. This makes it difficult to verify how well the search metrics correlate with these scores. It also prevented us to test the random policy baseline, as this requires the training to be repeated at least 10 times which requires significant computational power.
Communication with original authors An e-mail was sent to the original authors regarding the differences in results. Unfortunately no response has been received so far.",https://rescience.github.io/bibliography/Warmerdam_2022.html,,https://neurips.cc/virtual/2022/poster/56091,https://neurips.cc/virtual/2022/poster/56091,Model Extraction,Defence,"['privacy-preserving', 'collaborative learning', 'automatic transformation search', 'reconstruction attacks', 'data augmentations', 'protection', 'generalize', 'attacker', 'knowledge', 'optimizing', 'policies', 'search algorithm']",[],Other aspects,Data Extraction,Defence,,Arguable,
2099,https://neurips.cc/virtual/2022/poster/55440,Learning Substructure Invariance for Out-of-Distribution Molecular Representations,Poster,NeurIPS,2022,"Molecule representation learning (MRL) has been extensively studied and current methods have shown promising power for various tasks, e.g., molecular property prediction and target  identification. However, a common hypothesis of existing methods is that either the model development or experimental evaluation is mostly based on i.i.d. data across training and testing. Such a hypothesis can be violated in real-world applications where testing molecules could come from new environments, bringing about serious performance degradation or unexpected prediction. We propose a new representation learning framework entitled MoleOOD to enhance the robustness of MRL models against such distribution shifts, motivated by an observation that the (bio)chemical properties of molecules are usually invariantly associated with certain privileged molecular substructures across different environments (e.g., scaffolds, sizes, etc.). Specifically, We introduce an environment inference model to identify the latent factors that impact data generation from different distributions in a fully data-driven manner. We also propose a new learning objective to guide the molecule encoder to leverage environment-invariant substructures that more stably relate with the labels across environments. Extensive experiments on ten real-world datasets demonstrate that our model has a stronger generalization ability than existing methods under various out-of-distribution (OOD) settings, despite the absence of manual specifications of environments. Particularly, our method achieves up to 5.9\% and 3.9\% improvement over the strongest baselines on OGB and DrugOOD benchmarks in terms of ROC-AUC, respectively. Our source code is publicly available at \url{https://github.com/yangnianzu0515/MoleOOD}.",,https://openreview.net/forum?id=2nWUNTnFijm,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/7f6ffaa6bb0b408017b62254211691b5.png?t=1666803064.7131555,https://neurips.cc/media/neurips-2022/Slides/55440.pdf,Robustness,Defence,"['Molecular representation learning', 'Robustness', 'Out-of-distribution', 'Substructure Invariance', 'Environment-invariant substructures']","['Molecular property prediction', 'Target identification']",,,,,,
2101,https://neurips.cc/virtual/2022/poster/55423,Mix and Reason: Reasoning over Semantic Topology with Data Mixing for Domain Generalization,Poster,NeurIPS,2022,"Domain generalization (DG) enables generalizing a learning machine from multiple seen source domains to an unseen target one. The general objective of DG methods is to learn semantic representations that are independent of domain labels, which is theoretically sound but empirically challenged due to the complex mixture of common and domain-specific factors. Although disentangling the representations into two disjoint parts has been gaining momentum in DG, the strong presumption over the data limits its efficacy in many real-world scenarios. In this paper, we propose Mix and Reason (MiRe), a new DG framework that learns semantic representations via enforcing the structural invariance of semantic topology. MiRe consists of two key components, namely,  Category-aware Data Mixing (CDM) and Adaptive Semantic Topology Refinement (ASTR). CDM mixes two images from different domains in virtue of activation maps generated by two complementary classification losses, making the classifier focus on the representations of semantic objects. ASTR introduces relation graphs to represent semantic topology, which is progressively refined via the interactions between local feature aggregation and global cross-domain relational reasoning. Experiments on multiple DG benchmarks validate the effectiveness and robustness of the proposed MiRe. ",,https://openreview.net/forum?id=V0GwAmDclY,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55423.png?t=1669366699.947475,https://neurips.cc/virtual/2022/poster/55423,Other attack,Defence,"['domain generalization', 'semantic representations', 'semantic topology', 'Category-aware Data Mixing', 'Adaptive Semantic Topology Refinement']",['DG benchmarks'],,,,,,
2103,https://neurips.cc/virtual/2022/poster/55442,Unsupervised Causal Generative Understanding of Images,Poster,NeurIPS,2022,"We present a novel framework for unsupervised object-centric 3D scene understanding that generalizes robustly to out-of-distribution images. To achieve this, we design a causal generative model reflecting the physical process by which an image is produced, when a camera captures a scene containing multiple objects. This model is trained to reconstruct multi-view images via a latent representation describing the shapes, colours and positions of the 3D objects they show. It explicitly represents object instances as separate neural radiance fields, placed into a 3D scene. We then propose an inference algorithm that can infer this latent representation given a single out-of-distribution image as input -- even when it shows an unseen combination of components, unseen spatial compositions or a radically new viewpoint. We conduct extensive experiments applying our approach to test datasets that have zero probability under the training distribution. These show that it accurately reconstructs a scene's geometry, segments objects and infers their positions, despite not receiving any supervision. Our approach significantly out-performs baselines that do not capture the true causal image generation process.",,https://openreview.net/forum?id=VvOcK2DGM7G,https://neurips.cc/virtual/2022/poster/55442,https://neurips.cc/virtual/2022/poster/55442,Not relevant,Other aspects,"['Unsupervised', 'Causal', 'Generative', 'Images', 'Object-centric', '3D', 'Scene understanding', 'Robustly', 'Out-of-distribution', 'Physical process', 'Camera', 'Multi-view', 'Latent representation', 'Shapes', 'Colours', 'Positions', '3D objects', 'Neural radiance fields', 'Inference algorithm', 'Unseen combination', 'Unseen spatial compositions', 'Radically new viewpoint']",['3D scene understanding'],,,,,,
2111,https://neurips.cc/virtual/2022/poster/55400,Active Labeling: Streaming Stochastic Gradients,Poster,NeurIPS,2022,"The workhorse of machine learning is stochastic gradient descent.To access stochastic gradients, it is common to consider iteratively input/output pairs of a training dataset.Interestingly, it appears that one does not need full supervision to access stochastic gradients, which is the main motivation of this paper.After formalizing the ""active labeling"" problem, which focuses on active learning with partial supervision, we provide a streaming technique that provably minimizes the ratio of generalization error over the number of samples.We illustrate our technique in depth for robust regression.",,https://openreview.net/forum?id=Iqm6AiHPs_z,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55400.png?t=1669088360.977524,https://neurips.cc/virtual/2022/poster/55400,Not relevant,Defence,"['stochastic gradient descent', 'active labeling', 'active learning', 'partial supervision', 'streaming technique', 'generalization error']",,,,,,,
2112,https://neurips.cc/virtual/2022/poster/55401,Practical Adversarial Attacks on Spatiotemporal Traffic Forecasting Models,Poster,NeurIPS,2022,"Machine learning based traffic forecasting models leverage sophisticated spatiotemporal auto-correlations to provide accurate predictions of city-wide traffic states. However, existing methods assume a reliable and unbiased forecasting environment, which is not always available in the wild. In this work, we investigate the vulnerability of spatiotemporal traffic forecasting models and propose a practical adversarial spatiotemporal attack framework. Specifically, instead of simultaneously attacking all geo-distributed data sources, an iterative gradient guided node saliency method is proposed to identify the time-dependent set of victim nodes. Furthermore, we devise a spatiotemporal gradient descent based scheme to generate real-valued adversarial traffic states under a perturbation constraint.Meanwhile, we theoretically demonstrate the worst performance bound of adversarial traffic forecasting attacks. Extensive experiments on two real-world datasets show that the proposed two-step framework achieves up to 67.8% performance degradation on various advanced spatiotemporal forecasting models. Remarkably, we also show that adversarial training with our proposed attacks can significantly improve the robustness of spatiotemporal traffic forecasting models.",,https://openreview.net/forum?id=lTKXh991Ayv,https://neurips.cc/virtual/2022/poster/55401,https://neurips.cc/virtual/2022/poster/55401,Evasion,Attack,"['Adversarial attack', 'Spatiotemporal traffic forecasting', 'Vulnerability', 'Practical adversarial spatiotemporal attack framework', 'Gradient guided node saliency method', 'Spatiotemporal gradient descent', 'Performance degradation', 'Adversarial training', 'Robustness', 'Real-world datasets']",,,,,,,
2117,https://neurips.cc/virtual/2022/poster/55395,Finding Differences Between Transformers and ConvNets Using Counterfactual Simulation Testing,Poster,NeurIPS,2022,"Modern deep neural networks tend to be evaluated on static test sets. One shortcoming of this is the fact that these deep neural networks cannot be easily evaluated for robustness issues with respect to specific scene variations. For example, it is hard to study the robustness of these networks to variations of object scale, object pose, scene lighting and 3D occlusions. The main reason is that collecting real datasets with fine-grained naturalistic variations of sufficient scale can be extremely time-consuming and expensive. In this work, we present Counterfactual Simulation Testing, a counterfactual framework that allows us to study the robustness of neural networks with respect to some of these naturalistic variations by building realistic synthetic scenes that allow us to ask counterfactual questions to the models, ultimately providing answers to questions such as ""Would your classification still be correct if the object were viewed from the top?"" or ""Would your classification still be correct if the object were partially occluded by another object?"". Our method allows for a fair comparison of the robustness of recently released, state-of-the-art Convolutional Neural Networks and Vision Transformers, with respect to these naturalistic variations. We find evidence that ConvNext is more robust to pose and scale variations than Swin, that ConvNext generalizes better to our simulated domain and that Swin handles partial occlusion better than ConvNext. We also find that robustness for all networks improves with network scale and with data scale and variety. We release the Naturalistic Variation Object Dataset (NVD), a large simulated dataset of 272k images of everyday objects with naturalistic variations such as object pose, scale, viewpoint, lighting and occlusions. Project page: https://counterfactualsimulation.github.io.git",,https://openreview.net/forum?id=Aisi2oEq1sc,https://neurips.cc/virtual/2022/poster/55395,https://neurips.cc/virtual/2022/poster/55395,Robustness,Defence,"['Counterfactual Simulation Testing', 'Robustness', 'Neural networks', 'Convolutional Neural Networks', 'Vision Transformers', 'Naturalistic variations', 'Object pose', 'Scale', 'Viewpoint', 'Lighting', 'Occlusions', 'Generalization']",['Computer Vision'],,,,,,
2133,https://neurips.cc/virtual/2022/poster/55316,A2: Efficient Automated Attacker for Boosting Adversarial Training,Poster,NeurIPS,2022,"Based on the significant improvement of model robustness by AT (Adversarial Training), various variants have been proposed to further boost the performance. Well-recognized methods have focused on different components of AT (e.g., designing loss functions and leveraging additional unlabeled data). It is generally accepted that stronger perturbations yield more robust models.However, how to generate stronger perturbations efficiently is still missed. In this paper, we propose an efficient automated attacker called A2 to boost AT by generating the optimal perturbations on-the-fly during training. A2 is a parameterized automated attacker to search in the attacker space for the best attacker against the defense model and examples. Extensive experiments across different datasets demonstrate that A2 generates stronger perturbations with low extra cost and reliably improves the robustness of various AT methods against different attacks.",,https://openreview.net/forum?id=SsA-0BZa7B_,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55316.png?t=1668511538.7782776,https://neurips.cc/media/neurips-2022/Slides/55316.pdf,Evasion,Attack,"['Adversarial Training', 'Adversarial robustness', 'Automated Attacker', 'Efficient perturbations', 'Boosting performance']",['none'],Both,Evasion,Defence,,,Wrong
2138,https://neurips.cc/virtual/2022/poster/55299,In the Eye of the Beholder: Robust Prediction with Causal User Modeling,Poster,NeurIPS,2022,"Accurately predicting the relevance of items to users is crucial to the success of many social platforms. Conventional approaches train models on logged historical data; but recommendation systems, media services, and online marketplaces all exhibit a constant influx of new content---making relevancy a moving target, to which standard predictive models are not robust. In this paper, we propose a learning framework for relevance prediction that is robust to changes in the data distribution. Our key observation is that robustness can be obtained by accounting for \emph{how users causally perceive the environment}. We model users as boundedly-rational decision makers whose causal beliefs are encoded by a causal graph, and show how minimal information regarding the graph can be used to contend with distributional changes. Experiments in multiple settings demonstrate the effectiveness of our approach.",,https://openreview.net/forum?id=ikXoMuy_H4,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/7ce3284b743aefde80ffd9aec500e085.png?t=1666118657.171158,https://neurips.cc/virtual/2022/poster/55299,Robustness,Defence,"['Causal User Modeling', 'Robust Prediction', 'Causal Graph', 'Boundedly-Rational Decision Makers', 'Relevance Prediction', 'Social Platforms']",,,,,,,
2141,https://neurips.cc/virtual/2022/poster/55287,Pay attention to your loss : understanding misconceptions about Lipschitz neural networks,Poster,NeurIPS,2022,"Lipschitz constrained networks have gathered considerable attention in the deep learning community, with usages ranging from Wasserstein distance estimation to the training of certifiably robust classifiers. However they remain commonly considered as less accurate, and their properties in learning are still not fully understood. In this paper we clarify the matter: when it comes to classification 1-Lipschitz neural networks enjoy several advantages over their unconstrained counterpart. First, we show that these networks are as accurate as classical ones, and can fit arbitrarily difficult boundaries. Then, relying on a robustness metric that reflects operational needs we characterize the most robust classifier: the WGAN discriminator. Next, we show that 1-Lipschitz neural networks generalize well under milder assumptions. Finally, we show that hyper-parameters of the loss are crucial for controlling the accuracy-robustness trade-off. We conclude that they exhibit appealing properties to pave the way toward provably accurate, and provably robust neural networks.    ",,https://openreview.net/forum?id=BRIL0EFvTgc,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55287.png?t=1669481724.9811902,https://neurips.cc/virtual/2022/poster/55287,Robustness,Defence,"['Lipschitz constrained networks', 'Wasserstein distance estimation', 'Certifiably robust classifiers', '1-Lipschitz neural networks', 'Classification', 'WGAN discriminator', 'Generalization', 'Accuracy-robustness trade-off']",,,,,,,
2148,https://neurips.cc/virtual/2022/poster/55269,On the Robustness of Deep Clustering Models: Adversarial Attacks and Defenses,Poster,NeurIPS,2022,"Clustering models constitute a class of unsupervised machine learning methods which are used in a number of application pipelines, and play a vital role in modern data science. With recent advancements in deep learning-- deep clustering models have emerged as the current state-of-the-art over traditional clustering approaches, especially for high-dimensional image datasets. While traditional clustering approaches have been analyzed from a robustness perspective, no prior work has investigated adversarial attacks and robustness for deep clustering models in a principled manner. To bridge this gap, we propose a blackbox attack using Generative Adversarial Networks (GANs) where the adversary does not know which deep clustering model is being used, but can query it for outputs. We analyze our attack against multiple state-of-the-art deep clustering models and real-world datasets, and find that it is highly successful. We then employ some natural unsupervised defense approaches, but find that these are unable to mitigate our attack. Finally, we attack Face++, a production-level face clustering API service, and find that we can significantly reduce its performance as well. Through this work, we thus aim to motivate the need for truly robust deep clustering models.",,https://openreview.net/forum?id=p62j5eqi_g2,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/456ac9b0d15a8b7f1e71073221059886.png?t=1667374969.63495,https://neurips.cc/virtual/2022/poster/55269,Evasion,Attack,"['Adversarial attacks', 'Deep clustering models', 'Robustness', 'Generative Adversarial Networks (GANs)', 'Unsupervised defense approaches', 'Face++']","['Data science', 'Face clustering API service']",Both,Evasion,,,,Arguable
2160,https://neurips.cc/virtual/2022/poster/55231,Dataset Distillation via Factorization,Poster,NeurIPS,2022,"In this paper, we study dataset distillation (DD), from a novel perspective and introduce a \emph{dataset factorization} approach, termed \emph{HaBa}, which is a plug-and-play strategy portable to any existing DD baseline. Unlike conventional DD approaches that aim to produce distilled and representative samples, \emph{HaBa} explores decomposing a dataset into two components: data \emph{Ha}llucination networks and \emph{Ba}ses, where the latter is fed into the former to reconstruct image samples. The flexible combinations between bases and hallucination networks, therefore, equip the distilled data with exponential informativeness gain, which largely increase the representation capability of distilled datasets. To furthermore increase the data efficiency of compression results, we further introduce a pair of adversarial contrastive \xw{constraints} on the resultant hallucination networks and bases, which increase the diversity of generated images and inject more discriminant information into the factorization. Extensive comparisons and experiments demonstrate that our method can yield significant improvement on downstream classification tasks compared with previous state of the arts, while reducing the total number of compressed parameters by up to 65\%. Moreover, distilled datasets by our approach also achieve \textasciitilde10\% higher accuracy than baseline methods in cross-architecture generalization. Our code is available \href{https://github.com/Huage001/DatasetFactorization}{here}.",,https://openreview.net/forum?id=luGXvawYWJ,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55231.png?t=1668961755.9041288,https://neurips.cc/virtual/2022/poster/55231,Other attack,Defence,"['dataset distillation', 'dataset factorization', 'HaBa', 'Hallucination networks', 'Bases', 'adversarial contrastive constraints', 'compression', 'classification tasks', 'cross-architecture generalization']",,Not relevant,Not relevant,,,Wrong,Wrong
2161,https://neurips.cc/virtual/2022/poster/55199,Effective Backdoor Defense by Exploiting Sensitivity of Poisoned Samples,Poster,NeurIPS,2022,"Poisoning-based backdoor attacks are serious threat for training deep models on data from untrustworthy sources. Given a backdoored model, we observe that the feature representations of poisoned samples with trigger are more sensitive to transformations than those of clean samples. It inspires us to design a simple sensitivity metric, called feature consistency towards transformations (FCT), to distinguish poisoned samples from clean samples in the untrustworthy training set. Moreover, we propose two effective backdoor defense methods. Built upon a sample-distinguishment module utilizing the FCT metric, the first method trains a secure model from scratch using a two-stage secure training module. And the second method removes backdoor from a backdoored model with a backdoor removal module which alternatively unlearns the distinguished poisoned samples and relearns the distinguished clean samples. Extensive results on three benchmark datasets demonstrate the superior defense performance against eight types of backdoor attacks, to state-of-the-art backdoor defenses. Codes are available at: https://github.com/SCLBD/Effective",,https://openreview.net/forum?id=AsH-Tx2U0Ug,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/299fb2142d7de959380f91c01c3a293c.png?t=1666423108.199134,https://neurips.cc/virtual/2022/poster/55199,Poisoning,Defence,"['backdoor attack', 'poisoning-based attack', 'deep models', 'feature consistency towards transformations (FCT)', 'sample-distinguishment module', 'secure model', 'backdoor removal module']",['None'],Defence,Poisoning,,,,
2162,https://neurips.cc/virtual/2022/poster/55229,A Unified Analysis of Mixed Sample Data Augmentation: A Loss Function Perspective,Poster,NeurIPS,2022,"We propose the first unified theoretical analysis of mixed sample data augmentation (MSDA), such as Mixup and CutMix. Our theoretical results show that regardless of the choice of the mixing strategy, MSDA behaves as a pixel-level regularization of the underlying training loss and a regularization of the first layer parameters. Similarly, our theoretical results support that the MSDA training strategy can improve adversarial robustness and generalization compared to the vanilla training strategy. Using the theoretical results, we provide a high-level understanding of how different design choices of MSDA work differently. For example, we show that the most popular MSDA methods, Mixup and CutMix, behave differently, e.g., CutMix regularizes the input gradients by pixel distances, while Mixup regularizes the input gradients regardless of pixel distances. Our theoretical results also show that the optimal MSDA strategy depends on tasks, datasets, or model parameters. From these observations, we propose generalized MSDAs, a Hybrid version of Mixup and CutMix  (HMix) and Gaussian Mixup (GMix), simple extensions of Mixup and CutMix. Our implementation can leverage the advantages of Mixup and CutMix, while our implementation is very efficient, and the computation cost is almost neglectable as Mixup and CutMix. Our empirical study shows that our HMix and GMix outperform the previous state-of-the-art MSDA methods in CIFAR-100 and ImageNet classification tasks.",,https://openreview.net/forum?id=SLdfxFdIFeN,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/459a4ddcb586f24efd9395aa7662bc7c.png?t=1667049606.3860223,https://neurips.cc/media/neurips-2022/Slides/55229.pdf,Evasion,Defence,"['Adversarial robustness', 'Mixed sample data augmentation', 'Mixup', 'CutMix', 'Regularization', 'Pixel-level regularization', 'Generalization', 'Theoretical results', 'Loss function perspective', 'First layer parameters', 'Design choices', 'Hybrid version', 'Gaussian Mixup', 'Empirical study', 'CIFAR-100', 'ImageNet classification']",,Not relevant,Not relevant,Other aspects,Robustness,Arguable,Correct
2166,https://neurips.cc/virtual/2022/poster/55209,S$^3$-NeRF: Neural Reflectance Field from Shading and Shadow under a Single Viewpoint,Poster,NeurIPS,2022,"In this paper, we address the ""dual problem"" of multi-view scene reconstruction in which we utilize single-view images captured under different point lights to learn a neural scene representation. Different from existing single-view methods which can only recover a 2.5D scene representation (i.e., a normal / depth map for the visible surface), our method learns a neural reflectance field to represent the 3D geometry and BRDFs of a scene. Instead of relying on multi-view photo-consistency, our method exploits two information-rich monocular cues, namely shading and shadow, to infer scene geometry. Experiments on multiple challenging datasets show that our method is capable of recovering 3D geometry, including both visible and invisible parts, of a scene from single-view images. Thanks to the neural reflectance field representation, our method is robust to depth discontinuities. It supports applications like novel-view synthesis and relighting. Our code and model can be found at https://ywq.github.io/s3nerf.",,https://openreview.net/forum?id=tvwkeAIcRP8,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55209.png?t=1669299803.4833179,https://neurips.cc/virtual/2022/poster/55209,Not relevant,Other aspects,"['Neural Reflectance Field', 'Multi-view scene reconstruction', 'Single-view images', 'Point lights', '3D geometry', 'BRDFs', 'Shading', 'Shadow', 'Inferring scene geometry', 'Depth discontinuities', 'Novel-view synthesis', 'Relighting']",['3D geometry'],,,,,,
2179,https://neurips.cc/virtual/2022/poster/55171,LASSIE: Learning Articulated Shapes from Sparse Image Ensemble via 3D Part Discovery,Poster,NeurIPS,2022,"Creating high-quality articulated 3D models of animals is challenging either via manual creation or using 3D scanning tools. Therefore, techniques to reconstruct articulated 3D objects from 2D images are crucial and highly useful. In this work, we propose a practical problem setting to estimate 3D pose and shape of animals given only a few (10-30) in-the-wild images of a particular animal species (say, horse). Contrary to existing works that rely on pre-defined template shapes, we do not assume any form of 2D or 3D ground-truth annotations, nor do we leverage any multi-view or temporal information. Moreover, each input image ensemble can contain animal instances with varying poses, backgrounds, illuminations, and textures. Our key insight is that 3D parts have much simpler shape compared to the overall animal and that they are robust w.r.t. animal pose articulations. Following these insights, we propose LASSIE, a novel optimization framework which discovers 3D parts in a self-supervised manner with minimal user intervention. A key driving force behind LASSIE is the enforcing of 2D-3D part consistency using self-supervisory deep features. Experiments on Pascal-Part and self-collected in-the-wild animal datasets demonstrate considerably better 3D reconstructions as well as both 2D and 3D part discovery compared to prior arts. Project page: https://chhankyao.github.io/lassie/",,https://openreview.net/forum?id=0TDki1mlcwz,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/894b77f805bd94d292574c38c5d628d5.png?t=1667671852.9158401,https://neurips.cc/media/neurips-2022/Slides/55171.pdf,Not relevant,Defence,"['3D reconstruction', 'Articulated shapes', 'Sparse image ensemble', '3D part discovery', 'Self-supervised', 'Deep features', '2D-3D consistency']",['animals'],,,,,,
2182,https://neurips.cc/virtual/2022/poster/55084,Adversarial Training with Complementary Labels: On the Benefit of Gradually Informative Attacks,Poster,NeurIPS,2022,"Adversarial training (AT) with imperfect supervision is significant but receives limited attention. To push AT towards more practical scenarios, we explore a brand new yet challenging setting, i.e., AT with complementary labels (CLs), which specify a class that a data sample does not belong to. However, the direct combination of AT with existing methods for CLs results in consistent failure, but not on a simple baseline of two-stage training. In this paper, we further explore the phenomenon and identify the underlying challenges of AT with CLs as intractable adversarial optimization and low-quality adversarial examples. To address the above problems, we propose a new learning strategy using gradually informative attacks, which consists of two critical components: 1) Warm-up Attack (Warm-up) gently raises the adversarial perturbation budgets to ease the adversarial optimization with CLs; 2) Pseudo-Label Attack (PLA) incorporates the progressively informative model predictions into a corrected complementary loss. Extensive experiments are conducted to demonstrate the effectiveness of our method on a range of benchmarked datasets. The code is publicly available at: https://github.com/RoyalSkye/ATCL.",,https://openreview.net/forum?id=s7SukMH7ie9,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/d77f00766fd3be3f2189c843a6af3fb2.png?t=1667191829.7703528,https://neurips.cc/virtual/2022/poster/55084,Evasion,Defence,"['Adversarial training', 'Complementary labels', 'Gradually informative attacks', 'Adversarial optimization', 'Low-quality adversarial examples', 'Warm-up attack', 'Pseudo-label attack']",,Defence,Evasion,Both,,,
2183,https://neurips.cc/virtual/2022/poster/55141,Planning for Sample Efficient Imitation Learning,Poster,NeurIPS,2022,"Imitation learning is a class of promising policy learning algorithms that is free from many practical issues with reinforcement learning, such as the reward design issue and the exploration hardness. However, the current imitation algorithm struggles to achieve both high performance and high in-environment sample efficiency simultaneously. Behavioral Cloning~(BC) does not need in-environment interactions, but it suffers from the covariate shift problem which harms its performance. Adversarial Imitation Learning~(AIL) turns imitation learning into a distribution matching problem. It can achieve better performance on some tasks but it requires a large number of in-environment interactions. Inspired by the recent success of EfficientZero in RL, we propose EfficientImitate~(EI), a planning-based imitation learning method that can achieve high in-environment sample efficiency and performance simultaneously. Our algorithmic contribution in this paper is two-fold. First, we extend AIL into the MCTS-based RL. Second, we show the seemingly incompatible two classes of imitation algorithms (BC and AIL) can be naturally unified under our framework, enjoying the benefits of both. We benchmark our method not only on the state-based DeepMind Control Suite, but also on the image version which many previous works find highly challenging. Experimental results show that EI achieves state-of-the-art results in performance and sample efficiency. EI shows over 4x gain in performance in the limited sample setting on state-based and image-based tasks and can solve challenging problems like Humanoid, where previous methods fail with small amount of interactions. ",,https://openreview.net/forum?id=iKKfdIm81Jt,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55141.png?t=1668414041.0466259,https://neurips.cc/virtual/2022/poster/55141,Not relevant,Defence,"['Imitation learning', 'Policy learning', 'Reinforcement learning', 'Behavioral Cloning', 'Adversarial Imitation Learning', 'EfficientZero', 'MCTS-based RL', 'DeepMind Control Suite']",['None'],Not relevant,Not relevant,,,,
2184,https://neurips.cc/virtual/2022/poster/55150,CATER: Intellectual Property Protection on Text Generation APIs via Conditional Watermarks,Poster,NeurIPS,2022,"Previous works have validated that text generation APIs can be stolen through imitation attacks, causing IP violations. In order to protect the IP of text generation APIs, recent work has introduced a watermarking algorithm and utilized the null-hypothesis test as a post-hoc ownership verification on the imitation models. However, we find that it is possible to detect those watermarks via sufficient statistics of the frequencies of candidate watermarking words. To address this drawback, in this paper, we propose a novel Conditional wATERmarking framework (CATER) for protecting the IP of text generation APIs. An optimization method is proposed to decide the watermarking rules that can minimize the distortion of overall word distributions while maximizing the change of conditional word selections. Theoretically, we prove that it is infeasible for even the savviest attacker (they know how CATER works) to reveal the used watermarks from a large pool of potential word pairs based on statistical inspection. Empirically, we observe that high-order conditions lead to an exponential growth of suspicious (unused) watermarks, making our crafted watermarks more stealthy. In addition, CATER can effectively identify IP infringement under architectural mismatch and cross-domain imitation attacks, with negligible impairments on the generation quality of victim APIs. We envision our work as a milestone for stealthily protecting the IP of text generation APIs.",,https://openreview.net/forum?id=L7P3IvsoUXY,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/5d79099fcdf499f12b79770834c0164a.png?t=1666791120.0592813,https://neurips.cc/virtual/2022/poster/55150,Model Extraction,Defence,"['text generation APIs', 'IP protection', 'Watermarking', 'Conditional Watermarks', 'null-hypothesis test', 'ownership verification', 'imitation attacks', 'IP violations', 'word distributions', 'watermarking rules', 'architectural mismatch', 'cross-domain imitation attacks']",,,,,,,
2197,https://neurips.cc/virtual/2022/poster/55120,Increasing Confidence in Adversarial Robustness Evaluations,Poster,NeurIPS,2022,"Hundreds of defenses have been proposed to make deep neural networks robust against minimal (adversarial) input perturbations. However, only a handful of these defenses held up their claims because correctly evaluating robustness is extremely challenging: Weak attacks often fail to find adversarial examples even if they unknowingly exist, thereby making a vulnerable network look robust. In this paper, we propose a test to identify weak attacks and, thus, weak defense evaluations. Our test slightly modifies a neural network to guarantee the existence of an adversarial example for every sample. Consequentially, any correct attack must succeed in breaking this modified network. For eleven out of thirteen previously-published defenses, the original evaluation of the defense fails our test, while stronger attacks that break these defenses pass it. We hope that attack unit tests - such as ours - will be a major component in future robustness evaluations and increase confidence in an empirical field that is currently riddled with skepticism.",,https://openreview.net/forum?id=NkK4i91VWp,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55120.png?t=1669469507.6144154,https://neurips.cc/virtual/2022/poster/55120,Evasion,Defence,"['Adversarial robustness evaluations', 'Deep neural networks', 'Adversarial examples', 'Defense evaluation', 'Attack unit tests']",['Deep Learning'],Other aspects,Evasion,,,,Arguable
2202,https://neurips.cc/virtual/2022/poster/55110,Hiding Images in Deep Probabilistic Models,Poster,NeurIPS,2022,"Data hiding with deep neural networks (DNNs) has experienced impressive successes in recent years. A prevailing scheme is to train an autoencoder, consisting of an encoding network to embed (or transform) secret messages in (or into) a carrier, and a decoding network to extract the hidden messages. This scheme may suffer from several limitations regarding practicability, security, and embedding capacity. In this work, we describe a different computational framework to hide images in deep probabilistic models. Specifically, we use a DNN to model the probability density of cover images, and hide a secret image in one particular location of the learned distribution. As an instantiation, we adopt a SinGAN, a pyramid of generative adversarial networks (GANs), to learn the patch distribution of one cover image. We hide the secret image by fitting a deterministic mapping from a fixed set of noise maps (generated by an embedding key) to the secret image during patch distribution learning. The stego SinGAN, behaving as the original SinGAN, is publicly communicated; only the receiver with the embedding key is able to extract the secret image. We demonstrate the feasibility of our SinGAN approach in terms of extraction accuracy and model security. Moreover, we show the flexibility of the proposed method in terms of hiding multiple images for different receivers and obfuscating the secret image. ",,https://openreview.net/forum?id=3I8VTXMhuPx,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55110.png?t=1669535990.170695,https://neurips.cc/virtual/2022/poster/55110,Data Extraction,Defence,"['deep neural networks', 'autoencoder', 'secret messages', 'probabilistic models', 'SinGAN', 'generative adversarial networks', 'embedding key']",['image'],Other aspects,Other attack,,,Wrong,Wrong
2215,https://neurips.cc/virtual/2022/poster/55066,Masked Generative Adversarial Networks are Data-Efficient Generation Learners,Poster,NeurIPS,2022,"This paper shows that masked generative adversarial network (MaskedGAN) is robust image generation learners with limited training data. The idea of MaskedGAN is simple: it randomly masks out certain image information for effective GAN training with limited data. We develop two masking strategies that work along orthogonal dimensions of training images, including a shifted spatial masking that masks the images in spatial dimensions with random shifts, and a balanced spectral masking that masks certain image spectral bands with self-adaptive probabilities. The two masking strategies complement each other which together encourage more challenging holistic learning from limited training data, ultimately suppressing trivial solutions and failures in GAN training. Albeit simple, extensive experiments show that MaskedGAN achieves superior performance consistently across different network architectures (e.g., CNNs including BigGAN and StyleGAN-v2 and Transformers including TransGAN and GANformer) and datasets (e.g., CIFAR-10, CIFAR-100, ImageNet, 100-shot, AFHQ, FFHQ and Cityscapes).",,https://openreview.net/forum?id=js2ssA77fX,https://neurips.cc/virtual/2022/poster/55066,https://neurips.cc/virtual/2022/poster/55066,Not relevant,Defence,"['masked generative adversarial network (MaskedGAN)', 'image generation', 'GAN training', 'limited data', 'spatial masking', 'spectral masking', 'holistic learning', 'network architectures', 'CNNs', 'BigGAN', 'StyleGAN-v2', 'Transformers', 'TransGAN', 'GANformer', 'datasets', 'CIFAR-10', 'CIFAR-100', 'ImageNet', '100-shot', 'AFHQ', 'FFHQ', 'Cityscapes']",['image generation'],,,,,,
2225,https://neurips.cc/virtual/2022/poster/55019,Mutual Information Divergence: A Unified Metric for Multimodal Generative Models,Poster,NeurIPS,2022,"Text-to-image generation and image captioning are recently emerged as a new experimental paradigm to assess machine intelligence. They predict continuous quantity accompanied by their sampling techniques in the generation, making evaluation complicated and intractable to get marginal distributions. Based on a recent trend that multimodal generative evaluations exploit a vison-and-language pre-trained model, we propose the negative Gaussian cross-mutual information using the CLIP features as a unified metric, coined by Mutual Information Divergence (MID). To validate, we extensively compare it with competing metrics using carefully-generated or human-annotated judgments in text-to-image generation and image captioning tasks. The proposed MID significantly outperforms the competitive methods by having consistency across benchmarks, sample parsimony, and robustness toward the exploited CLIP model. We look forward to seeing the underrepresented implications of the Gaussian cross-mutual information in multimodal representation learning and future works based on this novel proposition. ",,https://openreview.net/forum?id=wKd2XtSRsjl,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55019.png?t=1668399912.1588016,https://neurips.cc/virtual/2022/poster/55019,Not relevant,Defence,"['Mutual Information Divergence', 'Multimodal Generative Models', 'Text-to-image generation', 'Image captioning', 'Machine intelligence', 'Evaluation', 'Generation', 'Metrics', 'CLIP features', 'Consistency', 'Sample parsimony', 'Robustness']",,,,,,,
2232,https://neurips.cc/virtual/2022/poster/54416,Momentum Adversarial Distillation: Handling Large Distribution Shifts in Data-Free Knowledge Distillation,Poster,NeurIPS,2022,"Data-free Knowledge Distillation (DFKD) has attracted attention recently thanks to its appealing capability of transferring knowledge from a teacher network to a student network without using training data. The main idea is to use a generator to synthesize data for training the student. As the generator gets updated, the distribution of synthetic data will change. Such distribution shift could be large if the generator and the student are trained adversarially, causing the student to forget the knowledge it acquired at the previous steps. To alleviate this problem, we propose a simple yet effective method called Momentum Adversarial Distillation (MAD) which maintains an exponential moving average (EMA) copy of the generator and uses synthetic samples from both the generator and the EMA generator to train the student. Since the EMA generator can be considered as an ensemble of the generator's old versions and often undergoes a smaller change in updates compared to the generator, training on its synthetic samples can help the student recall the past knowledge and prevent the student from adapting too quickly to the new updates of the generator. Our experiments on six benchmark datasets including big datasets like ImageNet and Places365 demonstrate the superior performance of MAD over competing methods for handling the large distribution shift problem. Our method also compares favorably to existing DFKD methods and even achieves state-of-the-art results in some cases.",,https://openreview.net/forum?id=C7jm6YgJaT,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/aa108f56a10e75c1f20f27723ecac85f.png?t=1666499054.7639298,https://neurips.cc/virtual/2022/poster/54416,Evasion,Defence,"['Momentum Adversarial Distillation', 'Large Distribution Shifts', 'Data-Free Knowledge Distillation', 'Transferring knowledge', 'Synthetic data', 'Student network', 'Teacher network', 'Adversarial training', 'EMA copy', 'Generator']","['ImageNet', 'Places365']",,,,,,
2233,https://neurips.cc/virtual/2022/poster/54395,Follow-the-Perturbed-Leader for Adversarial Markov Decision Processes with Bandit Feedback,Poster,NeurIPS,2022,"We consider regret minimization for Adversarial Markov Decision Processes (AMDPs), where the loss functions are changing over time and adversarially chosen, and the learner only observes the losses for the visited state-action pairs (i.e., bandit feedback). While there has been a surge of studies on this problem using Online-Mirror-Descent (OMD) methods, very little is known about the Follow-the-Perturbed-Leader (FTPL) methods, which are usually computationally more efficient and also easier to implement since it only requires solving an offline planning problem. Motivated by this, we take a closer look at FTPL for learning AMDPs, starting from the standard episodic finite-horizon setting. We find some unique and intriguing difficulties in the analysis and propose a workaround to eventually show that FTPL is also able to achieve near-optimal regret bounds in this case. More importantly, we then find two significant applications: First, the analysis of FTPL turns out to be readily generalizable to delayed bandit feedback with order-optimal regret, while OMD methods exhibit extra difficulties (Jin et al., 2022). Second, using FTPL, we also develop the first no-regret algorithm for learning communicating AMDPs in the infinite-horizon setting with bandit feedback and stochastic transitions. Our algorithm is efficient assuming access to an offline planning oracle, while even for the easier full-information setting, the only existing algorithm (Chandrasekaran and Tewari, 2021) is computationally inefficient.",,https://openreview.net/forum?id=25XwID3wKsi,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/1fd09c5f59a8ff35d499c0ee25a1d47e.png?t=1666854880.242098,https://neurips.cc/virtual/2022/poster/54395,Not relevant,Defence,"['Adversarial Markov Decision Processes', 'Follow-the-Perturbed-Leader', 'Regret minimization', 'Online-Mirror-Descent', 'Bandit feedback', 'Delayed bandit feedback', 'Communicating AMDPs', 'No-regret algorithm', 'Offline planning oracle']",[],,,,,,
2254,https://neurips.cc/virtual/2022/poster/53247,Luckiness in Multiscale Online Learning,Poster,NeurIPS,2022,"Algorithms for full-information online learning are classically tuned to minimize their worst-case regret. Modern algorithms additionally provide tighter guarantees outside the adversarial regime, most notably in the form of constant pseudoregret bounds under statistical margin assumptions. We investigate the multiscale extension of the problem where the loss ranges of the experts are vastly different. Here, the regret with respect to each expert needs to scale with its range, instead of the maximum overall range. We develop new multiscale algorithms, tuning schemes and analysis techniques to show that worst-case robustness and adaptation to easy data can be combined at a negligible cost. We further develop an extension with optimism and apply it to solve multiscale two-player zero-sum games. We demonstrate experimentally the superior performance of our scale-adaptive algorithm and discuss the subtle relationship of our results to Freund's 2016 open problem.",,https://openreview.net/forum?id=0tpZgkAKVjB,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53247.png?t=1669384418.737173,https://neurips.cc/virtual/2022/poster/53247,Not relevant,Defence,"['multiscale online learning', 'full-information online learning', 'worst-case regret', 'statistical margin assumptions', 'multiscale extension', 'loss ranges of the experts', 'worst-case robustness', 'adaptation to easy data', 'optimism', 'two-player zero-sum games', 'scale-adaptive algorithm', ""Freund's 2016 open problem""]",,Other aspects,Robustness,,,Arguable,Arguable
2260,https://neurips.cc/virtual/2022/poster/54279,On the Tradeoff Between Robustness and Fairness,Poster,NeurIPS,2022,"Interestingly, recent experimental results [2, 26, 22] have identified a robust fairness phenomenon in adversarial training (AT), namely that a robust model well-trained by AT exhibits a remarkable disparity of standard accuracy and robust accuracy among different classes compared with natural training. However, the effect of different perturbation radii in AT on robust fairness has not been studied, and one natural question is raised: does a tradeoff exist between average robustness and robust fairness? Our extensive experimental results provide an affirmative answer to this question: with an increasing perturbation radius, stronger AT will lead to a larger class-wise disparity of robust accuracy. Theoretically, we analyze the class-wise performance of adversarially trained linear models with mixture Gaussian distribution. Our theoretical results support our observations. Moreover, our theory shows that  adversarial training easily leads to more serious robust fairness issue than natural training. Motivated by theoretical results, we propose a fairly adversarial training (FAT) method to mitigate the tradeoff between average robustness and robust fairness. Experimental results validate the effectiveness of our proposed method.",,https://openreview.net/forum?id=LqGA2JMLwBw,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/b05bf587ec7ce83518b72eb0d011a353.png?t=1666404771.1041577,https://neurips.cc/virtual/2022/poster/54279,Evasion,Defence,"['Tradeoff', 'Robustness', 'Fairness', 'Adversarial training', 'Perturbation radii', 'Class-wise disparity', 'Linear models', 'Mixture Gaussian distribution', 'Mitigate', 'Effectiveness']",,Other aspects,Evasion,Other aspects,,,Arguable
2263,https://neurips.cc/virtual/2022/poster/54281,Amplifying Membership Exposure via Data Poisoning,Poster,NeurIPS,2022,"As in-the-wild data are increasingly involved in the training stage, machine learning applications become more susceptible to data poisoning attacks. Such attacks typically lead to test-time accuracy degradation or controlled misprediction. In this paper, we investigate the third type of exploitation of data poisoning - increasing the risks of privacy leakage of benign training samples. To this end, we demonstrate a set of data poisoning attacks to amplify the membership exposure of the targeted class. We first propose a generic dirty-label attack for supervised classification algorithms. We then propose an optimization-based clean-label attack in the transfer learning scenario, whereby the poisoning samples are correctly labeled and look ""natural"" to evade human moderation. We extensively evaluate our attacks on computer vision benchmarks. Our results show that the proposed attacks can substantially increase the membership inference precision with minimum overall test-time model performance degradation. To mitigate the potential negative impacts of our attacks, we also investigate feasible countermeasures.",,https://openreview.net/forum?id=mT18WLu9J_,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54281.png?t=1669734758.2070205,https://neurips.cc/virtual/2022/poster/54281,Poisoning,Attack,"['data poisoning attacks', 'membership exposure', 'dirty-label attack', 'clean-label attack', 'transfer learning', 'computer vision benchmarks', 'membership inference precision', 'countermeasures']",,,,,,,
2273,https://neurips.cc/virtual/2022/poster/54291,Are AlphaZero-like Agents Robust to Adversarial Perturbations?,Poster,NeurIPS,2022,"The success of AlphaZero (AZ) has demonstrated that neural-network-based Go AIs can surpass human performance by a large margin. Given that the state space of Go is extremely large and a human player can play the game from any legal state, we ask whether adversarial states exist for Go AIs that may lead them to play surprisingly wrong actions.In this paper, we first extend the concept of adversarial examples to the game of Go: we generate perturbed states that are ``semantically'' equivalent to the original state by adding meaningless moves to the game, and an adversarial state is a perturbed state leading to an undoubtedly inferior action that is obvious even for Go beginners. However, searching the adversarial state is challenging due to the large, discrete, and non-differentiable search space. To tackle this challenge, we develop the first adversarial attack on Go AIs that can efficiently search for adversarial states by strategically reducing the search space. This method can also be extended to other board games such as NoGo. Experimentally, we show that the actions taken by both Policy-Value neural network (PV-NN) and Monte Carlo tree search (MCTS) can be misled by adding one or two meaningless stones; for example, on 58\% of the AlphaGo Zero self-play games, our method can make the widely used KataGo agent with 50 simulations of MCTS plays a losing action by adding two meaningless stones. We additionally evaluatedthe adversarial examples found by our algorithm with amateur human Go players and 90\%of examples indeed lead the Go agent to play an obviously inferior action.",,https://openreview.net/forum?id=yZ_JlZaOCzv,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54291.png?t=1669823859.8710613,https://neurips.cc/virtual/2022/poster/54291,Evasion,Attack,"['AlphaZero', 'Adversarial Perturbations', 'Go', 'Adversarial examples', 'Semantically equivalent', 'Meaningless moves', 'Inferior action', 'Policy-Value neural network', 'Monte Carlo tree search', 'NoGo', 'KataGo']",['Go'],Attack,Evasion,,,,
2279,https://neurips.cc/virtual/2022/poster/54265,Modeling the Machine Learning Multiverse,Poster,NeurIPS,2022,"Amid mounting concern about the reliability and credibility of machine learning research, we present a principled framework for making robust and generalizable claims: the multiverse analysis. Our framework builds upon the multiverse analysis introduced in response to psychology's own reproducibility crisis. To efficiently explore high-dimensional and often continuous ML search spaces, we model the multiverse with a Gaussian Process surrogate and apply Bayesian experimental design. Our framework is designed to facilitate drawing robust scientific conclusions about model performance, and thus our approach focuses on exploration rather than conventional optimization. In the first of two case studies, we investigate disputed claims about the relative merit of adaptive optimizers.  Second, we synthesize conflicting research on the effect of learning rate on the large batch training generalization gap. For the machine learning community, the multiverse analysis is a simple and effective technique for identifying robust claims, for increasing transparency, and a step toward improved reproducibility.",,https://openreview.net/forum?id=8OH6t0YQGPJ,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54265.png?t=1669735161.9355652,https://neurips.cc/virtual/2022/poster/54265,Not relevant,Other aspects,"['Multiverse analysis', 'Robust and generalizable claims', 'Machine learning research', 'Gaussian Process surrogate', 'Bayesian experimental design', 'Model performance', 'Adaptive optimizers', 'Learning rate', 'Large batch training generalization gap', 'Reproducibility']",[],,,,,,
2284,https://neurips.cc/virtual/2022/poster/54259,Rethinking and Improving Robustness of Convolutional Neural Networks: a Shapley Value-based Approach in Frequency Domain,Poster,NeurIPS,2022,"The existence of adversarial examples poses concerns for the robustness of convolutional neural networks (CNN), for which a popular hypothesis is about the frequency bias phenomenon: CNNs rely more on high-frequency components (HFC) for classification than humans, which causes the brittleness of CNNs. However, most previous works manually select and roughly divide the image frequency spectrum and conduct qualitative analysis. In this work, we introduce Shapley value, a metric of cooperative game theory, into the frequency domain and propose to quantify the positive (negative) impact of every frequency component of data on CNNs. Based on the Shapley value, we quantify the impact in a fine-grained way and show intriguing instance disparity. Statistically, we investigate adversarial training(AT) and the adversarial attack in the frequency domain. The observations motivate us to perform an in-depth analysis and lead to multiple novel hypotheses about i) the cause of adversarial robustness of the AT model; ii) the fairness problem of AT between different classes in the same dataset; iii) the attack bias on different frequency components. Finally, we propose a Shapley-value guided data augmentation technique for improving the robustness. Experimental results on image classification benchmarks show its effectiveness.",,https://openreview.net/forum?id=rQ1cNbi07Vq,https://neurips.cc/virtual/2022/poster/54259,https://neurips.cc/virtual/2022/poster/54259,Evasion,Defence,"['convolutional neural networks', 'robustness', 'adversarial examples', 'frequency bias', 'Shapley value', 'frequency domain', 'adversarial training', 'adversarial attack', 'data augmentation']",['image classification'],,,,,,
2295,https://neurips.cc/virtual/2022/poster/54223,Between Stochastic and Adversarial Online Convex Optimization: Improved Regret Bounds via Smoothness,Poster,NeurIPS,2022,"Stochastic and adversarial data are two widely studied settings in online learning. But many optimizationtasks are neither i.i.d. nor fully adversarial, which makes it of  fundamental interest to get a better theoretical understanding of the world between these extremes. In this work we establish novel regret bounds for online convex optimization in a setting that interpolates between stochastic i.i.d. and fully adversarial losses. By exploiting smoothness of the expected losses, these bounds replace a dependence on the maximum gradient length by the variance of the gradients, which was previously known only for linear losses. In addition, they weaken the i.i.d. assumption by allowing, for example, adversarially poisoned rounds, which were previously considered in the expert and bandit setting. Our results extend this to the online convex optimization framework.  In the fully i.i.d. case, our bounds match the rates one would expect from results in stochastic acceleration, and in the fully adversarial case they gracefully deteriorate to match the minimax regret.  We further provide lower bounds showing that our regret upper bounds aretight for all intermediate regimes in terms of the stochastic variance and theadversarial variation of the loss gradients.",,https://openreview.net/forum?id=_gA20SUfd4a,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54223.png?t=1669327471.6066105,https://neurips.cc/virtual/2022/poster/54223,Not relevant,Defence,"['Online convex optimization', 'Regret bounds', 'Stochastic', 'Adversarial', 'Smoothness']",[],,,,,,
2296,https://neurips.cc/virtual/2022/poster/54233,Moment Distributionally Robust Tree Structured Prediction,Poster,NeurIPS,2022,"Structured prediction of tree-shaped objects is heavily studied under the name of syntactic dependency parsing. Current practice based on maximum likelihood or margin is either agnostic to or inconsistent with the evaluation loss. Risk minimization alleviates the discrepancy between training and test objectives but typically induces a non-convex problem. These approaches adopt explicit regularization to combat overfitting without probabilistic interpretation. We propose a moment-based distributionally robust optimization approach for tree structured prediction, where the worst-case expected loss over a set of distributions within bounded moment divergence from the empirical distribution is minimized. We develop efficient algorithms for arborescences and other variants of trees. We derive Fisher consistency, convergence rates and generalization bounds for our proposed method. We evaluate its empirical effectiveness on dependency parsing benchmarks.",,https://openreview.net/forum?id=Tq2XqINV1Jz,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54233.png?t=1669219081.98299,https://neurips.cc/media/neurips-2022/Slides/54233_PfYBkLU.pdf,Not relevant,Defence,"['structured prediction', 'tree-shaped objects', 'syntactic dependency parsing', 'maximum likelihood', 'margin', 'risk minimization', 'non-convex problem', 'regularization', 'probabilistic interpretation', 'moment-based distributionally robust optimization', 'worst-case expected loss', 'bounded moment divergence', 'algorithms', 'Fisher consistency', 'convergence rates', 'generalization bounds', 'dependency parsing benchmarks']",['Dependency parsing'],,,,,,
2297,https://neurips.cc/virtual/2022/poster/54214,Efficient Adversarial Training without Attacking: Worst-Case-Aware Robust Reinforcement Learning,Poster,NeurIPS,2022,"Recent studies reveal that a well-trained deep reinforcement learning (RL) policy can be particularly vulnerable to adversarial perturbations on input observations. Therefore, it is crucial to train RL agents that are robust against any attacks with a bounded budget. Existing robust training methods in deep RL either treat correlated steps separately, ignoring the robustness of long-term rewards, or train the agents and RL-based attacker together, doubling the computational burden and sample complexity of the training process. In this work, we propose a strong and efficient robust training framework for RL, named Worst-case-aware Robust RL (WocaR-RL) that directly estimates and optimizes the worst-case reward of a policy under bounded l_p attacks without requiring extra samples for learning an attacker. Experiments on multiple environments show that WocaR-RL achieves state-of-the-art performance under various strong attacks, and obtains significantly higher training efficiency than prior state-of-the-art robust training methods. The code of this work is available at https://github.com/umd-huang-lab/WocaR-RL.",,https://openreview.net/forum?id=y-E1htoQl-n,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54214.png?t=1669270765.2098591,https://neurips.cc/media/neurips-2022/Slides/54214.pdf,Evasion,Defence,"['Adversarial perturbations', 'Deep reinforcement learning', 'Robustness', 'Worst-case-aware', 'Robust training framework', 'RL', 'Bounded l_p attacks']",,Defence,Evasion,,,,
2303,https://neurips.cc/virtual/2022/poster/54205,Improving GANs with A Dynamic Discriminator,Poster,NeurIPS,2022,"Discriminator plays a vital role in training generative adversarial networks (GANs) via distinguishing real and synthesized samples. While the real data distribution remains the same, the synthesis distribution keeps varying because of the evolving generator, and thus effects a corresponding change of the bi-classification task assigned to the discriminator. We argue that a discriminator with an on-the-fly adjustment on its capacity can better accommodate such a time-varying task. A comprehensive empirical study confirms that the proposed training strategy, termed as DynamicD, improves the synthesis performance without incurring any additional computation cost or training objectives. Two capacity adjusting schemes are developed for training GANs under different data regimes: i) given a sufficient amount of training data, the discriminator benefits from a progressively increased learning capacity, and ii) when the training data is limited, gradually decreasing the layer width mitigates the over-fitting issue of the discriminator. Experiments on both 2D and 3D-aware image synthesis tasks conducted on a range of datasets substantiate the generalizability of our DynamicD as well as its substantial improvement over the baselines. Furthermore, DynamicD is synergistic to other discriminator-improving approaches (including data augmentation, regularizers, and pre-training), and brings continuous performance gain when combined with them for learning GANs. Code will be made publicly available.",,https://openreview.net/forum?id=Cp9sWmkd1H0,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/7ed2d3454c5eea71148b11d0c25104ff.png?t=1666231507.769036,https://neurips.cc/virtual/2022/poster/54205,Not relevant,Defence,"['Generative Adversarial Networks', 'Discriminator', 'Synthesis Performance', 'GANs', 'Data regimes', 'Image Synthesis']","['2D', '3D-aware image synthesis']",,,,,,
2305,https://neurips.cc/virtual/2022/poster/54192,Sparse Winning Tickets are Data-Efficient Image Recognizers,Poster,NeurIPS,2022,"Improving the performance of deep networks in data-limited regimes has warranted much attention. In this work, we empirically show that “winning tickets” (small sub-networks) obtained via magnitude pruning based on the lottery ticket hypothesis, apart from being sparse are also effective recognizers in data-limited regimes. Based on extensive experiments, we find that in low data regimes (datasets of 50-100 examples per class), sparse winning tickets substantially outperform the original dense networks. This approach, when combined with augmentations or fine-tuning from a self-supervised backbone network, shows further improvements in performance by as much as 16% (absolute) on low-sample datasets and long-tailed classification. Further, sparse winning tickets are more robust to synthetic noise and distribution shifts compared to their dense counterparts. Our analysis of winning tickets on small datasets indicates that, though sparse, the networks retain density in the initial layers and their representations are more generalizable. Code is available at https://github.com/VITA-Group/DataEfficientLTH.",,https://openreview.net/forum?id=wfKbtSjHA6F,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54192.png?t=1669487313.4348316,https://neurips.cc/media/neurips-2022/Slides/54192.pdf,Robustness,Defence,"['winning tickets', 'lottery ticket hypothesis', 'magnitude pruning', 'sparse', 'deep networks', 'performance', 'data-limited regimes', 'recognizers', 'low data regimes', 'datasets', 'long-tailed classification', 'generalizable', 'robust']",['image recognition'],,,,,,
2322,https://neurips.cc/virtual/2022/poster/54142,Robust Binary Models by Pruning Randomly-initialized Networks,Poster,NeurIPS,2022,"Robustness to adversarial attacks was shown to require a larger model capacity, and thus a larger memory footprint. In this paper, we introduce an approach to obtain robust yet compact models by pruning randomly-initialized binary networks. Unlike adversarial training, which learns the model parameters, we initialize the model parameters as either +1 or −1, keep them fixed, and find a subnetwork structure that is robust to attacks. Our method confirms the Strong Lottery Ticket Hypothesis in the presence of adversarial attacks, and extends this to binary networks. Furthermore, it yields more compact networks with competitive performance than existing works by 1) adaptively pruning different network layers; 2) exploiting an effective binary initialization scheme; 3) incorporating a last batch normalization layer to improve training stability. Our experiments demonstrate that our approach not only always outperforms the state-of-the-art robust binary networks, but also can achieve accuracy better than full-precision ones on some datasets. Finally, we show the structured patterns of our pruned binary networks.",,https://openreview.net/forum?id=5g-h_DILemH,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/64ff7983a47d331b13a81156e2f4d29d.png?t=1667587035.5195856,https://neurips.cc/virtual/2022/poster/54142,Evasion,Defence,"['Robust Binary Models', 'Pruning Randomly-initialized Networks', 'Adversarial attacks', 'Model capacity', 'Memory footprint', 'binary networks', 'Strong Lottery Ticket Hypothesis', 'Binary initialization scheme', 'Batch normalization layer', 'Training stability']",['None'],,,,,,
2327,https://neurips.cc/virtual/2022/poster/54125,Efficient Training of Low-Curvature Neural Networks,Poster,NeurIPS,2022,"Standard deep neural networks often have excess non-linearity, making them susceptible to issues such as low adversarial robustness and gradient instability. Common methods to address these downstream issues, such as adversarial training, are expensive and often sacrifice predictive accuracy. In this work, we address the core issue of excess non-linearity via curvature, and demonstrate low-curvature neural networks (LCNNs) that obtain drastically lower curvature than standard models while exhibiting similar predictive performance. This leads to improved robustness and stable gradients, at a fraction of the cost of standard adversarial training. To achieve this, we decompose overall model curvature in terms of curvatures and slopes of its constituent layers. To enable efficient curvature minimization of constituent layers, we introduce two novel architectural components: first, a non-linearity called centered-softplus that is a stable variant of the softplus non-linearity, and second, a Lipschitz-constrained batch normalization layer.Our experiments show that LCNNs have lower curvature, more stable gradients and increased off-the-shelf adversarial robustness when compared to standard neural networks, all without affecting predictive performance. Our approach is easy to use and can be readily incorporated into existing neural network architectures.",,https://openreview.net/forum?id=2B2xIJ299rx,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/aff82e881075d9c1ec306f86ae15c833.png?t=1666999423.827189,https://neurips.cc/virtual/2022/poster/54125,Robustness,Defence,"['Low-Curvature Neural Networks', 'Curvature', 'Adversarial robustness', 'Gradient stability', 'Centered-softplus', 'Lipschitz-constrained batch normalization']",,Defence,Evasion,Other aspects,,Arguable,
2343,https://neurips.cc/virtual/2022/poster/54051,Data-IQ: Characterizing subgroups with heterogeneous outcomes in tabular data,Poster,NeurIPS,2022,"High model performance, on average, can hide that models may systematically underperform on subgroups of the data. We consider the tabular setting, which surfaces the unique issue of outcome heterogeneity - this is prevalent in areas such as healthcare, where patients with similar features can have different outcomes, thus making reliable predictions challenging. To tackle this, we propose Data-IQ, a framework to systematically stratify examples into subgroups with respect to their outcomes. We do this by analyzing the behavior of individual examples during training, based on their predictive confidence and, importantly, the aleatoric (data) uncertainty. Capturing the aleatoric uncertainty permits a principled characterization and then subsequent stratification of data examples into three distinct subgroups (Easy, Ambiguous, Hard). We experimentally demonstrate the benefits of Data-IQ on four real-world medical datasets. We show that Data-IQ's characterization of examples is most robust to variation across similarly performant (yet different models), compared to baselines. Since Data-IQ can be used with any ML model (including neural networks, gradient boosting etc.), this property ensures consistency of data characterization, while allowing flexible model selection. Taking this a step further, we demonstrate that the subgroups enable us to construct new approaches to both feature acquisition and dataset selection. Furthermore, we highlight how the subgroups can inform reliable model usage, noting the significant impact of the Ambiguous subgroup on model generalization.",,https://openreview.net/forum?id=qC2BwvfaNdd,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54051.png?t=1669031357.5002835,https://neurips.cc/virtual/2022/poster/54051,Robustness,Defence,"['Tabular setting', 'Outcome heterogeneity', 'Subgroup analysis', 'Predictive confidence', 'Aleatoric uncertainty', 'Data-IQ', 'Stratification', 'Medical datasets', 'ML model selection']",['Healthcare'],,,,,,
2354,https://neurips.cc/virtual/2022/poster/53993,Density-driven Regularization for Out-of-distribution Detection,Poster,NeurIPS,2022,"Detecting out-of-distribution (OOD) samples is essential for reliably deploying deep learning classifiers in open-world applications. However, existing detectors relying on discriminative probability suffer from the overconfident posterior estimate for OOD data. Other reported approaches either impose strong unproven parametric assumptions to estimate OOD sample density or develop empirical detectors lacking clear theoretical motivations. To address these issues, we propose a theoretical probabilistic framework for OOD detection in deep classification networks, in which two regularization constraints are constructed to reliably calibrate and estimate sample density to identify OOD. Specifically, the density consistency regularization enforces the agreement between analytical and empirical densities of observable low-dimensional categorical labels. The contrastive distribution regularization separates the densities between in distribution (ID) and distribution-deviated samples. A simple and robust implementation algorithm is also provided, which can be used for any pre-trained neural network classifiers. To the best of our knowledge, we have conducted the most extensive evaluations and comparisons on computer vision benchmarks. The results show that our method significantly outperforms state-of-the-art detectors, and even achieves comparable or better performance than methods utilizing additional large-scale outlier exposure datasets.",,https://openreview.net/forum?id=aZQJMVx8fk,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/c55430fdfdac11fc3771a9b94e2bb854.png?t=1666184914.79281,https://neurips.cc/virtual/2022/poster/53993,Robustness,Defence,"['Out-of-distribution detection', 'Deep learning classifiers', 'Probabilistic framework', 'Regularization constraints', 'Density consistency', 'Contrastive distribution regularization', 'Neural network classifiers']",['Computer vision benchmarks'],,,,,,
2363,https://neurips.cc/virtual/2022/poster/53976,CoPur: Certifiably Robust Collaborative Inference via Feature Purification,Poster,NeurIPS,2022,"Collaborative inference leverages diverse features provided by different agents (e.g., sensors) for more accurate inference. A common setup is where each agent sends its embedded features instead of the raw data to the Fusion Center (FC) for joint prediction. In this setting, we consider the inference-time attacks when a small fraction of agents are compromised. The compromised agent either does not send embedded features to the FC, or sends arbitrarily embedded features. To address this, we propose a certifiably robust COllaborative inference framework via feature PURification (CoPur), by leveraging the block-sparse nature of adversarial perturbations on the feature vector, as well as exploring the underlying redundancy across the embedded features (by assuming the overall features lie on an underlying lower dimensional manifold). We theoretically show that the proposed feature purification method can robustly recover the true feature vector, despite adversarial corruptions and/or incomplete observations. We also propose and test an untargeted distributed feature-flipping attack, which is agnostic to the model, training data, label, as well as the features held by other agents, and is shown to be effective in attacking state-of-the-art defenses. Experiments on ExtraSensory and NUS-WIDE datasets show that CoPur significantly outperforms existing defenses in terms of robustness against targeted and untargeted adversarial attacks.",,https://openreview.net/forum?id=r5rzV51GZx,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53976.png?t=1669584162.0567286,https://neurips.cc/virtual/2022/poster/53976,Evasion,Defence,"['Collaborative inference', 'Inference-time attacks', 'Block-sparse nature of adversarial perturbations', 'Redundancy across embedded features', 'Lower dimensional manifold', 'Certifiably robust', 'Feature purification', 'Untargeted distributed feature-flipping attack', 'ExtraSensory dataset', 'NUS-WIDE dataset']","['ExtraSensory dataset', 'NUS-WIDE dataset']",,,,,,
2401,https://neurips.cc/virtual/2022/poster/53874,Bayesian Spline Learning for Equation Discovery of Nonlinear Dynamics with Quantified Uncertainty,Poster,NeurIPS,2022,"Nonlinear dynamics are ubiquitous in science and engineering applications, but the physics of most complex systems is far from being fully understood. Discovering interpretable governing equations from measurement data can help us understand and predict the behavior of complex dynamic systems. Although extensive work has recently been done in this field, robustly distilling explicit model forms from very sparse data with considerable noise remains intractable. Moreover, quantifying and propagating the uncertainty of the identified system from noisy data is challenging, and relevant literature is still limited. To bridge this gap, we develop a novel Bayesian spline learning framework to identify parsimonious governing equations of nonlinear (spatio)temporal dynamics from sparse, noisy data with quantified uncertainty. The proposed method utilizes spline basis to handle the data scarcity and measurement noise, upon which a group of derivatives can be accurately computed to form a library of candidate model terms. The equation residuals are used to inform the spline learning in a Bayesian manner, where approximate Bayesian uncertainty calibration techniques are employed to approximate posterior distributions of the trainable parameters. To promote the sparsity, an iterative sequential-threshold Bayesian learning approach is developed, using the alternative direction optimization strategy to systematically approximate L0 sparsity constraints. The proposed algorithm is evaluated on multiple nonlinear dynamical systems governed by canonical ordinary and partial differential equations, and the merit/superiority of the proposed method is demonstrated by comparison with state-of-the-art methods.",,https://openreview.net/forum?id=_5rdhnrbl-z,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/7c250678f61f49092fa0d4040e5e54e9.png?t=1667765186.1983197,https://neurips.cc/virtual/2022/poster/53874,Robustness,Defence,"['Bayesian Spline Learning', 'Equation Discovery', 'Nonlinear Dynamics', 'Quantified Uncertainty', 'Robustly distilling explicit model forms', 'Sparse data', 'Noise', 'Uncertainty', 'Spline basis', 'Derivatives', 'Bayesian uncertainty calibration', 'Sparsity', 'Alternative direction optimization']",['Nonlinear dynamical systems'],,,,,,
2407,https://neurips.cc/virtual/2022/poster/53756,Faster Deep Reinforcement Learning with Slower Online Network,Poster,NeurIPS,2022,"Deep reinforcement learning algorithms often use two networks for value function optimization: an online network, and a target network that tracks the online network with some delay. Using two separate networks enables the agent to hedge against issues that arise when performing bootstrapping. In this paper we endow two popular deep reinforcement learning algorithms, namely DQN and Rainbow, with updates that incentivize the online network to remain in the proximity of the target network. This improves the robustness of deep reinforcement learning in presence of noisy updates. The resultant agents, called DQN Pro and Rainbow Pro, exhibit significant performance improvements over their original counterparts on the Atari benchmark demonstrating the effectiveness of this simple idea in deep reinforcement learning. The code for our paper is available here: Github.com/amazon-research/fast-rl-with-slow-updates.",,https://openreview.net/forum?id=Cl9dcH6Xkcj,https://neurips.cc/virtual/2022/poster/53756,https://neurips.cc/virtual/2022/poster/53756,Not relevant,Defence,"['Deep Reinforcement Learning', 'Value function optimization', 'Online network', 'Target network', 'DQN', 'Rainbow', 'Noisy updates', 'Performance improvements', 'Atari benchmark']",['Reinforcement Learning'],,,,,,
2419,https://neurips.cc/virtual/2022/poster/53696,Sparsity in Continuous-Depth Neural Networks,Poster,NeurIPS,2022,"Neural Ordinary Differential Equations (NODEs) have proven successful in learning dynamical systems in terms of accurately recovering the observed trajectories. While different types of sparsity have been proposed to improve robustness, the generalization properties of NODEs for dynamical systems beyond the observed data are underexplored. We systematically study the influence of weight and feature sparsity on forecasting as well as on identifying the underlying dynamical laws. Besides assessing existing methods, we propose a regularization technique to sparsify ``input-output connections'' and extract relevant features during training. Moreover, we curate real-world datasets including human motion capture and human hematopoiesis single-cell RNA-seq data to realistically analyze different levels of out-of-distribution (OOD) generalization in forecasting and dynamics identification respectively. Our extensive empirical evaluation on these challenging benchmarks suggests that weight sparsity improves generalization in the presence of noise or irregular sampling. However, it does not prevent learning spurious feature dependencies in the inferred dynamics, rendering them impractical for predictions under interventions, or for inferring the true underlying dynamics. Instead, feature sparsity can indeed help with recovering sparse ground-truth dynamics compared to unregularized NODEs.",,https://openreview.net/forum?id=HZ20IYYAwah,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53696.png?t=1669836954.1971164,https://neurips.cc/virtual/2022/poster/53696,Robustness,Defence,"['Neural Ordinary Differential Equations (NODEs)', 'Weight sparsity', 'Feature sparsity', 'Out-of-distribution (OOD) generalization', 'Forecasting', 'Dynamics identification', 'Regularization technique', 'Challenging benchmarks', 'Spurious feature dependencies', 'Predictions under interventions', 'Inferring true underlying dynamics']","['Human motion capture', 'Human hematopoiesis single-cell RNA-seq data']",,,,,,
2424,https://neurips.cc/virtual/2022/poster/53733,Robust On-Policy Sampling for Data-Efficient Policy Evaluation in Reinforcement Learning,Poster,NeurIPS,2022,"Reinforcement learning (RL) algorithms are often categorized as either on-policy or off-policy  depending on whether they use data from a target policy of interest or from a different behavior policy. In this paper, we study a subtle distinction between on-policy data and on-policy sampling in the context of the RL sub-problem of policy evaluation. We observe that on-policy sampling may fail to match the expected distribution of on-policy data after observing only a finite number of trajectories and this failure hinders data-efficient policy evaluation. Towards improved data-efficiency, we show how non-i.i.d., off-policy sampling can produce data that more closely matches the expected on-policy data distribution and consequently increases the accuracy of the Monte Carlo estimator for policy evaluation. We introduce a method called Robust On-Policy Sampling and demonstrate theoretically and empirically that it produces data that converges faster to the expected on-policy distribution compared to on-policy sampling. Empirically, we show that this faster convergence leads to lower mean squared error policy value estimates.",,https://openreview.net/forum?id=eK8Z4Ydt2_b,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53733.png?t=1669511994.889515,https://neurips.cc/virtual/2022/poster/53733,Not relevant,Defence,"['reinforcement learning', 'policy evaluation', 'on-policy sampling', 'off-policy sampling', 'Monte Carlo estimator']",[],,,,,,
2443,https://neurips.cc/virtual/2022/poster/53666,Learning to Mitigate AI Collusion on Economic Platforms,Poster,NeurIPS,2022,"Algorithmic pricing on online e-commerce platforms raises the concern of tacit collusion, where reinforcement learning algorithms learn to set collusive prices in a decentralized manner and through nothing more than profit feedback. This raises the question as to whether collusive pricing can be prevented through the design of suitable ""buy boxes,"" i.e., through the design of the rules that govern the elements of e-commerce sites that promote particular products and prices to consumers. In this paper, we demonstrate that reinforcement learning (RL) can also be used by platforms to learn buy box rules that are effective in preventing collusion by RL sellers. For this, we adopt the methodology of Stackelberg POMDPs, and demonstrate success in learning robust rules that continue to provide high consumer welfare together with sellers employing different behavior models or having out-of-distribution costs for goods.",,https://openreview.net/forum?id=ONFaDyl_uVq,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53666.png?t=1669612705.9430702,https://neurips.cc/virtual/2022/poster/53666,Other attack,Defence,"['Algorithmic pricing', 'Economic platforms', 'AI collusion', 'Reinforcement learning', 'Decentralized pricing', 'Buy boxes', 'Stackelberg POMDPs']",['Online e-commerce'],,,,,,
2446,https://neurips.cc/virtual/2022/poster/53659,Human-AI Collaborative Bayesian Optimisation,Poster,NeurIPS,2022,"Abstract Human-AI collaboration looks at harnessing the complementary strengths of both humans and AI. We propose a new method for human-AI collaboration in Bayesian optimisation where the optimum is mainly pursued by the Bayesian optimisation algorithm following complex computation, whilst getting occasional help from the accompanying expert having a deeper knowledge of the underlying physical phenomenon. We expect experts to have some understanding of the correlation structures of the experimental system, but not the location of the optimum. The expert provides feedback by either changing the current recommendation or providing her belief on the good and bad regions of the search space based on the current observations. Our proposed method takes such feedback to build a model that aligns with the expert’s model and then uses it for optimisation. We provide theoretical underpinning on why such an approach may be more efficient than the one without expert’s feedback. The empirical results show the robustness and superiority of our method with promising efficiency gains.",,https://openreview.net/forum?id=atd4X6U1jT,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53659.png?t=1669723400.595577,https://neurips.cc/media/neurips-2022/Slides/53659.pdf,Not relevant,Other aspects,"['Human-AI collaboration', 'Bayesian optimization', 'Expert feedback', 'Optimization efficiency']",['None'],,,,,,
2459,https://neurips.cc/virtual/2022/poster/53606,What's the Harm? Sharp Bounds on the Fraction Negatively Affected by Treatment,Poster,NeurIPS,2022,"The fundamental problem of causal inference -- that we never observe counterfactuals -- prevents us from identifying how many might be negatively affected by a proposed intervention. If, in an A/B test, half of users click (or buy, or watch, or renew, etc.), whether exposed to the standard experience A or a new one B, hypothetically it could be because the change affects no one,  because the change positively affects half the user population to go from no-click to click while negatively affecting the other half, or something in between. While unknowable, this impact is clearly of material importance to the decision to implement a change or not, whether due to fairness, long-term, systemic, or operational considerations. We therefore derive the tightest-possible (i.e., sharp) bounds on the fraction negatively affected (and other related estimands) given data with only factual observations, whether experimental or observational. Naturally, the more we can stratify individuals by observable covariates, the tighter the sharp bounds. Since these bounds involve unknown functions that must be learned from data, we develop a robust inference algorithm that is efficient almost regardless of how and how fast these functions are learned, remains consistent when some are mislearned, and still gives valid conservative bounds when most are mislearned. Our methodology altogether therefore strongly supports credible conclusions: it avoids spuriously point-identifying this unknowable impact, focusing on the best bounds instead, and it permits exceedingly robust inference on these. We demonstrate our method in simulation studies and in a case study of career counseling for the unemployed.",,https://openreview.net/forum?id=_Lz540aYDPi,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53606.png?t=1669686086.2810557,https://neurips.cc/media/neurips-2022/Slides/53606.pdf,Not relevant,Not relevant,"['causal inference', 'counterfactuals', 'A/B testing', 'experimental data', 'observational data', 'estimands', 'sharp bounds', 'covariates', 'robust inference', 'algorithm', 'conservative bounds', 'career counseling']",['unemployed'],,,,,,
2464,https://neurips.cc/virtual/2022/poster/53588,Understanding Square Loss in Training Overparametrized Neural Network Classifiers,Poster,NeurIPS,2022,"Deep learning has achieved many breakthroughs in modern classification tasks. Numerous architectures have been proposed for different data structures but when it comes to the loss function, the cross-entropy loss is the predominant choice. Recently, several alternative losses have seen revived interests for deep classifiers. In particular, empirical evidence seems to promote square loss but a theoretical justification is still lacking. In this work, we contribute to the theoretical understanding of square loss in classification by systematically investigating how it performs for overparametrized neural networks in the neural tangent kernel (NTK) regime. Interesting properties regarding the generalization error, robustness, and calibration error are revealed. We consider two cases, according to whether classes are separable or not. In the general non-separable case, fast convergence rate is established for both misclassification rate and calibration error. When classes are separable, the misclassification rate improves to be exponentially fast. Further, the resulting margin is proven to be lower bounded away from zero, providing theoretical guarantees for robustness. We expect our findings to hold beyond the NTK regime and translate to practical settings. To this end, we conduct extensive empirical studies on practical neural networks, demonstrating the effectiveness of square loss in both synthetic low-dimensional data and real image data. Comparing to cross-entropy, square loss has comparable generalization error but noticeable advantages in robustness and model calibration.",,https://openreview.net/forum?id=q85GV4aSpt,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53588.png?t=1669523516.3219552,https://neurips.cc/media/neurips-2022/Slides/53588.pdf,Robustness,Defence,"['square loss', 'classification', 'overparametrized neural networks', 'neural tangent kernel', 'generalization error', 'robustness', 'calibration error', 'separable classes', 'misclassification rate', 'margin']","['Synthetic low-dimensional data', 'Real image data']",,,,,,
2472,https://neurips.cc/virtual/2022/poster/53577,Patching open-vocabulary models by interpolating weights,Poster,NeurIPS,2022,"Open-vocabulary models like CLIP achieve high accuracy across many image classification tasks. However, there are still settings where their zero-shot performance is far from optimal. We study model patching, where the goal is to improve accuracy on specific tasks without degrading accuracy on tasks where performance is already adequate. Towards this goal, we introduce PAINT, a patching method that uses interpolations between the weights of a model before fine-tuning and the weights after fine-tuning on a task to be patched. On nine tasks where zero-shot CLIP performs poorly, PAINT increases accuracy by 15 to 60 percentage points while preserving accuracy on ImageNet within one percentage point of the zero-shot model. PAINT also allows a single model to be patched on multiple tasks and improves with model scale. Furthermore, we identify cases of broad transfer, where patching on one task increases accuracy on other tasks even when the tasks have disjoint classes. Finally, we investigate applications beyond common benchmarks such as counting or reducing the impact of typographic attacks on CLIP. Our findings demonstrate that it is possible to expand the set of tasks on which open-vocabulary models achieve high accuracy without re-training them from scratch.",,https://openreview.net/forum?id=CZZFRxbOLC,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53577.png?t=1669490714.0044346,https://neurips.cc/virtual/2022/poster/53577,Not relevant,Defence,"['open-vocabulary models', 'image classification', 'model patching', 'PAINT', 'fine-tuning', 'zero-shot performance', 'model scale', 'transfer learning', 'typographic attacks', 'CLIP']",['ImageNet'],,,,,,
2473,https://neurips.cc/virtual/2022/poster/53575,Maximum Likelihood Training of Implicit Nonlinear Diffusion Model,Poster,NeurIPS,2022,"Whereas diverse variations of diffusion models exist, extending the linear diffusion into a nonlinear diffusion process is investigated by very few works. The nonlinearity effect has been hardly understood, but intuitively, there would be promising diffusion patterns to efficiently train the generative distribution towards the data distribution. This paper introduces a data-adaptive nonlinear diffusion process for score-based diffusion models. The proposed Implicit Nonlinear Diffusion Model (INDM) learns by combining a normalizing flow and a diffusion process. Specifically, INDM implicitly constructs a nonlinear diffusion on the data space by leveraging a linear diffusion on the latent space through a flow network. This flow network is key to forming a nonlinear diffusion, as the nonlinearity depends on the flow network. This flexible nonlinearity improves the learning curve of INDM to nearly Maximum Likelihood Estimation (MLE) against the non-MLE curve of DDPM++, which turns out to be an inflexible version of INDM with the flow fixed as an identity mapping. Also, the discretization of INDM shows the sampling robustness. In experiments, INDM achieves the state-of-the-art FID of 1.75 on CelebA. We release our code at https://github.com/byeonghu-na/INDM.",,https://openreview.net/forum?id=TQn44YPuOR2,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53575.png?t=1669189700.1308908,https://neurips.cc/media/neurips-2022/Slides/53575.pdf,Not relevant,Defence,"['Implicit Nonlinear Diffusion Model (INDM)', 'Diffusion Models', 'Linear Diffusion', 'Nonlinear Diffusion', 'Normalizing flow', 'Latent space', 'Flow network', 'Maximum Likelihood Estimation (MLE)', 'Generative distribution', 'Data distribution', 'Score-based diffusion models']",[],,,,,,
2490,https://neurips.cc/virtual/2022/poster/53516,Label Noise in Adversarial Training: A Novel Perspective to Study Robust Overfitting,Poster,NeurIPS,2022,"We show that label noise exists in adversarial training. Such label noise is due to the mismatch between the true label distribution of adversarial examples and the label inherited from clean examples – the true label distribution is distorted by the adversarial perturbation, but is neglected by the common practice that inherits labels from clean examples. Recognizing label noise sheds insights on the prevalence of robust overfitting in adversarial training, and explains its intriguing dependence on perturbation radius and data quality. Also, our label noise perspective aligns well with our observations of the epoch-wise double descent in adversarial training. Guided by our analyses, we proposed a method to automatically calibrate the label to address the label noise and robust overfitting. Our method achieves consistent performance improvements across various models and datasets without introducing new hyper-parameters or additional tuning.",,https://openreview.net/forum?id=9_O9mTLYJQp,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53516.png?t=1669836424.01432,https://neurips.cc/media/neurips-2022/Slides/53516.pdf,Robustness,Defence,"['label noise', 'adversarial training', 'robust overfitting', 'perturbation radius', 'data quality', 'double descent', 'calibration', 'performance improvements', 'models', 'datasets']",['None'],,,,,,
2501,https://neurips.cc/virtual/2022/poster/53467,HyperDomainNet: Universal Domain Adaptation for Generative Adversarial Networks,Poster,NeurIPS,2022,"Domain adaptation framework of GANs has achieved great progress in recent years as a main successful approach of training contemporary GANs in the case of very limited training data. In this work, we significantly improve this framework by proposing an extremely compact parameter space for fine-tuning the generator. We introduce a novel domain-modulation technique that allows to optimize only 6 thousand-dimensional vector instead of 30 million weights of StyleGAN2 to adapt to a target domain. We apply this parameterization to the state-of-art domain adaptation methods and show that it has almost the same expressiveness as the full parameter space. Additionally, we propose a new regularization loss that considerably enhances the diversity of the fine-tuned generator. Inspired by the reduction in the size of the optimizing parameter space we consider the problem of multi-domain adaptation of GANs, i.e. setting when the same model can adapt to several domains depending on the input query. We propose the HyperDomainNet that is a hypernetwork that predicts our parameterization given the target domain. We empirically confirm that it can successfully learn a number of domains at once and may even generalize to unseen domains. Source code can be found at https://github.com/MACderRu/HyperDomainNet",,https://openreview.net/forum?id=MhpB7Rxyyr,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53467.png?t=1670152558.2285542,https://neurips.cc/virtual/2022/poster/53467,Not relevant,Defence,"['Domain adaptation', 'GANs', 'Hypernetwork', 'Fine-tuning', 'Regularization loss', 'Multi-domain adaptation']",['None'],Not relevant,Not relevant,,,,
2505,https://neurips.cc/virtual/2022/poster/53473,Debiased Causal Tree: Heterogeneous Treatment Effects Estimation with Unmeasured Confounding,Poster,NeurIPS,2022,"Unmeasured confounding poses a significant threat to the validity of causal inference. Despite that various ad hoc methods are developed to remove confounding effects, they are subject to certain fairly strong assumptions. In this work, we consider the estimation of conditional causal effects in the presence of unmeasured confounding using observational data and historical controls. Under an interpretable transportability condition, we prove the partial identifiability of conditional average treatment effect on the treated group (CATT). For tree-based models, a new notion, \emph{confounding entropy}, is proposed to measure the discrepancy introduced by unobserved confounders between the conditional outcome distribution of the treated and control groups. The confounding entropy generalizes conventional confounding bias, and can be estimated effectively using historical controls. We develop a new method, debiased causal tree, whose splitting rule is to minimize the empirical risk regularized by the confounding entropy. Notably, our method integrates current observational data (for empirical risk) and their historical controls (for confounding entropy) harmoniously.  We highlight that, debiased causal tree can not only estimate CATT well in the presence of unmeasured confounding, but also is a robust estimator of conditional average treatment effect (CATE) against the imbalance of the treated and control populations when all confounders are observed. An extension of combining multiple debiased causal trees to further reduce biases by gradient boosting is considered. The computational feasibility and statistical power of our method are evidenced by simulations and a study of a credit card balance dataset.",,https://openreview.net/forum?id=B26CPuYw9VA,https://neurips.cc/virtual/2022/poster/53473,https://neurips.cc/virtual/2022/poster/53473,Not relevant,Other aspects,"['causal inference', 'unmeasured confounding', 'conditional average treatment effect', 'tree-based models', 'observational data', 'historical controls', 'gradient boosting']",['credit card balance'],,,,,,
2513,https://neurips.cc/virtual/2022/poster/53464,Sound and Complete Verification of Polynomial Networks,Poster,NeurIPS,2022,"Polynomial Networks (PNs) have demonstrated promising performance on face and image recognition recently. However, robustness of PNs is unclear and thus obtaining certificates becomes imperative for enabling their adoption in real-world applications. Existing verification algorithms on ReLU neural networks (NNs) based on classical branch and bound (BaB) techniques cannot be trivially applied to PN verification. In this work, we devise a new bounding method, equipped with BaB for global convergence guarantees, called Verification of Polynomial Networks or VPN for short. One key insight is that we obtain much tighter bounds than the interval bound propagation (IBP) and DeepT-Fast [Bonaert et al., 2021] baselines. This enables sound and complete PN verification with empirical validation on MNIST, CIFAR10 and STL10 datasets. We believe our method has its own interest to NN verification. The source code is publicly available at https://github.com/megaelius/PNVerification.",,https://openreview.net/forum?id=gsdHDI-p6NI,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53464.png?t=1669041086.2087176,https://neurips.cc/virtual/2022/poster/53464,Evasion,Defence,"['Polynomial Networks', 'PNs', 'Robustness', 'Certificates', 'Verification', 'ReLU neural networks', 'NNs', 'Branch and bound', 'BaB', 'Interval bound propagation', 'IBP', 'DeepT-Fast', 'MNIST', 'CIFAR10', 'STL10']",['image recognition'],,,,,,
2544,https://neurips.cc/virtual/2022/poster/53358,The computational and learning benefits of Daleian neural networks,Poster,NeurIPS,2022,"Dale’s principle implies that biological neural networks are composed of neurons that are either excitatory or inhibitory. While the number of possible architectures of such Daleian networks is exponentially smaller than the number of non-Daleian ones, the computational and functional implications of using Daleian networks by the brain are mostly unknown. Here, we use models of recurrent spiking neural networks and rate-based ones to show, surprisingly, that despite the structural limitations on Daleian networks, they can approximate the computation performed by non-Daleian networks to a very high degree of accuracy. Moreover, we find that Daleian networks are more functionally robust to synaptic noise. We then show that unlike non-Daleian networks, Daleian ones can learn efficiently by tuning of single neuron features, nearly as well as learning by tuning individual synaptic weights. Importantly, this suggests a simpler and more biologically plausible learning mechanisms. We therefore suggest that in addition to architectural simplicity, Dale's principle confers computational and learning benefits for biological networks, and offer new directions for constructing and training biologically-inspired artificial neural networks.",,https://openreview.net/forum?id=ckQvYXizgd1,https://neurips.cc/virtual/2022/poster/53358,https://neurips.cc/virtual/2022/poster/53358,Not relevant,Other aspects,"[""Dale's principle"", 'neural networks', 'recurrent spiking neural networks', 'rate-based networks', 'synaptic noise', 'learning mechanisms', 'biologically-inspired artificial neural networks']",['Biological'],,,,,,
2547,https://neurips.cc/virtual/2022/poster/53310,Probing Classifiers are Unreliable for Concept Removal and Detection,Poster,NeurIPS,2022,"Neural network models trained on text data have been found to encode undesirable linguistic or sensitive concepts in their representation. Removing such concepts is non-trivial because of a complex relationship between the concept, text input, and the learnt representation. Recent work has proposed post-hoc and adversarial methods to remove such unwanted concepts from a model's representation. Through an extensive theoretical and empirical analysis, we show that these methods can be counter-productive: they are unable to remove the concepts entirely, and in the worst case may end up destroying all task-relevant features. The reason is the methods' reliance on a probing classifier as a proxy for the concept. Even under the most favorable conditions for learning a probing classifier when a concept's relevant features in representation space alone can provide 100% accuracy, we prove that a probing classifier is likely to use non-concept features and thus post-hoc or adversarial methods will fail to remove the concept correctly. These theoretical implications are confirmed by experiments on models trained on synthetic, Multi-NLI, and Twitter datasets. For sensitive applications of concept removal such as fairness, we recommend caution against using these methods and propose a spuriousness metric to gauge the quality of the final classifier.",,https://openreview.net/forum?id=nLGRGuzjtoR,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/0f6b1f657ac30ab76519ed4c677e9909.png?t=1666688764.0799496,https://neurips.cc/virtual/2022/poster/53310,Not relevant,Other aspects,"['Concept removal', 'Concept detection', 'Probing classifier', 'Adversarial methods', 'Task-relevant features', 'Representation space', 'Spuriousness metric']",['Fairness'],,,,,,
2563,https://neurips.cc/virtual/2022/poster/53323,Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch,Poster,NeurIPS,2022,"As the curation of data for machine learning becomes increasingly automated, dataset tampering is a mounting threat.  Backdoor attackers tamper with training data to embed a vulnerability in models that are trained on that data. This vulnerability is then activated at inference time by placing a ""trigger'' into the model's input. Typical backdoor attacks insert the trigger directly into the training data, although the presence of such an attack may be visible upon inspection. In contrast, the Hidden Trigger Backdoor Attack achieves poisoning without placing a trigger into the training data at all.   However, this hidden trigger attack is ineffective at poisoning neural networks trained from scratch.  We develop a new hidden trigger attack,  Sleeper Agent, which employs gradient matching, data selection, and target model re-training during the crafting process.  Sleeper Agent is the first hidden trigger backdoor attack to be effective against neural networks trained from scratch. We demonstrate its effectiveness on ImageNet and in black-box settings. Our implementation code can be found at: https://github.com/hsouri/Sleeper-Agent.",,https://openreview.net/forum?id=p0LJa6_XHM_,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53323.png?t=1669588507.1993735,https://neurips.cc/virtual/2022/poster/53323,Poisoning,Attack,"['Hidden trigger backdoors', 'Poisoning attack', 'Neural networks', 'Scalable', 'Training data tampering', 'Gradient matching', 'Data selection', 'Target model re-training']",['ImageNet'],Attack,Poisoning,,,,
2603,https://neurips.cc/virtual/2022/poster/53191,Learning-Augmented Algorithms for Online Linear and Semidefinite Programming,Poster,NeurIPS,2022,"Semidefinite programming (SDP) is a unifying framework that generalizes both linear programming and quadratically-constrained  quadratic programming, while also yielding efficient solvers, both in theory and in practice. However, there exist known impossibility results for approximating the optimal solution when constraints for covering SDPs arrive in an online fashion. In this paper, we study online covering linear and semidefinite programs in which the algorithm is augmented with advice from a possibly erroneous predictor. We show that if the predictor is accurate, we can efficiently bypass these impossibility results and achieve a constant-factor approximation to the optimal solution, i.e., consistency. On the other hand, if the predictor is inaccurate, under some technical conditions, we achieve results that match both the classical optimal upper bounds and the tight lower bounds up to constant factors, i.e., robustness. More broadly, we introduce a framework that extends both (1) the online set cover problem augmented with machine-learning predictors, studied by Bamas, Maggiori, and Svensson (NeurIPS 2020), and (2) the online covering SDP problem, initiated by Elad, Kale, and Naor (ICALP 2016).  Specifically, we obtain general online learning-augmented algorithms for covering linear programs with fractional advice and constraints, and initiate the study of learning-augmented algorithms for covering SDP problems. Our techniques are based on the primal-dual framework of Buchbinder and Naor (Mathematics of Operations Research, 34, 2009) and can be further adjusted to handle constraints where the variables lie in a bounded region, i.e., box constraints. ",,https://openreview.net/forum?id=KzC7Pejhp3z,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53191.png?t=1669527116.2439492,https://neurips.cc/virtual/2022/poster/53191,Not relevant,Defence,"['Semidefinite programming', 'Linear programming', 'Quadratically-constrained quadratic programming', 'Online', 'Augmented', 'Predictor', 'Approximating', 'Optimal solution', 'Constant-factor approximation', 'Consistency', 'Robustness', 'Online set cover problem', 'Machine-learning predictors', 'Online covering SDP problem', 'Primal-dual framework', 'Bounded region', 'Box constraints']",['None'],,,,,,
2607,https://neurips.cc/virtual/2022/poster/53181,Neural Attentive Circuits,Poster,NeurIPS,2022,"Recent work has seen the development of general purpose neural architectures that can be trained to perform tasks across diverse data modalities. General purpose models typically make few assumptions about the underlying data-structure and are known to perform well in the large-data regime. At the same time, there has been growing interest in modular neural architectures that represent the data using sparsely interacting modules. These models can be more robust out-of-distribution, computationally efficient, and capable of sample-efficient adaptation to new data. However, they tend to make domain-specific assumptions about the data, and present challenges in how module behavior (i.e., parameterization) and connectivity (i.e., their layout) can be jointly learned. In this work, we introduce a general purpose, yet modular neural architecture called Neural Attentive Circuits (NACs) that jointly learns the parameterization and a sparse connectivity of neural modules without using domain knowledge. NACs are best understood as the combination of two systems that are jointly trained end-to-end: one that determines the module configuration and the other that executes it on an input.  We demonstrate qualitatively that NACs learn diverse and meaningful module configurations on the Natural Language and Visual Reasoning for Real (NLVR2) dataset without additional supervision. Quantitatively, we show that by incorporating modularity in this way, NACs improve upon a strong non-modular baseline in terms of low-shot adaptation on CIFAR and Caltech-UCSD Birds dataset (CUB) by about 10 percent, and OOD robustness on Tiny ImageNet-R by about 2.5 percent. Further, we find that NACs can achieve an 8x speedup at inference time while losing less than 3 percent performance. Finally, we find NACs to yield competitive results on diverse data modalities spanning point-cloud classification, symbolic processing and text-classification from ASCII bytes, thereby confirming its general purpose nature. ",,https://openreview.net/forum?id=q41xK9Bunq1,https://neurips.cc/virtual/2022/poster/53181,https://neurips.cc/virtual/2022/poster/53181,Robustness,Defence,"['modular neural architectures', 'sparsely interacting modules', 'neural attentive circuits', 'parameterization', 'connectivity', 'end-to-end training', 'low-shot adaptation', 'OOD robustness', 'inference time', 'diverse data modalities', 'point-cloud classification', 'symbolic processing', 'text-classification']","['Natural Language', 'Visual Reasoning', 'CIFAR', 'Caltech-UCSD Birds', 'Tiny ImageNet-R']",,,,,,
2619,https://neurips.cc/virtual/2022/poster/53165,Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples,Poster,NeurIPS,2022,"Evaluating robustness of machine-learning models to adversarial examples is a challenging problem. Many defenses have been shown to provide a false sense of robustness by causing gradient-based attacks to fail, and they have been broken under more rigorous evaluations.Although guidelines and best practices have been suggested to improve current adversarial robustness evaluations, the lack of automatic testing and debugging tools makes it difficult to apply these recommendations in a systematic manner.In this work, we overcome these limitations by: (i) categorizing   attack failures based on how they affect the optimization of gradient-based attacks, while also  unveiling two novel failures affecting many popular attack implementations and past evaluations; (ii) proposing six novel \emph{indicators of failure}, to automatically detect the presence of such failures in the attack optimization process; and (iii) suggesting a systematic protocol to apply the corresponding fixes. Our extensive experimental analysis, involving more than 15 models in 3 distinct application domains, shows that our indicators of failure can be used to debug and improve current adversarial robustness evaluations, thereby providing a first concrete step towards automatizing and systematizing them. Our open-source code is available at: https://github.com/pralab/IndicatorsOfAttackFailure.",,https://openreview.net/forum?id=Y1sWzKW0k4L,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/eaf76caaba574ebf8e825f321c14ba29.png?t=1666441583.9777317,https://neurips.cc/virtual/2022/poster/53165,Evasion,Defence,"['Adversarial robustness evaluations', 'Gradient-based attacks', 'Machine-learning models', 'Attack failures', 'Optimization', 'Debugging', 'Indicators of failure']",['Application domains'],,,,,,
2624,https://neurips.cc/virtual/2022/poster/53115,Are Defenses for Graph Neural Networks Robust?,Poster,NeurIPS,2022,"A cursory reading of the literature suggests that we have made a lot of progress in designing effective adversarial defenses for Graph Neural Networks (GNNs). Yet, the standard methodology has a serious flaw – virtually all of the defenses are evaluated against non-adaptive attacks leading to overly optimistic robustness estimates. We perform a thorough robustness analysis of 7 of the most popular defenses spanning the entire spectrum of strategies, i.e., aimed at improving the graph, the architecture, or the training. The results are sobering – most defenses show no or only marginal improvement compared to an undefended baseline. We advocate using custom adaptive attacks as a gold standard and we outline the lessons we learned from successfully designing such attacks. Moreover, our diverse collection of perturbed graphs forms a (black-box) unit test offering a first glance at a model's robustness.",,https://openreview.net/forum?id=yCJVkELVT9d,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53115.png?t=1669725561.7336724,https://neurips.cc/virtual/2022/poster/53115,Evasion,Defence,"['Graph Neural Networks', 'Adversarial defenses', 'Robustness analysis', 'Custom adaptive attacks', 'Perturbed graphs', 'Non-adaptive attacks']",,Other aspects,Evasion,Both,,,Arguable
2625,https://neurips.cc/virtual/2022/poster/53110,Adversarial Robustness is at Odds with Lazy Training,Poster,NeurIPS,2022,"Recent works show that adversarial examples exist for random neural networks [Daniely and Schacham, 2020] and that these examples can be found using a single step of gradient ascent [Bubeck et al., 2021]. In this work, we extend this line of work to ``lazy training'' of neural networks -- a dominant model in deep learning theory in which neural networks are provably efficiently learnable. We show that over-parametrized neural networks that are guaranteed to generalize well and enjoy strong computational guarantees remain vulnerable to attacks generated using a single step of gradient ascent. ",,https://openreview.net/forum?id=bt25vx3aW_,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53110.png?t=1668969907.8796723,https://neurips.cc/media/neurips-2022/Slides/53110.pdf,Evasion,Defence,"['Adversarial robustness', 'Lazy training', 'Random neural networks', 'Gradient ascent', 'Over-parametrized neural networks', 'Generalization', 'Computational guarantees']",,,,,,,
2627,https://neurips.cc/virtual/2022/poster/53114,Trajectory balance: Improved credit assignment in GFlowNets,Poster,NeurIPS,2022,"Generative flow networks (GFlowNets) are a method for learning a stochastic policy for generating compositional objects, such as graphs or strings, from a given unnormalized density by sequences of actions, where many possible action sequences may lead to the same object. We find previously proposed learning objectives for GFlowNets, flow matching and detailed balance, which are analogous to temporal difference learning, to be prone to inefficient credit propagation across long action sequences. We thus propose a new learning objective for GFlowNets, trajectory balance, as a more efficient alternative to previously used objectives. We prove that any global minimizer of the trajectory balance objective can define a policy that samples exactly from the target distribution. In experiments on four distinct domains, we empirically demonstrate the benefits of the trajectory balance objective for GFlowNet convergence, diversity of generated samples, and robustness to long action sequences and large action spaces.  ",,https://openreview.net/forum?id=5btWTw1vcw1,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53114.png?t=1669225333.7066355,https://neurips.cc/media/neurips-2022/Slides/53114.pdf,Not relevant,Other aspects,"['Generative flow networks', 'GFlowNets', 'Stochastic policy', 'Compositional objects', 'Graphs', 'Strings', 'Unnormalized density', 'Action sequences', 'Credit assignment', 'Temporal difference learning', 'Trajectory balance', 'Efficient credit propagation']",None,,,,,,
2635,https://neurips.cc/virtual/2022/poster/53068,Environment Diversification with Multi-head Neural Network for Invariant Learning,Poster,NeurIPS,2022,"Neural networks are often trained with empirical risk minimization; however, it has been shown that a shift between training and testing distributions can cause unpredictable performance degradation. On this issue, a research direction, invariant learning, has been proposed to extract causal features insensitive to the distributional changes. This work proposes EDNIL, an invariant learning framework containing a multi-head neural network to absorb data biases. We show that this framework does not require prior knowledge about environments or strong assumptions about the pre-trained model. We also reveal that the proposed algorithm has theoretical connections to recent studies discussing properties of variant and invariant features. Finally, we demonstrate that models trained with EDNIL are empirically more robust against distributional shifts. ",,https://openreview.net/forum?id=FDmIo6o09H,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53068.png?t=1669120505.8706927,https://neurips.cc/virtual/2022/poster/53068,Robustness,Defence,"['invariant learning', 'environment diversification', 'multi-head neural network', 'distributional shifts', 'empirical risk minimization', 'causal features']",['None'],,,,,,
2645,https://neurips.cc/virtual/2022/poster/53049,On the symmetries of the synchronization problem in Cryo-EM: Multi-Frequency Vector Diffusion Maps on the Projective Plane,Poster,NeurIPS,2022,"Cryo-Electron Microscopy (Cryo-EM) is an important imaging method which allows high-resolution reconstruction of the 3D structures of biomolecules. It produces highly noisy 2D images by projecting a molecule's 3D density from random viewing directions. Because the projection directions are unknown, estimating the images' poses is necessary to perform the reconstruction. We focus on this task and study it under the group synchronization framework: if the relative poses of pairs of images can be approximated from the data, an estimation of the images' poses is given by the assignment which is most consistent with the relative ones.In particular, by studying the symmetries of cryo-EM, we show that relative poses in the group O(2) provide sufficient constraints to identify the images' poses, up to the molecule's chirality. With this in mind, we improve the existing multi-frequency vector diffusion maps (MFVDM) method: by using O(2) relative poses, our method not only predicts the similarity between the images' viewing directions but also recovers their poses. Hence, we can leverage all input images in a 3D reconstruction algorithm by initializing the poses with our estimation rather than just clustering and averaging the input images. We validate the recovery capabilities and robustness of our method on randomly generated synchronization graphs and a synthetic cryo-EM dataset.",,https://openreview.net/forum?id=owDcdLGgEm,https://neurips.cc/virtual/2022/poster/53049,https://neurips.cc/virtual/2022/poster/53049,Not relevant,Other aspects,"['Cryo-Electron Microscopy', '3D structures', 'biomolecules', 'noisy 2D images', 'projection directions', 'group synchronization framework', 'relative poses', 'image pose estimation', 'multi-frequency vector diffusion maps', 'MFVDM', 'O(2) relative poses']","['Cryo-EM', 'biomolecules', 'imaging method', '3D reconstruction algorithm']",,,,,,
2649,https://neurips.cc/virtual/2022/poster/53040,Robust Reinforcement Learning using Offline Data,Poster,NeurIPS,2022,"The  goal of robust reinforcement learning (RL)  is to learn a policy that is robust against the uncertainty in model parameters. Parameter uncertainty commonly occurs in many real-world RL applications due to  simulator modeling errors,  changes in the real-world system dynamics over time, and  adversarial disturbances. Robust RL is typically formulated as a max-min problem, where the objective is to learn the policy that maximizes the value  against the worst possible models that lie in an uncertainty set. In this work, we propose a  robust RL algorithm called Robust Fitted Q-Iteration (RFQI), which uses only an offline dataset to learn the optimal robust policy.  Robust RL with offline data is significantly more challenging than its non-robust counterpart because of the minimization over all models present in the robust Bellman operator. This poses challenges in offline data collection,  optimization over the models, and unbiased estimation. In this work, we propose a systematic approach to overcome these challenges, resulting in our RFQI algorithm. We prove that RFQI learns a near-optimal robust policy under standard assumptions and demonstrate its superior performance on standard benchmark problems.",,https://openreview.net/forum?id=AK6S9MZwM0,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/6dfa678a0fa26a0b36addfbc8fdc23e1.png?t=1667688428.595235,https://neurips.cc/virtual/2022/poster/53040,Robustness,Defence,"['robust reinforcement learning', 'RL', 'model parameters', 'uncertainty', 'Robust Fitted Q-Iteration', 'RFQI', 'offline data', 'minimization', 'Bellman operator', 'optimization', 'unbiased estimation']",['Reinforcement learning'],,,,,,
2653,https://neurips.cc/virtual/2022/poster/53015,Task Discovery: Finding the Tasks that Neural Networks Generalize on,Poster,NeurIPS,2022,"When developing deep learning models, we usually decide what task we want to solve then search for a model that generalizes well on the task. An intriguing question would be: what if, instead of fixing the task and searching in the model space, we fix the model and search in the task space? Can we find tasks that the model generalizes on? How do they look, or do they indicate anything? These are the questions we address in this paper. We propose a task discovery framework that automatically finds examples of such tasks via optimizing a generalization-based quantity called agreement score. We demonstrate that one set of images can give rise to many tasks on which neural networks generalize well. These tasks are a reflection of the inductive biases of the learning framework and the statistical patterns present in the data, thus they can make a useful tool for analyzing the neural networks and their biases. As an example, we show that the discovered tasks can be used to automatically create ''adversarial train-test splits'' which make a model fail at test time, without changing the pixels or labels, but by only selecting how the datapoints should be split between the train and test sets. We end with a discussion on human-interpretability of the discovered tasks.",,https://openreview.net/forum?id=hw-n6BUmiyI,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53015.png?t=1669286198.5083137,https://neurips.cc/virtual/2022/poster/53015,Not relevant,Other aspects,"['Task discovery', 'Generalization', 'Deep learning models', 'Inductive biases', 'Learning framework', 'Statistical patterns', 'Adversarial train-test splits', 'Human-interpretability']",[],,,,,,
2654,https://neurips.cc/virtual/2022/poster/53021,(De-)Randomized Smoothing for Decision Stump Ensembles,Poster,NeurIPS,2022,"Tree-based models are used in many high-stakes application domains such as ﬁnance and medicine, where robustness and interpretability are of utmost importance. Yet, methods for improving and certifying their robustness are severely under-explored, in contrast to those focusing on neural networks. Targeting this important challenge, we propose deterministic smoothing for decision stump ensembles. Whereas most prior work on randomized smoothing focuses on evaluating arbitrary base models approximately under input randomization, the key insight of our work is that decision stump ensembles enable exact yet efﬁcient evaluation via dynamic programming. Importantly, we obtain deterministic robustness certiﬁcates, even jointly over numerical and categorical features, a setting ubiquitous in the real world. Further, we derive an MLE-optimal training method for smoothed decision stumps under randomization and propose two boosting approaches to improve their provable robustness. An extensive experimental evaluation on computer vision and tabular data tasks shows that our approach yields signiﬁcantly higher certiﬁed accuracies than the state-of-the-art for tree-based models. We release all code and trained models at https://github.com/eth-sri/drs.",,https://openreview.net/forum?id=IbBHnPyjkco,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53021.png?t=1669406179.981383,https://neurips.cc/virtual/2022/poster/53021,Robustness,Defence,"['Tree-based models', 'Robustness', 'Interpretability', 'Decision stump ensembles', 'Dynamic programming', 'Certifying', 'Randomized smoothing', 'MLE-optimal training method', 'Boosting approaches', 'Certified accuracy', 'Computer vision', 'Tabular data']","['Finance', 'Medicine']",,,,,,
2671,https://neurips.cc/virtual/2022/poster/52979,Randomized Channel Shuffling: Minimal-Overhead Backdoor Attack Detection without Clean Datasets,Poster,NeurIPS,2022,"Deep neural networks (DNNs) typically require massive data to train on, which is a hurdle for numerous practical domains. Facing the data shortfall, one viable option is to acquire domain-specific training data from external uncensored sources, such as open webs or third-party data collectors. However, the quality of such acquired data is often not rigorously scrutinized, and one cannot easily rule out the risk of `""poisoned"" examples being included in such unreliable datasets, resulting in unreliable trained models which pose potential risks to many high-stake applications. While existing options usually suffer from high computational costs or assumptions on clean data access, this paper attempts to detect backdoors for potential victim models with minimal prior knowledge. In particular, provided with a trained model, users are assumed to (1) have no prior knowledge of whether it is already poisoned, or what the target class/percentage of samples is poisoned, and (2) have no access to a clean sample set from the same training distribution, nor any trusted model trained on such clean data. To tackle this challenging scenario, we first observe the contrasting channel-level statistics between the backdoor trigger and clean image features, and consequently, how they can be differentiated by progressive channel shuffling. We then propose the randomized channel shuffling method for backdoor-targeted class detection, which requires only a few feed-forward passes. It thus incurs minimal overheads and demands no clean sample nor prior knowledge. We further explore a “full” clean data-free setting, where neither the target class detection nor the trigger recovery can access the clean data. Extensive experiments are conducted with three datasets (CIFAR-10,  GTSRB, Tiny ImageNet), three architectures (AlexNet, ResNet-20, SENet-18), and three attacks (BadNets, clean label attack, and WaNet). Results consistently endorse the effectiveness of our proposed technique in backdoor model detection,  with margins of 0.291 ～ 0.640 AUROC over the current state-of-the-arts. Codes are available at https://github.com/VITA-Group/Random-Shuffling-BackdoorDetect.",,https://openreview.net/forum?id=TItRK4VP9X2,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/1f1a330a04265fcc56b37df4f9bc529c.png?t=1667450325.2538817,https://neurips.cc/virtual/2022/poster/52979,Poisoning,Defence,"['Deep neural networks', 'Backdoor detection', 'Poisoned data', 'Channel shuffling', 'Randomized channel shuffling', 'Minimal-overhead', 'Clean data-free setting', 'AUROC']","['CIFAR-10', 'GTSRB', 'Tiny ImageNet', 'AlexNet', 'ResNet-20', 'SENet-18', 'BadNets', 'Clean label attack', 'WaNet']",Defence,Poisoning,Other aspects,,,
2680,https://neurips.cc/virtual/2022/poster/52958,Lifting Weak Supervision To Structured Prediction,Poster,NeurIPS,2022,"Weak supervision (WS) is a rich set of techniques that produce pseudolabels by aggregating easily obtained but potentially noisy label estimates from various sources. WS is theoretically well-understood for binary classification, where simple approaches enable consistent estimation of pseudolabel noise rates. Using this result, it has been shown that downstream models trained on the pseudolabels have generalization guarantees nearly identical to those trained on clean labels. While this is exciting, users often wish to use WS for \emph{structured prediction}, where the output space consists of more than a binary or multi-class label set: e.g. rankings, graphs, manifolds, and more. Do the favorable theoretical properties of WS for binary classification lift to this setting? We answer this question in the affirmative for a wide range of scenarios. For labels taking values in a finite metric space, we introduce techniques new to weak supervision based on pseudo-Euclidean embeddings and tensor decompositions, providing a nearly-consistent noise rate estimator. For labels in constant-curvature Riemannian manifolds, we introduce new invariants that also yield consistent noise rate estimation. In both cases, when using the resulting pseudolabels in concert with a flexible downstream model, we obtain generalization guarantees nearly identical to those for models trained on clean data. Several of our results, which can be viewed as robustness guarantees in structured prediction with noisy labels, may be of independent interest.",,https://openreview.net/forum?id=Cntmos_Ndf0,https://neurips.cc/virtual/2022/poster/52958,https://neurips.cc/virtual/2022/poster/52958,Not relevant,Other aspects,"['Weak supervision', 'Structured prediction', 'Pseudolabels', 'Binary classification', 'Finite metric space', 'Pseudo-Euclidean embeddings', 'Tensor decompositions', 'Constant-curvature Riemannian manifolds', 'Invariants', 'Noisy labels', 'Robustness guarantees']",,,,,,,
2681,https://neurips.cc/virtual/2022/poster/52974,Concept Embedding Models,Poster,NeurIPS,2022,"Deploying AI-powered systems requires trustworthy models supporting effective human interactions, going beyond raw prediction accuracy. Concept bottleneck models promote trustworthiness by conditioning classification tasks on an intermediate level of human-like concepts. This enables human interventions which can correct mispredicted concepts to improve the model's performance. However, existing concept bottleneck models are unable to find optimal compromises between high task accuracy, robust concept-based explanations, and effective interventions on concepts---particularly in real-world conditions where complete and accurate concept supervisions are scarce. To address this, we propose Concept Embedding Models, a novel family of concept bottleneck models which goes beyond the current accuracy-vs-interpretability trade-off by learning interpretable high-dimensional concept representations. Our experiments demonstrate that Concept Embedding Models  (1) attain better or competitive task accuracy w.r.t. standard neural models without concepts, (2) provide concept representations capturing meaningful semantics including and beyond their ground truth labels, (3) support test-time concept interventions whose effect in test accuracy surpasses that in standard concept bottleneck models, and (4) scale to real-world conditions where complete concept supervisions are scarce.",,https://openreview.net/forum?id=HXCPA2GXf_,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/52974.png?t=1669730447.3153996,https://neurips.cc/virtual/2022/poster/52974,Not relevant,Other aspects,"['Concept bottleneck models', 'Intermediate level of human-like concepts', 'Concept Embedding Models', 'interpretable high-dimensional concept representations', 'task accuracy']",[],,,,,,
2686,https://neurips.cc/virtual/2022/poster/52929,Learning Concept Credible Models for Mitigating Shortcuts,Poster,NeurIPS,2022,"During training, models can exploit spurious correlations as shortcuts, resulting in poor generalization performance when shortcuts do not persist. In this work, assuming access to a representation based on domain knowledge (i.e., known concepts) that is invariant to shortcuts, we aim to learn robust and accurate models from biased training data. In contrast to previous work, we do not rely solely on known concepts, but allow the model to also learn unknown concepts. We propose two approaches for mitigating shortcuts that incorporate domain knowledge, while accounting for potentially important yet unknown concepts. The first approach is two-staged. After fitting a model using known concepts, it accounts for the residual using unknown concepts. While flexible, we show that this approach is vulnerable when shortcuts are correlated with the unknown concepts. This limitation is addressed by our second approach that extends a recently proposed regularization penalty. Applied to two real-world datasets, we demonstrate that both approaches can successfully mitigate shortcut learning.",,https://openreview.net/forum?id=yKYCwTvl8eU,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/52929.png?t=1668641514.3086438,https://neurips.cc/virtual/2022/poster/52929,Robustness,Defence,"['concept credible models', 'shortcuts', 'robust', 'accurate', 'biased training data', 'mitigating', 'domain knowledge']",['None'],,,,,,
2689,https://neurips.cc/virtual/2022/poster/52924,Marksman Backdoor: Backdoor Attacks with Arbitrary Target Class,Poster,NeurIPS,2022,"In recent years, machine learning models have been shown to be vulnerable to backdoor attacks. Under such attacks, an adversary embeds a stealthy backdoor into the trained model such that the compromised models will behave normally on clean inputs but will misclassify according to the adversary's control on maliciously constructed input with a trigger. While these existing attacks are very effective, the adversary's capability is limited: given an input, these attacks can only cause the model to misclassify toward a single pre-defined or target class. In contrast, this paper exploits a novel backdoor attack with a much more powerful payload, denoted as Marksman, where the adversary can arbitrarily choose which target class the model will misclassify given any input during inference. To achieve this goal, we propose to represent the trigger function as a class-conditional generative model and to inject the backdoor in a constrained optimization framework, where the trigger function learns to generate an optimal trigger pattern to attack any target class at will while simultaneously embedding this generative backdoor into the trained model. Given the learned trigger-generation function, during inference, the adversary can specify an arbitrary backdoor attack target class, and an appropriate trigger causing the model to classify toward this target class is created accordingly. We show empirically that the proposed framework achieves high attack performance (e.g., 100% attack success rates in several experiments) while preserving the clean-data performance in several benchmark datasets, including MNIST, CIFAR10, GTSRB, and TinyImageNet. The proposed Marksman backdoor attack can also easily bypass existing backdoor defenses that were originally designed against backdoor attacks with a single target class. Our work takes another significant step toward understanding the extensive risks of backdoor attacks in practice.",,https://openreview.net/forum?id=i-k6J4VkCDq,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/52924.png?t=1669816159.1167705,https://neurips.cc/media/neurips-2022/Slides/52924.pdf,Evasion,Attack,"['backdoor attack', 'target class', 'malicious input', 'trigger', 'class-conditional generative model', 'optimization framework', 'MNIST', 'CIFAR10', 'GTSRB', 'TinyImageNet']",['None'],Attack,Poisoning,,,Wrong,
2720,https://neurips.cc/virtual/2022/poster/52849,Robustness to Unbounded Smoothness of Generalized SignSGD,Poster,NeurIPS,2022,"Traditional analyses in non-convex optimization typically rely on the smoothness assumption, namely requiring the gradients to be Lipschitz. However, recent evidence shows that this smoothness condition does not capture the properties of some deep learning objective functions, including the ones involving Recurrent Neural Networks and LSTMs. Instead, they satisfy a much more relaxed condition, with potentially unbounded smoothness. Under this relaxed assumption, it has been theoretically and empirically shown that the gradient-clipped SGD has an advantage over the vanilla one. In this paper, we show that clipping is not indispensable for Adam-type algorithms in tackling such scenarios: we theoretically prove that a generalized SignSGD algorithm can obtain similar convergence rates as SGD with clipping but does not need explicit clipping at all. This family of algorithms on one end recovers SignSGD and on the other end closely resembles the popular Adam algorithm. Our analysis underlines the critical role that momentum plays in analyzing SignSGD-type and Adam-type algorithms: it not only reduces the effects of noise, thus removing the need for large mini-batch in previous analyses of SignSGD-type algorithms, but it also substantially reduces the effects of unbounded smoothness and gradient norms. To the best of our knowledge, this work is the first one showing the benefit of Adam-type algorithms compared with non-adaptive gradient algorithms such as gradient descent in the unbounded smoothness setting. We also compare these algorithms with popular optimizers on a set of deep learning tasks, observing that we can match the performance of Adam while beating others.",,https://openreview.net/forum?id=8oj_2Ypp0j,https://neurips.cc/virtual/2022/poster/52849,https://neurips.cc/virtual/2022/poster/52849,Robustness,Defence,"['Robustness', 'Smoothness', 'Generalized SignSGD', 'Non-convex optimization', 'Deep learning', 'Adam-type algorithms', 'Unbounded smoothness', 'Gradient descent', 'Momentum', 'Noise', 'Mini-batch']",['Deep learning tasks'],,,,,,
2724,https://neurips.cc/virtual/2022/poster/52826,Certifying Some Distributional Fairness with Subpopulation Decomposition,Poster,NeurIPS,2022,"Extensive efforts have been made to understand and improve the fairness of machine learning models based on observational metrics, especially in high-stakes domains such as medical insurance, education, and hiring decisions. However, there is a lack of certified fairness considering the end-to-end performance of an ML model. In this paper, we first formulate the certified fairness of an ML model trained on a given data distribution as an optimization problem based on the model performance loss bound on a fairness constrained distribution, which is within bounded distributional distance with the training distribution. We then propose a general fairness certification framework and instantiate it for both sensitive shifting and general shifting scenarios. In particular, we propose to solve the optimization problem by decomposing the original data distribution into analytical subpopulations and proving the convexity of the subproblems to solve them. We evaluate our certified fairness on six real-world datasets and show that our certification is tight in the sensitive shifting scenario and provides non-trivial certification under general shifting. Our framework is flexible to integrate additional non-skewness constraints and we show that it provides even tighter certification under different real-world scenarios. We also compare our certified fairness bound with adapted existing distributional robustness bounds on Gaussian data and demonstrate that our method is significantly tighter.",,https://openreview.net/forum?id=6mej19W1ppP,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/52826.png?t=1669659986.0194826,https://neurips.cc/virtual/2022/poster/52826,Robustness,Defence,"['Fairness', 'Machine Learning', 'Model Performance', 'Data Distribution', 'Subpopulation Decomposition', 'Optimization Problem', 'Certification Framework', 'Sensitive Shifting', 'General Shifting', 'Real-world datasets', 'Non-skewness constraints', 'Distributional robustness bounds', 'Gaussian data']","['Medical insurance', 'Education', 'Hiring decisions']",,,,,,
2725,https://neurips.cc/virtual/2022/poster/52842,Subgame Solving in Adversarial Team Games,Poster,NeurIPS,2022,"In adversarial team games, a team of players sequentially faces a team of adversaries. These games are the simplest setting with multiple players where cooperation and competition coexist, and it is known that the information asymmetry among the team members makes equilibrium approximation computationally hard. Although much effort has been spent designing scalable algorithms, the problem of solving large game instances is open. In this paper, we extend the successful approach of solving huge two-",,https://openreview.net/forum?id=Roiw2Trm-qP,https://neurips.cc/virtual/2022/poster/52842,https://neurips.cc/virtual/2022/poster/52842,Not relevant,Defence,"['Adversarial team games', 'Equilibrium approximation', 'Subgame solving', 'Multiple players', 'Cooperation', 'Competition', 'Information asymmetry']",[],Not relevant,Not relevant,,,,
2730,https://neurips.cc/virtual/2022/poster/52816,End-to-end Stochastic Optimization with Energy-based Model,Poster,NeurIPS,2022,"Decision-focused learning (DFL) was recently proposed for stochastic optimization problems that involve unknown parameters. By integrating predictive modeling with an implicitly differentiable optimization layer, DFL has shown superior performance to the standard two-stage predict-then-optimize pipeline. However, most existing DFL methods are only applicable to convex problems or a subset of nonconvex problems that can be easily relaxed to convex ones. Further, they can be inefficient in training due to the requirement of solving and differentiating through the optimization problem in every training iteration. We propose SO-EBM, a general and efficient DFL method for stochastic optimization using energy-based models. Instead of relying on KKT conditions to induce an implicit optimization layer, SO-EBM explicitly parameterizes the original optimization problem using a differentiable optimization layer based on energy functions. To better approximate the optimization landscape, we propose a coupled training objective that uses a maximum likelihood loss to capture the optimum location and a distribution-based regularizer to capture the overall energy landscape. Finally, we propose an efficient training procedure for SO-EBM with a self-normalized importance sampler based on a Gaussian mixture proposal. We evaluate SO-EBM in three applications: power scheduling, COVID-19 resource allocation, and non-convex adversarial security game, demonstrating the effectiveness and efficiency of SO-EBM.",,https://openreview.net/forum?id=_sYOodxTMcF,https://neurips.cc/virtual/2022/poster/52816,https://neurips.cc/virtual/2022/poster/52816,Not relevant,Defence,"['Stochastic optimization', 'Energy-based model', 'Decision-focused learning', 'Predictive modeling', 'Implicitly differentiable optimization layer', 'Training efficiency', 'KKT conditions', 'Maximum likelihood loss', 'Distribution-based regularizer', 'Self-normalized importance sampler', 'Gaussian mixture proposal', 'Power scheduling', 'COVID-19 resource allocation', 'Non-convex adversarial security game']","['Power scheduling', 'COVID-19 resource allocation']",Not relevant,Not relevant,,,,
2734,https://neurips.cc/virtual/2022/competition/50094,"AutoML Decathlon: Diverse Tasks, Modern Methods, and Efficiency at Scale",Competition,NeurIPS,2022,"As more areas beyond the traditional AI domains (e.g., computer vision and natural language processing) seek to take advantage of data-driven tools, the need for developing ML systems that can adapt to a wide range of downstream tasks in an efficient and automatic way continues to grow. The AutoML for the 2020s competition aims to catalyze research in this area and establish a benchmark for the current state of automated machine learning. Unlike previous challenges which focus on a single class of methods such as non-deep-learning AutoML, hyperparameter optimization, or meta-learning, this competition proposes to (1) evaluate automation on a diverse set of small and large-scale tasks, and (2) allow the incorporation of the latest methods such as neural architecture search and unsupervised pretraining. To this end, we curate 20 datasets that represent a broad spectrum of practical applications in scientific, technological, and industrial domains. Participants are given a set of 10 development tasks selected from these datasets and are required to come up with automated programs that perform well on as many problems as possible and generalize to the remaining private test tasks. To ensure efficiency, the evaluation will be conducted under a fixed computational budget. To ensure robustness, the performance profiles methodology is used for determining the winners. The organizers will provide computational resources to the participants as needed and monetary prizes to the winners.",,,https://neurips.cc/virtual/2022/competition/50094,https://neurips.cc/virtual/2022/competition/50094,Not relevant,Other aspects,"['AutoML', 'Diverse Tasks', 'Modern Methods', 'Efficiency', 'Scale', '2020s competition', 'benchmark', 'automated machine learning', 'non-deep-learning AutoML', 'hyperparameter optimization', 'meta-learning', 'neural architecture search', 'unsupervised pretraining', 'practical applications', 'scientific', 'technological', 'industrial domains', 'computational budget', 'performance profiles methodology', 'monetary prizes']","['scientific', 'technological', 'industrial']",,,,,,
2737,https://neurips.cc/virtual/2022/competition/50100,The Trojan Detection Challenge,Competition,NeurIPS,2022,"A growing concern for the security of ML systems is the possibility for Trojan attacks on neural networks. There is now considerable literature for methods detecting these attacks. We propose the Trojan Detection Challenge to further the community's understanding of methods to construct and detect Trojans. This competition will consist of complimentary tracks on detecting/analyzing Trojans and creating evasive Trojans. Participants will be tasked with devising methods to better detect Trojans using a new dataset containing over 6,000 neural networks. Code and evaluations from three established baseline detectors will provide a starting point, and a novel Minimal Trojan attack will challenge participants to push the state-of-the-art in Trojan detection. At the end of the day, we hope our competition spurs practical innovations and clarifies deep questions surrounding the offense-defense balance of Trojan attacks.",,,https://neurips.cc/virtual/2022/competition/50100,https://neurips.cc/virtual/2022/competition/50100,Evasion,Defence,"['Trojan detection', 'Trojan attacks', 'Neural networks', 'Security', 'ML systems', 'Offense-defense balance']",['Trojans'],Other aspects,Poisoning,Both,,Arguable,Wrong
2747,https://neurips.cc/virtual/2022/competition/50082,Cross-Domain MetaDL: Any-Way Any-Shot Learning Competition with Novel Datasets from Practical Domains,Competition,NeurIPS,2022,"Meta-learning aims to leverage the experience from previous tasks to solve new tasks using only little training data, train faster and/or get better performance. The proposed challenge focuses on ""cross-domain meta-learning"" for few-shot image classification using a novel ""any-way"" and ""any-shot"" setting. The goal is to meta-learn a good model that can quickly learn tasks from a variety of domains, with any number of classes also called ""ways"" (within the range 2-20) and any number of training examples per class also called ""shots"" (within the range 1-20). We carve such tasks from various ""mother datasets"" selected from diverse domains, such as healthcare, ecology, biology, manufacturing, and others. By using mother datasets from these practical domains, we aim to maximize the humanitarian and societal impact. The competition is with code submission, fully blind-tested on the CodaLab challenge platform. A single (final) submission will be evaluated during the final phase, using ten datasets previously unused by the meta-learning community. After the competition is over, it will remain active to be used as a long-lasting benchmark resource for research in this field. The scientific and technical motivations of this challenge include scalability, robustness to domain changes, and generalization ability to tasks (a.k.a. episodes) in different regimes (any-way any-shot).",,,https://neurips.cc/virtual/2022/competition/50082,https://neurips.cc/virtual/2022/competition/50082,Not relevant,Defence,"['Cross-domain meta-learning', 'few-shot image classification', 'any-way any-shot setting', 'meta-learn', 'practical domains', 'scalability', 'robustness to domain changes', 'generalization ability']","['healthcare', 'ecology', 'biology', 'manufacturing']",,,,,,
2748,https://neurips.cc/virtual/2022/poster/52801,Autoregressive Perturbations for Data Poisoning,Poster,NeurIPS,2022,"The prevalence of data scraping from social media as a means to obtain datasets has led to growing concerns regarding unauthorized use of data. Data poisoning attacks have been proposed as a bulwark against scraping, as they make data ``unlearnable'' by adding small, imperceptible perturbations. Unfortunately, existing methods require knowledge of both the target architecture and the complete dataset so that a surrogate network can be trained, the parameters of which are used to generate the attack. In this work, we introduce autoregressive (AR) poisoning, a method that can generate poisoned data without access to the broader dataset. The proposed AR perturbations are generic, can be applied across different datasets, and can poison different architectures. Compared to existing unlearnable methods, our AR poisons are more resistant against common defenses such as adversarial training and strong data augmentations. Our analysis further provides insight into what makes an effective data poison. ",,https://openreview.net/forum?id=1vusesyN7E,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/5a1106fcb6c23317695f2f619988ef41.png?t=1667845510.4506493,https://neurips.cc/virtual/2022/poster/52801,Poisoning,Attack,"['Autoregressive', 'Data Poisoning', 'Unlearnable', 'Adversarial training', 'Data augmentations']",,Attack,Poisoning,Other aspects,,,
2750,https://neurips.cc/virtual/2022/poster/52795,ELIGN: Expectation Alignment as a Multi-Agent Intrinsic Reward,Poster,NeurIPS,2022,"Modern multi-agent reinforcement learning frameworks rely on centralized training and reward shaping to perform well. However, centralized training and dense rewards are not readily available in the real world. Current multi-agent algorithms struggle to learn in the alternative setup of decentralized training or sparse rewards. To address these issues, we propose a self-supervised intrinsic reward  \textit{ELIGN - expectation alignment - } inspired by the self-organization principle in Zoology. Similar to how animals collaborate in a decentralized manner with those in their vicinity, agents trained with expectation alignment learn behaviors that match their neighbors' expectations. This allows the agents to learn collaborative behaviors without any external reward or centralized training. We demonstrate the efficacy of our approach across 6 tasks in the multi-agent particle and the complex Google Research football environments, comparing ELIGN to sparse and curiosity-based intrinsic rewards. When the number of agents increases, ELIGN scales well in all multi-agent tasks except for one where agents have different capabilities. We show that agent coordination improves through expectation alignment because agents learn to divide tasks amongst themselves, break coordination symmetries, and confuse adversaries. These results identify tasks where expectation alignment is a more useful strategy than curiosity-driven exploration for multi-agent coordination, enabling agents to do zero-shot coordination.",,https://openreview.net/forum?id=uPyNR2yPoe,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/52795.png?t=1669591083.4553216,https://neurips.cc/virtual/2022/poster/52795,Not relevant,Defence,"['multi-agent reinforcement learning', 'centralized training', 'reward shaping', 'decentralized training', 'sparse rewards', 'self-supervised intrinsic reward', 'expectation alignment', 'animal collaboration', 'agent coordination', 'zero-shot coordination']","['multi-agent particle', 'Google Research football']",Not relevant,Not relevant,,,,
2765,https://neurips.cc/virtual/2022/poster/52992,Safety Guarantees for Neural Network Dynamic Systems via Stochastic Barrier Functions,Poster,NeurIPS,2022,"Neural Networks (NNs) have been successfully employed to represent the state evolution of complex dynamical systems.  Such models, referred to as NN dynamic models (NNDMs), use iterative noisy predictions of NN to estimate a distribution of system trajectories over time. Despite their accuracy, safety analysis of NNDMs is known to be a challenging problem and remains largely unexplored.  To address this issue, in this paper, we introduce a method of providing safety guarantees for NNDMs.  Our approach is based on stochastic barrier functions, whose relation with safety are analogous to that of Lyapunov functions with stability.  We first show a method of synthesizing stochastic barrier functions for NNDMs via a convex optimization problem, which in turn provides a lower bound on the system's safety probability.  A key step in our method is the employment of the recent convex approximation results for NNs to find piece-wise linear bounds, which allow the formulation of the barrier function synthesis problem as a sum-of-squares optimization program.  If the obtained safety probability is above the desired threshold, the system is certified.  Otherwise, we introduce a method of generating controls for the system that robustly minimize the unsafety probability in a minimally-invasive manner.  We exploit the convexity property of the barrier function to formulate the optimal control synthesis problem as a linear program.  Experimental results illustrate the efficacy of the method. Namely, they show that the method can scale to multi-dimensional NNDMs with multiple layers and hundreds of neurons per layer, and that the controller can significantly improve the safety probability.",,https://openreview.net/forum?id=9XQa6cgLo21,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/52992.png?t=1669271731.3596401,https://neurips.cc/virtual/2022/poster/52992,Robustness,Defence,"['Neural Networks', 'Dynamic Systems', 'Stochastic Barrier Functions', 'Safety Analysis', 'Convex Optimization', 'Lyapunov functions', 'Stability', 'Certification', 'Control Synthesis', 'Experimental Results']",['None'],,,,,,
2773,https://neurips.cc/virtual/2022/poster/54618,Where to Pay Attention in Sparse Training for Feature Selection?,Poster,NeurIPS,2022,"A new line of research for feature selection based on neural networks has recently emerged. Despite its superiority to classical methods, it requires many training iterations to converge and detect the informative features. For datasets with a large number of samples or a very high dimensional feature space, the computational time becomes prohibitively long. In this paper, we present a new efficient unsupervised method for feature selection based on sparse autoencoders. In particular, we propose a new sparse training algorithm that optimizes a model's sparse topology during training to quickly pay attention to informative features. The attention-based adaptation of the sparse topology enables fast detection of informative features after a few training iterations. We performed extensive experiments on 10 datasets of different types, including image, speech, text, artificial, and biological. They cover a wide range of characteristics, such as low and high-dimensional feature spaces, as well as few and large training samples. Our proposed approach outperforms the state-of-the-art methods in terms of the selection of informative features while reducing training iterations and computational costs substantially. Moreover, the experiments show the robustness of our method in extremely noisy environments.",,https://openreview.net/forum?id=xWvI9z37Xd,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54618.png?t=1669602642.4053397,https://neurips.cc/virtual/2022/poster/54618,Not relevant,Defence,"['feature selection', 'neural networks', 'sparse autoencoders', 'sparse training', 'informative features', 'computational time', 'unsupervised method']","['image', 'speech', 'text', 'artificial', 'biological']",,,,,,
2774,https://neurips.cc/virtual/2022/poster/53029,Contrastive Adapters for Foundation Model Group Robustness,Poster,NeurIPS,2022,"While large pretrained foundation models (FMs) have shown remarkable zero-shot classification robustness to dataset-level distribution shifts, their robustness to subpopulation or group shifts is relatively underexplored. We study this problem, and find that foundation models such as CLIP may not be robust to various group shifts. Across 9 robustness benchmarks, zero-shot classification with their embeddings results in gaps of up to 80.7 percentage points (pp) between average and worst-group accuracy. Unfortunately, existing methods to improve robustness require retraining, which can be prohibitively expensive on large foundation models. We also find that efficient ways to improve model inference (e.g. via adapters, lightweight networks that transform FM embeddings) do not consistently improve and can sometimes ",,https://openreview.net/forum?id=uPdS_7pdA9p,https://neurips.cc/virtual/2022/poster/53029,https://neurips.cc/virtual/2022/poster/53029,Robustness,Defence,"['Foundation Model', 'Robustness', 'Pretrained', 'Group shifts', 'Adapters', 'Zero-shot classification']",['None'],,,,,,
2781,https://neurips.cc/virtual/2022/poster/54675,On the Limitations of Stochastic Pre-processing Defenses,Poster,NeurIPS,2022,"Defending against adversarial examples remains an open problem. A common belief is that randomness at inference increases the cost of finding adversarial inputs. An example of such a defense is to apply a random transformation to inputs prior to feeding them to the model. In this paper, we empirically and theoretically investigate such stochastic pre-processing defenses and demonstrate that they are flawed. First, we show that most stochastic defenses are weaker than previously thought; they lack sufficient randomness to withstand even standard attacks like projected gradient descent. This casts doubt on a long-held assumption that stochastic defenses invalidate attacks designed to evade deterministic defenses and force attackers to integrate the Expectation over Transformation (EOT) concept. Second, we show that stochastic defenses confront a trade-off between adversarial robustness and model invariance; they become less effective as the defended model acquires more invariance to their randomization. Future work will need to decouple these two effects. We also discuss implications and guidance for future research.",,https://openreview.net/forum?id=P_eBjUlzlV,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54675.png?t=1669528081.0696576,https://neurips.cc/media/neurips-2022/Slides/54675.pdf,Evasion,Defence,"['Stochastic pre-processing defenses', 'Adversarial examples', 'randomization', 'projected gradient descent', 'Expectation over Transformation', 'adversarial robustness', 'model invariance']",['None'],Defence,Evasion,Other aspects,Robustness,,
2802,https://neurips.cc/virtual/2022/poster/54524,Augmentations in Hypergraph Contrastive Learning: Fabricated and Generative,Poster,NeurIPS,2022,"This paper targets at improving the generalizability of hypergraph neural networks in the low-label regime, through applying the contrastive learning approach from images/graphs (we refer to it as HyperGCL). We focus on the following question: How to construct contrastive views for hypergraphs via augmentations? We provide the solutions in two folds. First, guided by domain knowledge, we fabricate two schemes to augment hyperedges with higher-order relations encoded, and adopt three vertex augmentation strategies from graph-structured data. Second, in search of more effective views in a data-driven manner, we for the first time propose a hypergraph generative model to  generate augmented views, and then an end-to-end differentiable pipeline to jointly learn hypergraph augmentations and model parameters. Our technical innovations are reflected in designing both fabricated and generative augmentations of hypergraphs. The experimental findings include: (i) Among fabricated augmentations in HyperGCL, augmenting hyperedges provides the most numerical gains, implying that higher-order information in structures is usually more downstream-relevant; (ii) Generative augmentations do better in preserving higher-order information to further benefit generalizability; (iii) HyperGCL also boosts robustness and fairness in hypergraph representation learning. Codes are released at https://github.com/weitianxin/HyperGCL.",,https://openreview.net/forum?id=igMc_C9pgYG,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54524.png?t=1669324864.2286983,https://neurips.cc/media/neurips-2022/Slides/54524.pdf,Not relevant,Defence,"['Adversarial examples', 'Stochastic pre-processing defenses', 'Randomness', 'Inference', 'Projected gradient descent', 'Expectation over Transformation', 'Adversarial robustness', 'Model invariance', 'Hypergraph neural networks', 'Hypergraph', 'Contrastive learning', 'Generalizability', 'Low-label regime', 'HyperGCL', 'Augmentations', 'Hypertrees', 'Higher-order relations', 'Vertex augmentation', 'Graph-structured data', 'Hypergraph generative model', 'End-to-end differentiable pipeline', 'Fabricated and Generative']",['Hypergraph representation learning'],,,,,,
2813,https://neurips.cc/virtual/2022/poster/52809,Empirical Gateaux Derivatives for Causal Inference,Poster,NeurIPS,2022,"We study a constructive procedure that approximates Gateaux derivatives for statistical functionals by finite-differencing, with attention to causal inference functionals. We focus on the case where probability distributions are not known a priori but need also to be estimated from data, leading to empirical Gateaux derivatives, and study relationships between empirical, numerical, and analytical Gateaux derivatives. Starting with a case study of counterfactual mean estimation, we verify the exact relationship between finite-differences and the analytical Gateaux derivative. We then derive requirements on the rates of numerical approximation in perturbation and smoothing that preserve statistical benefits. We study more complicated functionals such as dynamic treatment regimes and the linear-programming formulation for policy optimization infinite-horizon Markov decision processes. In the case of the latter, this approach can be used to approximate bias adjustments in the presence of arbitrary constraints, illustrating the usefulness of constructive approaches for Gateaux derivatives. We find that, omitting unfavorable dimension dependence of smoothing, although rate-double robustness permits for coarser rates of perturbation size than implied by generic approximation analysis of finite-differences for the case of the counterfactual mean, this is not the case for the infinite-horizon MDP policy value. ",,https://openreview.net/forum?id=8gUjpEsLCU,https://neurips.cc/virtual/2022/poster/52809,https://neurips.cc/media/neurips-2022/Slides/52809.pdf,Not relevant,Defence,"['Gateaux derivatives', 'causal inference', 'empirical', 'finite-differencing', 'counterfactual mean estimation', 'dynamic treatment regimes', 'linear-programming formulation', 'policy optimization', 'infinite-horizon Markov decision processes', 'bias adjustments']",['None'],,,,,,
2820,https://neurips.cc/virtual/2022/poster/53646,Gaussian Copula Embeddings,Poster,NeurIPS,2022,"Learning latent vector representations via embedding models has been shown promising in machine learning. However, most of the embedding models are still limited to a single type of observation data. We propose a Gaussian copula embedding model to learn latent vector representations of items in a heterogeneous data setting. The proposed model can effectively incorporate different types of observed data and, at the same time, yield robust embeddings. We demonstrate the proposed model can effectively learn in many different scenarios, outperforming competing models in modeling quality and task performance.",,https://openreview.net/forum?id=Fh9l_pVsBfv,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53646.png?t=1669191405.9402354,https://neurips.cc/virtual/2022/poster/53646,Not relevant,Defence,"['Embedding models', 'Latent vector representations', 'Heterogeneous data', 'Gaussian copula', 'Robust embeddings']",['None'],,,,,,
2830,https://neurips.cc/virtual/2022/poster/55047,RAMBO-RL: Robust Adversarial Model-Based Offline Reinforcement Learning,Poster,NeurIPS,2022,"Offline reinforcement learning (RL) aims to find performant policies from logged data without further environment interaction. Model-based algorithms, which learn a model of the environment from the dataset and perform conservative policy optimisation within that model, have emerged as a promising approach to this problem. In this work, we present Robust Adversarial Model-Based Offline RL (RAMBO), a novel approach to model-based offline RL. We formulate the problem as a two-player zero sum game against an adversarial environment model. The model is trained to minimise the value function while still accurately predicting the transitions in the dataset, forcing the policy to act conservatively in areas not covered by the dataset. To approximately solve the two-player game, we alternate between optimising the policy and adversarially optimising the model. The problem formulation that we address is theoretically grounded, resulting in a probably approximately correct (PAC) performance guarantee and a pessimistic value function which lower bounds the value function in the true environment. We evaluate our approach on widely studied offline RL benchmarks, and demonstrate that it outperforms existing state-of-the-art baselines.",,https://openreview.net/forum?id=nrksGSRT7kX,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55047.png?t=1668423028.809457,https://neurips.cc/virtual/2022/poster/55047,Not relevant,Defence,"['Adversarial', 'Reinforcement Learning', 'Robustness', 'Model-based', 'Offline', 'Value function', 'Environment model', 'PAC performance guarantee']",[],,,,,,
2833,https://neurips.cc/virtual/2022/poster/54168,Distributionally robust weighted k-nearest neighbors,Poster,NeurIPS,2022,"Learning a robust classifier from a few samples remains a key challenge in machine learning. A major thrust of research has been focused on developing k-nearest neighbor (k-NN) based algorithms combined with metric learning that captures similarities between samples. When the samples are limited, robustness is especially crucial to ensure the generalization capability of the classifier. In this paper, we study a minimax distributionally robust formulation of weighted k-nearest neighbors, which aims to find the optimal weighted k-NN classifiers that hedge against feature uncertainties. We develop an algorithm, Dr.k-NN, that efficiently solves this functional optimization problem and features in assigning minimax optimal weights to training samples when performing classification. These weights are class-dependent, and are determined by the similarities of sample features under the least favorable scenarios. When the size of the uncertainty set is properly tuned, the robust classifier has a smaller Lipschitz norm than the vanilla k-NN, and thus improves the generalization capability. We also couple our framework with neural-network-based feature embedding. We demonstrate the competitive performance of our algorithm compared to the state-of-the-art in the few-training-sample setting with various real-data experiments.",,https://openreview.net/forum?id=Yg2CRGUln5k,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/ce5193a069bea027a60e06c57a106eb6.png?t=1667098347.28517,https://neurips.cc/virtual/2022/poster/54168,Robustness,Defence,"['distributionally robust', 'weighted k-nearest neighbors', 'machine learning', 'robust classifier', 'metric learning', 'minimax', 'functional optimization', 'Lipschitz norm', 'feature embedding', 'few-training-sample setting']",['real-data experiments'],,,,,,
2837,https://neurips.cc/virtual/2022/poster/53144,On the Adversarial Robustness of Mixture of Experts,Poster,NeurIPS,2022,"Adversarial robustness is a key desirable property of neural networks. It has been empirically shown to be affected by their sizes, with larger networks being typically more robust. Recently, \citet{bubeck2021universal} proved a lower bound on the Lipschitz constant of functions that fit the training data in terms of their number of parameters. This raises an interesting open question, do---and can---functions with more parameters, but not necessarily more computational cost, have better robustness? We study this question for sparse Mixture of Expert models (MoEs), that make it possible to scale up the model size for a roughly constant computational cost. We theoretically show that under certain conditions on the routing and the structure of the data, MoEs can have significantly smaller Lipschitz constants than their dense counterparts. The robustness of MoEs can suffer when the highest weighted experts for an input implement sufficiently different functions. We next empirically evaluate the robustness of MoEs on ImageNet using adversarial attacks and show they are indeed more robust than dense models with the same computational cost. We make key observations showing the robustness of MoEs to the choice of experts, highlighting the redundancy of experts in models trained in practice.",,https://openreview.net/forum?id=Fd05J4Bu5Sp,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53144.png?t=1669048722.3524344,https://neurips.cc/virtual/2022/poster/53144,Evasion,Defence,"['Adversarial robustness', 'Neural networks', 'Lipschitz constant', 'Mixture of Experts', 'Sparse models', 'ImageNet', 'Adversarial attacks']",['ImageNet'],Defence,Evasion,Other aspects,,,
2839,https://neurips.cc/virtual/2022/poster/55002,BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework,Poster,NeurIPS,2022,"Fusing the camera and LiDAR information has become a de-facto standard for 3D object detection tasks. Current methods rely on point clouds from the LiDAR sensor as queries to leverage the feature from the image space. However, people discovered that this underlying assumption makes the current fusion framework infeasible to produce any prediction when there is a LiDAR malfunction, regardless of minor or major. This fundamentally limits the deployment capability to realistic autonomous driving scenarios. In contrast, we propose a surprisingly simple yet novel fusion framework, dubbed BEVFusion, whose camera stream does not depend on the input of LiDAR data, thus addressing the downside of previous methods. We empirically show that our framework surpasses the state-of-the-art methods under the normal training settings. Under the robustness training settings that simulate various LiDAR malfunctions, our framework significantly surpasses the state-of-the-art methods by 15.7% to 28.9% mAP. To the best of our knowledge, we are the first to handle realistic LiDAR malfunction and can be deployed to realistic scenarios without any post-processing procedure. ",,https://openreview.net/forum?id=zzDrPqn57DL,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55002.png?t=1669716385.7864528,https://neurips.cc/media/neurips-2022/Slides/55002.pdf,Robustness,Defence,"['LiDAR-Camera Fusion', '3D object detection', 'LiDAR malfunction', 'autonomous driving', 'BEVFusion', 'robustness training']",['autonomous driving'],,,,,,
2841,https://neurips.cc/virtual/2022/poster/54329,Retrospective Adversarial Replay for Continual Learning,Poster,NeurIPS,2022,"Continual learning is an emerging research challenge in machine learning that addresses the problem where models quickly fit the most recently trained-on data but suffer from catastrophic forgetting of previous data due to distribution shifts --- it does this by maintaining a small historical replay buffer in replay-based methods. To avoid these problems, this paper proposes a method, ``Retrospective Adversarial Replay (RAR)'', that synthesizes adversarial samples near the forgetting boundary. RAR perturbs a buffered sample towards its nearest neighbor drawn from the current task in a latent representation space. By replaying such samples, we are able to refine the boundary between previous and current tasks, hence combating forgetting and reducing bias towards the current task. To mitigate the severity of a small replay buffer, we develop a novel MixUp-based strategy to increase replay variation by replaying mixed augmentations. Combined with RAR, this achieves a holistic framework that helps to alleviate catastrophic forgetting. We show that this excels on broadly-used benchmarks and outperforms other continual learning baselines especially when only a small buffer is available. We conduct a thorough ablation study over each key component as well as a hyperparameter sensitivity analysis to demonstrate the effectiveness and robustness of RAR.",,https://openreview.net/forum?id=XEoih0EwCwL,https://neurips.cc/virtual/2022/poster/54329,https://neurips.cc/virtual/2022/poster/54329,Robustness,Defence,"['Continual Learning', 'Retrospective Adversarial Replay', 'Adversarial Samples', 'Forgetting Boundary', 'Latent Representation', 'MixUp-based strategy', 'Hyperparameter Sensitivity Analysis']",,,,,,,
2843,https://neurips.cc/virtual/2022/poster/52905,Adversarial Auto-Augment with Label Preservation: A Representation Learning Principle Guided Approach,Poster,NeurIPS,2022,"Data augmentation is a critical contributing factor to the success of deep learning but heavily relies on prior domain knowledge which is not always available. Recent works on automatic data augmentation learn a policy to form a sequence of augmentation operations, which are still pre-defined and restricted to limited options. In this paper, we show that a prior-free autonomous data augmentation's objective can be derived from a representation learning principle that aims to preserve the minimum sufficient information of the labels. Given an example, the objective aims at creating a distant ``hard positive example'' as the augmentation, while still preserving the original label. We then propose a practical surrogate to the objective that can be optimized efficiently and integrated seamlessly into existing methods for a broad class of machine learning tasks, e.g., supervised, semi-supervised, and noisy-label learning. Unlike previous works, our method does not require training an extra generative model but instead leverages the intermediate layer representations of the end-task model for generating data augmentations. In experiments, we show that our method consistently brings non-trivial improvements to the three aforementioned learning tasks from both efficiency and final performance, either or not combined with pre-defined augmentations, e.g., on medical images when domain knowledge is unavailable and the existing augmentation techniques perform poorly. Code will be released publicly.",,https://openreview.net/forum?id=yJV9zp5OKAY,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/52905.png?t=1669207649.5883284,https://neurips.cc/virtual/2022/poster/52905,Not relevant,Defence,"['Adversarial Auto-Augment', 'Label Preservation', 'Representation Learning Principle', 'Autonomous Data Augmentation', 'Data Augmentation', 'Deep Learning']",['Medical Images'],,,,,,
2844,https://neurips.cc/virtual/2022/poster/54642,Towards Hard-pose Virtual Try-on via 3D-aware Global Correspondence Learning,Poster,NeurIPS,2022,"In this paper, we target image-based person-to-person virtual try-on in the presence of diverse poses and large viewpoint variations. Existing methods are restricted in this setting as they estimate garment warping flows mainly based on 2D poses and appearance, which omits the geometric prior of the 3D human body shape.Moreover, current garment warping methods are confined to localized regions, which makes them ineffective in capturing long-range dependencies and results in inferior flows with artifacts.To tackle these issues, we present 3D-aware global correspondences, which are reliable flows that jointly encode global semantic correlations, local deformations, and geometric priors of 3D human bodies. Particularly, given an image pair depicting the source and target person, (a) we first obtain their pose-aware and high-level representations via two encoders, and introduce a coarse-to-fine decoder with multiple refinement modules to predict the pixel-wise global correspondence. (b) 3D parametric human models inferred from images are incorporated as priors to regularize the correspondence refinement process so that our flows can be 3D-aware and better handle variations of pose and viewpoint. (c) Finally, an adversarial generator takes the garment warped by the 3D-aware flow, and the image of the target person as inputs, to synthesize the photo-realistic try-on result. Extensive experiments on public benchmarks and our selected HardPose test set demonstrate the superiority of our method against state-of-the-art try-on approaches.",,https://openreview.net/forum?id=5Fg3XoHjQ4r,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54642.png?t=1669000196.6905503,https://neurips.cc/virtual/2022/poster/54642,Not relevant,Defence,"['Hard-pose virtual try-on', '3D-aware global correspondence learning', 'garment warping flows', '2D poses and appearance', 'geometric prior of the 3D human body shape', 'long-range dependencies', '3D human bodies', 'encoders', 'decoder', 'refinement modules', '3D parametric human models', 'adversarial generator', 'photo-realistic try-on result', 'public benchmarks', 'HardPose test set']",['image-based person-to-person virtual try-on'],,,,,,
2848,https://neurips.cc/virtual/2022/poster/54501,Class-Aware Adversarial Transformers for Medical Image Segmentation,Poster,NeurIPS,2022,"Transformers have made remarkable progress towards modeling long-range dependencies within the medical image analysis domain. However, current transformer-based models suffer from several disadvantages: (1) existing methods fail to capture the important features of the images due to the naive tokenization scheme; (2) the models suffer from information loss because they only consider single-scale feature representations; and (3) the segmentation label maps generated by the models are not accurate enough without considering rich semantic contexts and anatomical textures. In this work, we present CASTformer, a novel type of adversarial transformers, for 2D medical image segmentation. First, we take advantage of the pyramid structure to construct multi-scale representations and handle multi-scale variations. We then design a novel class-aware transformer module to better learn the discriminative regions of objects with semantic structures. Lastly, we utilize an adversarial training strategy that boosts segmentation accuracy and correspondingly allows a transformer-based discriminator to capture high-level semantically correlated contents and low-level anatomical features. Our experiments demonstrate that CASTformer dramatically outperforms previous state-of-the-art transformer-based approaches on three benchmarks, obtaining 2.54%-5.88% absolute improvements in Dice over previous models. Further qualitative experiments provide a more detailed picture of the model’s inner workings, shed light on the challenges in improved transparency, and demonstrate that transfer learning can greatly improve performance and reduce the size of medical image datasets in training, making CASTformer a strong starting point for downstream medical image analysis tasks.",,https://openreview.net/forum?id=aqLugNVQqRw,https://neurips.cc/virtual/2022/poster/54501,https://neurips.cc/virtual/2022/poster/54501,Evasion,Defence,"['Adversarial transformers', 'Medical image segmentation', 'Pyramid structure', 'Multi-scale representations', 'Class-aware transformer module', 'Adversarial training strategy', 'Dice', 'Transformer-based discriminator', 'Semantic structures', 'Transfer learning']",['Medical'],,,,,,
2849,https://neurips.cc/virtual/2022/poster/54777,UQGAN: A Unified Model for Uncertainty Quantification of Deep Classifiers trained via Conditional GANs,Poster,NeurIPS,2022,"We present an approach to quantifying both aleatoric and epistemic uncertainty for deep neural networks in image classification, based on generative adversarial networks (GANs). While most works in the literature that use GANs to generate out-of-distribution (OoD) examples only focus on the evaluation of OoD detection, we present a GAN based approach to learn a classifier that produces proper uncertainties for OoD examples as well as for false positives (FPs). Instead of shielding the entire in-distribution data with GAN generated OoD examples which is state-of-the-art, we shield each class separately with out-of-class examples generated by a conditional GAN and complement this with a one-vs-all image classifier. In our experiments, in particular on CIFAR10, CIFAR100 and Tiny ImageNet, we improve over the OoD detection and FP detection performance of state-of-the-art GAN-training based classifiers. Furthermore, we also find that the generated GAN examples do not significantly affect the calibration error of our classifier and result in a significant gain in model accuracy.",,https://openreview.net/forum?id=djOANbV2zSu,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54777.png?t=1668597886.3022838,https://neurips.cc/media/neurips-2022/Slides/54777.pdf,Robustness,Defence,"['Uncertainty quantification', 'Deep classifiers', 'Conditional GANs', 'Aleatoric uncertainty', 'Epistemic uncertainty', 'Out-of-distribution (OOD)', 'False positives']","['CIFAR10', 'CIFAR100', 'Tiny ImageNet']",,,,,,
2850,https://neurips.cc/virtual/2022/poster/52928,On Feature Learning in the Presence of Spurious Correlations,Poster,NeurIPS,2022,"Deep classifiers are known to rely on spurious features — patterns which are correlated with the target on the training data but not inherently relevant to the learning problem, such as the image backgrounds when classifying the foregrounds. In this paper we evaluate the amount of information about the core (non-spurious) features that can be decoded from the representations learned by standard empirical risk minimization (ERM) and specialized group robustness training. Following recent work on Deep Feature Reweighting (DFR), we evaluate the feature representations by re-training the last layer of the model on a held-out set where the spurious correlation is broken. On multiple vision and NLP problems, we show that the features learned by simple ERM are highly competitive with the features learned by specialized group robustness methods targeted at reducing the effect of spurious correlations. Moreover, we show that the quality of learned feature representations is greatly affected by the design decisions beyond the training method, such as the model architecture and pre-training strategy. On the other hand, we find that strong regularization is not necessary for learning high-quality feature representations.Finally, using insights from our analysis, we significantly improve upon the best results reported in the literature on the popular Waterbirds, CelebA hair color prediction and WILDS-FMOW problems, achieving 97\%, 92\% and 50\% worst-group accuracies, respectively.",,https://openreview.net/forum?id=wKhUPzqVap6,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/52928.png?t=1669272472.1032689,https://neurips.cc/virtual/2022/poster/52928,Robustness,Defence,"['Deep classifiers', 'Spurious features', 'Empirical risk minimization (ERM)', 'Group robustness training', 'Deep Feature Reweighting (DFR)', 'Representations', 'Vision and NLP problems', 'Model architecture', 'Pre-training strategy', 'Regularization']","['Waterbirds', 'CelebA hair color prediction', 'WILDS-FMOW']",,,,,,
2855,https://neurips.cc/virtual/2022/poster/53175,Privacy Induces Robustness: Information-Computation Gaps and Sparse Mean Estimation,Poster,NeurIPS,2022,We establish a simple connection between robust and differentially-private algorithms: private mechanisms ,,https://openreview.net/forum?id=g-OkeNXPy-X,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53175.png?t=1669647384.753252,https://neurips.cc/virtual/2022/poster/53175,Robustness,Defence,"['privacy', 'robustness', 'differentially-private algorithms', 'information-computation gaps', 'sparse mean estimation']",,,,,,,
2863,https://neurips.cc/virtual/2022/poster/54682,Hardness of Noise-Free Learning for Two-Hidden-Layer Neural Networks,Poster,NeurIPS,2022,"We give superpolynomial statistical query (SQ) lower bounds for learning two-hidden-layer ReLU networks with respect to Gaussian inputs in the standard (noise-free) model. No general SQ lower bounds were known for learning ReLU networks of any depth in this setting: previous SQ lower bounds held only for adversarial noise models (agnostic learning) (Kothari and Klivans 2014, Goel et al. 2020a, Diakonikolas et al. 2020a) or restricted models such as correlational SQ (Goel et al. 2020b, Diakonikolas et al. 2020b). Prior work hinted at the impossibility of our result: Vempala and Wilmes (2019) showed that general SQ lower bounds cannot apply to any real-valued family of functions that satisfies a simple non-degeneracy condition. To circumvent their result, we refine a lifting procedure due to Daniely and Vardi (2021) that reduces Boolean PAC learning problems to Gaussian ones. We show how to extend their technique to other learning models and, in many well-studied cases, obtain a more efficient reduction. As such, we also prove new cryptographic hardness results for PAC learning two-hidden-layer ReLU networks, as well as new lower bounds for learning constant-depth ReLU networks from membership queries.",,https://openreview.net/forum?id=GzESlaXaN04,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/28d437661d95291767e7402dfe969962.png?t=1666307639.714286,https://neurips.cc/virtual/2022/poster/54682,Not relevant,Other aspects,"['superpolynomial statistical query (SQ) lower bounds', 'learning', 'two-hidden-layer ReLU networks', 'Gaussian inputs', 'standard (noise-free) model', 'agnostic learning', 'correlational SQ', 'Vempala and Wilmes (2019)', 'Boolean PAC learning', 'cryptographic hardness', 'PAC learning', 'two-hidden-layer ReLU networks', 'membership queries']",[],,,,,,
2867,https://neurips.cc/virtual/2022/poster/54368,Perceptual Attacks of No-Reference Image Quality Models with Human-in-the-Loop,Poster,NeurIPS,2022,"No-reference image quality assessment (NR-IQA) aims to quantify how humans perceive visual distortions of digital images without access to their undistorted references. NR-IQA models are extensively studied in computational vision, and are widely used for performance evaluation and perceptual optimization of man-made vision systems. Here we make one of the first attempts to examine the perceptual robustness of NR-IQA models. Under a Lagrangian formulation, we identify insightful connections of the proposed perceptual attack to previous beautiful ideas in computer vision and machine learning. We test one knowledge-driven and three data-driven NR-IQA methods under four full-reference IQA models (as approximations to human perception of just-noticeable differences). Through carefully designed psychophysical experiments, we find that all four NR-IQA models are vulnerable to the proposed perceptual attack. More interestingly, we observe that the generated counterexamples are not transferable, manifesting themselves as distinct design flows of respective NR-IQA methods.",,https://openreview.net/forum?id=3AV_53iRfTi,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54368.png?t=1668571557.5535562,https://neurips.cc/virtual/2022/poster/54368,Evasion,Defence,"['No-reference image quality assessment', 'Perceptual robustness', 'Lagrangian formulation', 'Human perception', 'Psychophysical experiments', 'Counterexamples']","['Computational vision', 'Man-made vision systems']",,,,,,
2871,https://neurips.cc/virtual/2022/poster/53505,Learning Deep Input-Output Stable Dynamics,Poster,NeurIPS,2022,"Learning stable dynamics from observed time-series data is an essential problem in robotics, physical modeling, and systems biology. Many of these dynamics are represented as an inputs-output system to communicate with the external environment. In this study, we focus on input-output stable systems, exhibiting robustness against unexpected stimuli and noise. We propose a method to learn nonlinear systems guaranteeing the input-output stability. Our proposed method utilizes the differentiable projection onto the space satisfying the Hamilton-Jacobi inequality to realize the input-output stability. The problem of finding this projection can be formulated as a quadratic constraint quadratic programming problem, and we derive the particular solution analytically. Also, we apply our method to a toy bistable model and the task of training a benchmark generated from a glucose-insulin simulator. The results show that the nonlinear system with neural networks by our method achieves the input-output stability, unlike naive neural networks. Our code is available at https://github.com/clinfo/DeepIOStability .",,https://openreview.net/forum?id=BTeJpF_BhQ6,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53505.png?t=1668758800.020514,https://neurips.cc/virtual/2022/poster/53505,Not relevant,Defence,"['stable dynamics', 'input-output stable systems', 'robustness', 'nonlinear systems', 'Hamilton-Jacobi inequality', 'quadtratic constraint quadratic programming', 'neural networks', 'input-output stability']","['Robotics', 'Physical modeling', 'Systems biology']",,,,,,
2897,https://neurips.cc/virtual/2022/poster/53135,Learning Interface Conditions in Domain Decomposition Solvers,Poster,NeurIPS,2022,"Domain decomposition methods are widely used and effective in the approximation of solutions to partial differential equations.  Yet the \textit{optimal} construction of these methods requires tedious analysis and is often available only in simplified, structured-grid settings, limiting their use for more complex problems. In this work, we generalize optimized Schwarz domain decomposition methods to unstructured-grid problems, using Graph Convolutional Neural Networks (GCNNs) and unsupervised learning to learn optimal modifications at subdomain interfaces. A key ingredient in our approach is an improved loss function, enabling effective training on relatively small problems, but robust performance on arbitrarily large problems, with computational cost linear in problem size. The performance of the learned linear solvers is compared with both classical and optimized domain decomposition algorithms, for both structured- and unstructured-grid problems.",,https://openreview.net/forum?id=FvdOlVWL-w,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53135.png?t=1669134978.9784107,https://neurips.cc/virtual/2022/poster/53135,Not relevant,Defence,"['Domain decomposition', 'Graph Convolutional Neural Networks (GCNNs)', 'unsupervised learning', 'optimal modifications', 'subdomain interfaces', 'loss function', 'linear solvers', 'classical', 'optimized domain decomposition algorithms', 'structured-grid', 'unstructured-grid']",['PDE'],,,,,,
2911,https://neurips.cc/virtual/2022/poster/54178,Robust Semi-Supervised Learning when Not All Classes have Labels,Poster,NeurIPS,2022,"Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data. Existing SSL typically requires all classes have labels. However, in many real-world applications, there may exist some classes that are difficult to label or newly occurred classes that cannot be labeled in time, resulting in there are unseen classes in unlabeled data. Unseen classes will be misclassified as seen classes, causing poor classification performance. The performance of seen classes is also harmed by the existence of unseen classes. This limits the practical and wider application of SSL. To address this problem, this paper proposes a new SSL approach that can classify not only seen classes but also unseen classes. Our approach consists of two modules: unseen class classification and learning pace synchronization. Specifically, we first enable the SSL methods to classify unseen classes by exploiting pairwise similarity between examples and then synchronize the learning pace between seen and unseen classes by proposing an adaptive threshold with distribution alignment. Extensive empirical results show our approach achieves significant performance improvement in both seen and unseen classes compared with previous studies.",,https://openreview.net/forum?id=lDohSFOHr0,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/da54dd5a0398011cdfa50d559c2c0ef8.png?t=1667135386.6697762,https://neurips.cc/virtual/2022/poster/54178,Other attack,Defence,"['Semi-supervised learning', 'Unlabeled data', 'Unseen classes', 'Classification', 'Pairwise similarity', 'Learning pace synchronization']",,,,,,,
2912,https://neurips.cc/virtual/2022/poster/54443,LOG: Active Model Adaptation for Label-Efficient OOD Generalization,Poster,NeurIPS,2022,"This work discusses how to achieve worst-case Out-Of-Distribution (OOD) generalization for a variety of distributions based on a relatively small labeling cost. The problem has broad applications, especially in non-i.i.d. open-world scenarios. Previous studies either rely on a large amount of labeling cost or lack of guarantees about the worst-case generalization. In this work, we show for the first time that active model adaptation could achieve both good performance and robustness based on the invariant risk minimization principle. We propose \textsc{Log}, an interactive model adaptation framework, with two sub-modules: active sample selection and causal invariant learning. Specifically, we formulate the active selection as a mixture distribution separation problem and present an unbiased estimator, which could find the samples that violate the current invariant relationship, with a provable guarantee. The theoretical analysis supports that both sub-modules contribute to generalization. A large number of experimental results confirm the promising performance of the new algorithm.",,https://openreview.net/forum?id=VdQWVdT_8v,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54443.png?t=1669358823.0657082,https://neurips.cc/virtual/2022/poster/54443,Robustness,Defence,"['Active Model Adaptation', 'Label-Efficient OOD Generalization', 'Out-Of-Distribution', 'Invariant Risk Minimization', 'Interactive Model Adaptation', 'Active Sample Selection', 'Causal Invariant Learning', 'Mixture Distribution Separation', 'Unbiased Estimator']",,,,,,,
2914,https://neurips.cc/virtual/2022/poster/54719,Inference and Sampling for Archimax Copulas,Poster,NeurIPS,2022,"Understanding multivariate dependencies in both the bulk and the tails of a distribution is an important problem for many applications, such as ensuring algorithms are robust to observations that are infrequent but have devastating effects. Archimax copulas are a family of distributions endowed with a precise representation that allows simultaneous modeling of the bulk and the tails of a distribution. Rather than separating the two as is typically done in practice, incorporating additional information from the bulk may improve inference of the tails, where observations are limited. Building on the stochastic representation of Archimax copulas, we develop a non-parametric inference method and sampling algorithm. Our proposed methods, to the best of our knowledge, are the first that allow for highly flexible and scalable inference and sampling algorithms, enabling the increased use of Archimax copulas in practical settings. We experimentally compare to state-of-the-art density modeling techniques, and the results suggest that the proposed method effectively extrapolates to tails while scaling to higher dimensional data. Our findings suggest that the proposed algorithms can be used in a variety of applications where understanding the interplay between the bulk and the tails of a distribution is necessary, such as health and safety.",,https://openreview.net/forum?id=8cC2JeUyz9,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54719.png?t=1671116078.0273743,https://neurips.cc/virtual/2022/poster/54719,Not relevant,Other aspects,"['Archimax copulas', 'multivariate dependencies', 'bulk and tails of a distribution', 'non-parametric inference', 'sampling algorithm', 'density modeling techniques', 'interplay between the bulk and the tails of a distribution', 'health and safety']",['health and safety'],,,,,,
2923,https://neurips.cc/virtual/2022/poster/54230,MoGDE: Boosting Mobile Monocular 3D Object Detection with Ground Depth Estimation,Poster,NeurIPS,2022,"Monocular 3D object detection (Mono3D) in mobile settings (e.g., on a vehicle, a drone, or a robot) is an important yet challenging task. Due to the near-far disparity phenomenon of monocular vision and the ever-changing camera pose, it is hard to acquire high detection accuracy, especially for far objects. Inspired by the insight that the depth of an object can be well determined according to the depth of the ground where it stands, in this paper, we propose a novel Mono3D framework, called MoGDE, which constantly estimates the corresponding ground depth of an image and then utilizes the estimated ground depth information to guide Mono3D. To this end, we utilize a pose detection network to estimate the pose of the camera and then construct a feature map portraying pixel-level ground depth according to the 3D-to-2D perspective geometry. Moreover, to improve Mono3D with the estimated ground depth, we design an RGB-D feature fusion network based on the transformer structure, where the long-range self-attention mechanism is utilized to effectively identify ground-contacting points and pin the corresponding ground depth to the image feature map. We conduct extensive experiments on the real-world KITTI dataset. The results demonstrate that MoGDE can effectively improve the Mono3D accuracy and robustness for both near and far objects. MoGDE yields the best performance compared with the state-of-the-art methods by a large margin and is ranked number one on the KITTI 3D benchmark.",,https://openreview.net/forum?id=J3s8i8OfZZX,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/a13e00b0854808128933f99f4955f338.png?t=1666167928.3999417,https://neurips.cc/virtual/2022/poster/54230,Not relevant,Defence,"['Mobile Monocular 3D Object Detection', 'Ground Depth Estimation', 'Mono3D', 'camera pose', 'pose detection network', 'RGB-D feature fusion network', 'transformer structure', 'long-range self-attention mechanism', 'KITTI dataset']",['Automotive'],,,,,,
2931,https://neurips.cc/virtual/2022/poster/55741,A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks,Poster,NeurIPS,2022,"Textual backdoor attacks are a kind of practical threat to NLP systems. By injecting a backdoor in the training phase, the adversary could control model predictions via predefined triggers. As various attack and defense models have been proposed, it is of great significance to perform rigorous evaluations. However, we highlight two issues in previous backdoor learning evaluations: (1) The differences between real-world scenarios (e.g. releasing poisoned datasets or models) are neglected, and we argue that each scenario has its own constraints and concerns, thus requires specific evaluation protocols; (2) The evaluation metrics only consider whether the attacks could flip the models' predictions on poisoned samples and retain performances on benign samples, but ignore that poisoned samples should also be stealthy and semantic-preserving. To address these issues, we categorize existing works into three practical scenarios in which attackers release datasets, pre-trained models, and fine-tuned models respectively, then discuss their unique evaluation methodologies. On metrics, to completely evaluate poisoned samples, we use grammar error increase and perplexity difference for stealthiness, along with text similarity for validity. After formalizing the frameworks, we develop an open-source toolkit OpenBackdoor to foster the implementations and evaluations of textual backdoor learning. With this toolkit, we perform extensive experiments to benchmark attack and defense models under the suggested paradigm. To facilitate the underexplored defenses against poisoned datasets, we further propose CUBE, a simple yet strong clustering-based defense baseline. We hope that our frameworks and benchmarks could serve as the cornerstones for future model development and evaluations.",,https://openreview.net/forum?id=k3462dQtQhg,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55741.png?t=1669009628.8162732,https://neurips.cc/media/neurips-2022/Slides/55741.pdf,Poisoning,Defence,"['Textual backdoor attacks', 'NLP systems', 'Attack and defense models', 'Evaluation protocols', 'Stealthiness', 'Semantic-preserving', 'Perplexity difference', 'Text similarity', 'Open-source toolkit', 'Clustering-based defense', 'Poisoned datasets']",['NLP'],,,,,,
2933,https://neurips.cc/virtual/2022/poster/55712,Kantorovich Strikes Back! Wasserstein GANs are not Optimal Transport?,Poster,NeurIPS,2022,"Wasserstein Generative Adversarial Networks (WGANs) are the popular generative models built on the theory of Optimal Transport (OT) and the Kantorovich duality. Despite the success of WGANs, it is still unclear how well the underlying OT dual solvers approximate the OT cost (Wasserstein-1 distance, W1) and the OT gradient needed to update the generator. In this paper, we address these questions. We construct 1-Lipschitz functions and use them to build ray monotone transport plans. This strategy yields pairs of continuous benchmark distributions with the analytically known OT plan, OT cost and OT gradient in high-dimensional spaces such as spaces of images. We thoroughly evaluate popular WGAN dual form solvers (gradient penalty, spectral normalization, entropic regularization, etc.) using these benchmark pairs. Even though these solvers perform well in WGANs, none of them faithfully compute W1 in high dimensions. Nevertheless, many provide a meaningful approximation of the OT gradient. These observations suggest that these solvers should not be treated as good estimators of W1 but to some extent they indeed can be used in variational problems requiring the minimization of W1.",,https://openreview.net/forum?id=VtEEpi-dGlt,https://neurips.cc/virtual/2022/poster/55712,https://neurips.cc/virtual/2022/poster/55712,Not relevant,Defence,"['Wasserstein Generative Adversarial Networks (WGANs)', 'Optimal Transport (OT)', 'Kantorovich duality', 'Wasserstein-1 distance', '1-Lipschitz functions', 'ray monotone transport plans', 'benchmark distributions', 'analytically known OT plan', 'OT cost', 'OT gradient', 'high-dimensional spaces', 'images', 'gradient penalty', 'spectral normalization', 'entropic regularization']",,Not relevant,Not relevant,,,,
2937,https://neurips.cc/virtual/2022/poster/55754,Active-Passive SimStereo - Benchmarking the Cross-Generalization Capabilities of Deep Learning-based Stereo Methods,Poster,NeurIPS,2022,"In stereo vision, self-similar or bland regions can make it difficult to match patches between two images. Active stereo-based methods mitigate this problem by projecting a pseudo-random pattern on the scene so that each patch of an image pair can be identified without ambiguity. However, the projected pattern significantly alters the appearance of the image. If this pattern acts as a form of adversarial noise, it could negatively impact the performance of deep learning-based methods, which are now the de-facto standard for dense stereo vision. In this paper, we propose the Active-Passive SimStereo dataset and a corresponding benchmark to evaluate the performance gap between passive and active stereo images for stereo matching algorithms. Using the proposed benchmark and an additional ablation study, we show that the feature extraction and matching modules of a selection of twenty selected deep learning-based stereo matching methods generalize to active stereo without a problem. However, the disparity refinement modules of three of the twenty architectures (ACVNet, CascadeStereo, and StereoNet) are negatively affected by the active stereo patterns due to their reliance on the appearance of the input images.",,https://openreview.net/forum?id=ml1NjI-ujzf,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/66f041e16a60928b05a7e228a89c3799.png?t=1666433448.1215217,https://neurips.cc/virtual/2022/poster/55754,Robustness,Defence,"['stereo vision', 'active stereo', 'passive stereo', 'deep learning-based stereo', 'performance gap', 'feature extraction', 'matching modules', 'disparity refinement', 'ACVNet', 'CascadeStereo', 'StereoNet', 'Active-Passive SimStereo dataset', 'benchmarking']",['stereo vision'],Other aspects,Robustness,,,,Wrong
2938,https://neurips.cc/virtual/2022/poster/55751,MATE: Benchmarking Multi-Agent Reinforcement Learning in Distributed Target Coverage Control,Poster,NeurIPS,2022,"We introduce the Multi-Agent Tracking Environment (MATE), a novel multi-agent environment simulates the target coverage control problems in the real world. MATE hosts an asymmetric cooperative-competitive game consisting of two groups of learning agents--""cameras"" and ""targets""--with opposing interests. Specifically, ""cameras"", a group of directional sensors, are mandated to actively control the directional perception area to maximize the coverage rate of targets. On the other side, ""targets"" are mobile agents that aim to transport cargo between multiple randomly assigned warehouses while minimizing the exposure to the camera sensor networks. To showcase the practicality of MATE, we benchmark the multi-agent reinforcement learning (MARL) algorithms from different aspects, including cooperation, communication, scalability, robustness, and asymmetric self-play. We start by reporting results for cooperative tasks using MARL algorithms (MAPPO, IPPO, QMIX, MADDPG) and the results after augmenting with multi-agent communication protocols (TarMAC, I2C). We then evaluate the effectiveness of the popular self-play techniques (PSRO, fictitious self-play) in an asymmetric zero-sum competitive game. This process of co-evolution between cameras and targets helps to realize a less exploitable camera network. We also observe the emergence of different roles of the target agents while incorporating I2C into target-target communication. MATE is written purely in Python and integrated with OpenAI Gym API to enhance user-friendliness. Our project is released at https://github.com/UnrealTracking/mate.",,https://openreview.net/forum?id=SyoUVEyzJbE,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55751.png?t=1668113695.592553,https://neurips.cc/virtual/2022/poster/55751,Not relevant,Other aspects,"['Multi-Agent Tracking Environment (MATE)', 'Target coverage control', 'Asymmetric cooperative-competitive game', 'Directional sensors', 'Cargo transportation', 'Multi-agent reinforcement learning (MARL)', 'Cooperation', 'Communication', 'Scalability', 'Robustness', 'Asymmetric self-play', 'MAPPO', 'IPPO', 'QMIX', 'MADDPG', 'TarMAC', 'I2C', 'PSRO', 'Fictitious self-play', 'Zero-sum competitive game', 'Co-evolution', 'I2C', 'Target-target communication', 'Python', 'OpenAI Gym API']",,,,,,,
2943,https://neurips.cc/virtual/2022/poster/55693,pFL-Bench: A Comprehensive Benchmark for Personalized Federated Learning,Poster,NeurIPS,2022,"Personalized Federated Learning (pFL), which utilizes and deploys distinct local models, has gained increasing attention in recent years due to its success in handling the statistical heterogeneity of FL clients. However, standardized evaluation and systematical analysis of diverse pFL methods remain a challenge. Firstly, the highly varied datasets, FL simulation settings and pFL implementations prevent easy and fair comparisons of pFL methods. Secondly, the current pFL literature diverges in the adopted evaluation and ablation protocols. Finally, the effectiveness and robustness of pFL methods are under-explored in various practical scenarios, such as the generalization to new clients and the participation of resource-limited clients. To tackle these challenges, we propose the first comprehensive pFL benchmark, pFL-Bench, for facilitating rapid, reproducible, standardized and thorough pFL evaluation. The proposed benchmark contains more than 10 dataset variants in various application domains with a unified data partition and realistic heterogeneous settings; a modularized and easy-to-extend pFL codebase with more than 20 competitive pFL method implementations; and systematic evaluations under containerized environments in terms of generalization, fairness, system overhead, and convergence. We highlight the benefits and potential of state-of-the-art pFL methods and hope pFL-Bench enables further pFL research and broad applications that would otherwise be difficult owing to the absence of a dedicated benchmark. The code is released at https://github.com/alibaba/FederatedScope/tree/master/benchmark/pFL-Bench.",,https://openreview.net/forum?id=2ptbv_JjYKA,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55693.png?t=1669734032.556508,https://neurips.cc/virtual/2022/poster/55693,Not relevant,Defence,"['Personalized Federated Learning', 'pFL', 'Statistical heterogeneity', 'FL clients', 'Evaluation', 'Systematical analysis', 'Datasets', 'FL simulation settings', 'pFL implementations', 'Generalization', 'Fairness', 'System overhead', 'Convergence', 'State-of-the-art pFL methods', 'Codebase']",['Application domains'],,,,,,
2946,https://neurips.cc/virtual/2022/poster/55621,Evaluating Out-of-Distribution Performance on Document Image Classifiers,Poster,NeurIPS,2022,"The ability of a document classifier to handle inputs that are drawn from a distribution different from the training distribution is crucial for robust deployment and generalizability. The RVL-CDIP corpus is the de facto standard benchmark for document classification, yet to our knowledge all studies that use this corpus do not include evaluation on out-of-distribution documents. In this paper, we curate and release a new out-of-distribution benchmark for evaluating out-of-distribution performance for document classifiers. Our new out-of-distribution benchmark consists of two types of documents: those that are not part of any of the 16 in-domain RVL-CDIP categories (RVL-CDIP-O), and those that are one of the 16 in-domain categories yet are drawn from a distribution different from that of the original RVL-CDIP dataset (RVL-CDIP-N). While prior work on document classification for in-domain RVL-CDIP documents reports high accuracy scores, we find that these models exhibit accuracy drops of between roughly 15-30% on our new out-of-domain RVL-CDIP-N benchmark, and further struggle to distinguish between in-domain RVL-CDIP-N and out-of-domain RVL-CDIP-O inputs. Our new benchmark provides researchers with a valuable new resource for analyzing out-of-distribution performance on document classifiers.",,https://openreview.net/forum?id=uDlkiCI5N7Y,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/75fc093c0ee742f6dddaa13fff98f104.png?t=1666494391.9948943,https://neurips.cc/virtual/2022/poster/55621,Robustness,Defence,"['Document Image Classifiers', 'Out-of-Distribution Performance', 'RVL-CDIP corpus', 'Benchmark', 'Generalizability', 'Robust deployment']","['Document classification', 'RVL-CDIP']",,,,,,
2948,https://neurips.cc/virtual/2022/poster/55715,BackdoorBench: A Comprehensive Benchmark of Backdoor Learning,Poster,NeurIPS,2022,"Backdoor learning is an emerging and vital topic for studying deep neural networks' vulnerability (DNNs). Many pioneering backdoor attack and defense methods are being proposed, successively or concurrently, in the status of a rapid arms race. However, we find that the evaluations of new methods are often unthorough to verify their claims and accurate performance, mainly due to the rapid development, diverse settings, and the difficulties of implementation and reproducibility.  Without thorough evaluations and comparisons, it is not easy to track the current progress and design the future development roadmap of the literature. To alleviate this dilemma, we build a comprehensive benchmark of backdoor learning called BackdoorBench. It consists of an extensible modular-based codebase (currently including implementations of 8 state-of-the-art (SOTA) attacks and 9 SOTA defense algorithms) and a standardized protocol of complete backdoor learning. We also provide comprehensive evaluations of every pair of 8 attacks against 9 defenses, with 5 poisoning ratios, based on 5 models and 4 datasets, thus 8,000 pairs of evaluations in total. We present abundant analysis from different perspectives about these 8,000 evaluations, studying the effects of different factors in backdoor learning.  All codes and evaluations of BackdoorBench are publicly available at https://backdoorbench.github.io.",,https://openreview.net/forum?id=31_U7n18gM7,https://neurips.cc/virtual/2022/poster/55715,https://neurips.cc/virtual/2022/poster/55715,Poisoning,Attack,"['Backdoor learning', ""Deep neural networks' vulnerability"", 'Backdoor attack', 'Defense methods', 'Modular-based codebase', 'Standardized protocol', 'Poisoning ratios', 'Models', 'Datasets']",,Other aspects,Poisoning,Both,,,Wrong
2964,https://neurips.cc/virtual/2022/poster/55636,OpenXAI: Towards a Transparent Evaluation of Model Explanations,Poster,NeurIPS,2022,"While several types of post hoc explanation methods have been proposed in recent literature, there is very little work on systematically benchmarking these methods. Here, we introduce OpenXAI, a comprehensive and extensible open-source framework for evaluating and benchmarking post hoc explanation methods. OpenXAI comprises of the following key components: (i) a flexible synthetic data generator and a collection of diverse real-world datasets, pre-trained models, and state-of-the-art feature attribution methods, (ii) open-source implementations of twenty-two quantitative metrics for evaluating faithfulness, stability (robustness), and fairness of explanation methods, and (iii) the first ever public XAI leaderboards to readily compare several explanation methods across a wide variety of metrics, models, and datasets. OpenXAI is easily extensible, as users can readily evaluate custom explanation methods and incorporate them into our leaderboards. Overall, OpenXAI provides an automated end-to-end pipeline that not only simplifies and standardizes the evaluation of post hoc explanation methods, but also promotes transparency and reproducibility in benchmarking these methods. While the first release of OpenXAI supports only tabular datasets, the explanation methods and metrics that we consider are general enough to be applicable to other data modalities. OpenXAI datasets and data loaders, implementations of state-of-the-art explanation methods and evaluation metrics, as well as leaderboards are publicly available at https://open-xai.github.io/. OpenXAI will be regularly updated to incorporate text and image datasets, other new metrics and explanation methods, and welcomes inputs from the community.",,https://openreview.net/forum?id=MU2495w47rz,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/70c639df5e30bdee440e4cdf599fec2b.png?t=1666277217.4258637,https://neurips.cc/virtual/2022/poster/55636,Not relevant,Other aspects,"['post hoc explanation methods', 'benchmarking', 'synthetic data generator', 'real-world datasets', 'pre-trained models', 'feature attribution methods', 'quantitative metrics', 'faithfulness', 'stability', 'fairness', 'leaderboards', 'transparency', 'reproducibility', 'tabular datasets']",['tabular datasets'],,,,,,
2966,https://neurips.cc/virtual/2022/poster/55720,AirfRANS: High Fidelity Computational Fluid Dynamics Dataset for Approximating Reynolds-Averaged Navier–Stokes Solutions,Poster,NeurIPS,2022,"Surrogate models are necessary to optimize meaningful quantities in physical dynamics as their recursive numerical resolutions are often prohibitively expensive. It is mainly the case for fluid dynamics and the resolution of Navier–Stokes equations. However, despite the fast-growing field of data-driven models for physical systems, reference datasets representing real-world phenomena are lacking. In this work, we develop \textsc{AirfRANS}, a dataset for studying the two-dimensional incompressible steady-state Reynolds-Averaged Navier–Stokes equations over airfoils at a subsonic regime and for different angles of attacks. We also introduce metrics on the stress forces at the surface of geometries and visualization of boundary layers to assess the capabilities of models to accurately predict the meaningful information of the problem. Finally, we propose deep learning baselines on four machine learning tasks to study \textsc{AirfRANS} under different constraints for generalization considerations: big and scarce data regime, Reynolds number, and angle of attack extrapolation.",,https://openreview.net/forum?id=Zp8YmiQ_bDC,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/7f1de29e6da19d22b51c68001e7e0e54.png?t=1667485311.3479705,https://neurips.cc/media/neurips-2022/Slides/55720.pdf,Not relevant,Defence,"['surrogate models', 'computational fluid dynamics', 'Reynolds-Averaged Navier–Stokes', 'dataset', 'airfoils', 'machine learning', 'deep learning', 'generalization']",['Fluid dynamics'],Not relevant,Not relevant,,,,
2972,https://neurips.cc/virtual/2022/poster/55626,The Dollar Street Dataset: Images Representing the Geographic and Socioeconomic Diversity of the World,Poster,NeurIPS,2022,"It is crucial that image datasets for computer vision are representative and contain accurate demographic information to ensure their robustness and fairness, especially for smaller subpopulations. To address this issue, we present Dollar Street - a supervised dataset that contains 38,479 images of everyday household items from homes around the world. This dataset was manually curated and fully labeled, including tags for objects (e.g. “toilet,” “toothbrush,” “stove”) and demographic data such as region, country and home monthly income. This dataset includes images from homes with no internet access and incomes as low as \$26.99 per month, visually capturing valuable socioeconomic diversity of traditionally under-represented populations. All images and data are licensed under CC-BY, permitting their use in academic and commercial work. Moreover, we show that this dataset can improve the performance of classification tasks for images of household items from lower income homes, addressing a critical need for datasets that combat bias.",,https://openreview.net/forum?id=qnfYsave0U4,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55626.png?t=1668988801.4004734,https://neurips.cc/virtual/2022/poster/55626,Not relevant,Other aspects,"['image datasets', 'computer vision', 'representative', 'demographic information', 'robustness', 'fairness', 'subpopulations', 'Dollar Street', 'supervised dataset', 'images', 'everyday household items', 'homes', 'world', 'manually curated', 'fully labeled', 'tags', 'objects', 'region', 'country', 'home monthly income', 'internet access', 'income', 'socioeconomic diversity', 'under-represented populations', 'CC-BY', 'academic', 'commercial work', 'performance', 'classification tasks', 'lower income homes', 'bias']","['image datasets', 'computer vision', 'socioeconomic diversity', 'under-represented populations']",,,,,,
2977,https://neurips.cc/virtual/2022/poster/55659,LAION-5B: An open large-scale dataset for training next generation image-text models,Poster,NeurIPS,2022,"Groundbreaking language-vision architectures like CLIP and DALL-E proved the utility of training on large amounts of noisy image-text data, without relying on expensive accurate labels used in standard vision unimodal supervised learning. The resulting models showed capabilities of strong text-guided image generation and transfer to downstream tasks, while performing remarkably at zero-shot classification with noteworthy out-of-distribution robustness. Since then, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and Imagen made further improvements. Studying the training and capabilities of such models requires datasets containing billions of image-text pairs. Until now, no datasets of this size have been made openly available for the broader research community. To address this problem and democratize research on large-scale multi-modal models, we present LAION-5B - a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English language. We show successful replication and fine-tuning of foundational models like CLIP, GLIDE and Stable Diffusion using the dataset, and discuss further experiments enabled with an openly available dataset of this scale. Additionally we provide several nearest neighbor indices, an improved web-interface for dataset exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection.",,https://openreview.net/forum?id=M3Y74vmsMcY,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/3fe94a002317b5f9259f82690aeea4cd.png?t=1667841493.660399,https://neurips.cc/media/neurips-2022/Slides/55659.pdf,Not relevant,Defence,"['large-scale', 'image-text', 'dataset', 'CLIP-filtered', 'multi-modal', 'models', 'replication', 'fine-tuning', 'subset generation', 'watermark', 'NSFW', 'toxic content detection']",['Language-Vision'],Not relevant,Not relevant,,,,
2984,https://neurips.cc/virtual/2022/poster/55690,Ambiguous Images With Human Judgments for Robust Visual Event Classification,Poster,NeurIPS,2022,"Contemporary vision benchmarks predominantly consider tasks on which humans can achieve near-perfect performance. However, humans are frequently presented with visual data that they cannot classify with 100% certainty, and models trained on standard vision benchmarks achieve low performance when evaluated on this data. To address this issue, we introduce a procedure for creating datasets of ambiguous images and use it to produce SQUID-E (""Squidy""), a collection of noisy images extracted from videos. All images are annotated with ground truth values and a test set is annotated with human uncertainty judgments. We use this dataset to characterize human uncertainty in vision tasks and evaluate existing visual event classification models. Experimental results suggest that existing vision models are not sufficiently equipped to provide meaningful outputs for ambiguous images and that datasets of this nature can be used to assess and improve such models through model training and direct evaluation of model calibration. These findings motivate large-scale ambiguous dataset creation and further research focusing on noisy visual data.",,https://openreview.net/forum?id=6Hl7XoPNAVX,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/9b04d152845ec0a378394003c96da594.png?t=1667516785.3367321,https://neurips.cc/virtual/2022/poster/55690,Robustness,Defence,"['Ambiguous Images', 'Human Judgments', 'Robust Visual Event Classification', 'Noisy images', 'Model training', 'Model calibration']",['Visual event classification'],,,,,,
2990,https://neurips.cc/virtual/2022/tutorial/55807,"Incentive-Aware Machine Learning: A Tale of Robustness, Fairness, Improvement, and Performativity",Tutorial,NeurIPS,2022,"When an algorithm can make consequential decisions for people's lives, people have an incentive to respond to the algorithm strategically in order to obtain a more desirable decision. This means that unless the algorithm adapts to this strategizing, it may end up creating policy decisions that are incompatible with the original policy's goal. This has been the mantra of the rapidly growing research area of incentive-aware Machine Learning (ML). In this tutorial, we introduce this area to the broader ML community. After a primer on the basic background needed, we introduce the audience to the four perspectives that have been studied so far: the robustness perspective (where the decision-maker tries to create algorithms that are robust to strategizing), the fairness perspective (where we study the inequalities that arise or are reinforced as a result of strategizing), the improvement perspective (where the learner tries to incentivize effort exertion towards actually improving their points), and the performativity perspective (where the decision-maker wishes to achieve a notion of stability in these settings).",,,https://neurips.cc/virtual/2022/tutorial/55807,https://neurips.cc/virtual/2022/tutorial/55807,Not relevant,Other aspects,"['Incentive-aware Machine Learning', 'Robustness', 'Fairness', 'Improvement', 'Performativity', 'Strategizing', 'Policy decisions', 'Inequalities', 'Effort exertion']",['None'],,,,,,
2994,https://neurips.cc/virtual/2022/poster/55630,Towards Better Evaluation for Dynamic Link Prediction,Poster,NeurIPS,2022,"Despite the prevalence of recent success in learning from static graphs, learning from time-evolving graphs remains an open challenge. In this work, we design new, more stringent evaluation procedures for link prediction specific to dynamic graphs, which reflect real-world considerations, to better compare the strengths and weaknesses of methods. First, we create two visualization techniques to understand the reoccurring patterns of edges over time and show that many edges reoccur at later time steps. Based on this observation, we propose a pure memorization-based baseline called EdgeBank. EdgeBank achieves surprisingly strong performance across multiple settings which highlights that the negative edges used in the current evaluation are easy. To sample more challenging negative edges, we introducetwo novel negative sampling strategies that improve robustness and better match real-world applications. Lastly, we introduce six new dynamic graph datasets from a diverse set of domains missing from current benchmarks, providing new challenges and opportunities for future research. Our code repository is accessible at https://github.com/fpour/DGB.git.",,https://openreview.net/forum?id=1GVpwr2Tfdg,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55630.png?t=1669588728.2531986,https://neurips.cc/virtual/2022/poster/55630,Not relevant,Other aspects,"['Dynamic Link Prediction', 'Time-evolving graphs', 'Evaluation procedures', 'Visualization techniques', 'Memorization-based baseline', 'Negative sampling strategies', 'Dynamic graph datasets', 'Benchmarks']",['None'],,,,,,
3004,https://neurips.cc/virtual/2022/tutorial/55796,Foundational Robustness of Foundation Models,Tutorial,NeurIPS,2022,"Foundation models adopting the methodology of deep learning with pre-training on large-scale unlabeled data and finetuning with task-specific supervision are becoming a mainstream technique in machine learning. Although foundation models hold many promises in learning general representations and few-shot/zero-shot generalization across domains and data modalities, at the same time they raise unprecedented challenges and considerable risks in robustness and privacy due to the use of the excessive volume of data and complex neural network architectures. This tutorial aims to deliver a Coursera-like online tutorial containing comprehensive lectures, a hands-on and interactive Jupyter/Colab live coding demo, and a panel discussion on different aspects of trustworthiness in foundation models.",,,https://neurips.cc/virtual/2022/tutorial/55796,https://neurips.cc/media/neurips-2022/Slides/55796.pdf,Robustness,Other aspects,"['Foundation models', 'deep learning', 'pre-training', 'large-scale unlabeled data', 'finetuning', 'task-specific supervision', 'general representations', 'few-shot/zero-shot generalization', 'domains', 'data modalities', 'robustness', 'privacy', 'excessive volume of data', 'complex neural network architectures', 'trustworthiness']",None,,,,,,
3005,https://neurips.cc/virtual/2022/poster/55704,How Well Do Unsupervised Learning Algorithms Model Human Real-time and Life-long Learning?,Poster,NeurIPS,2022,"Humans learn from visual inputs at multiple timescales, both rapidly and flexibly acquiring visual knowledge over short periods, and robustly accumulating online learning progress over longer periods. Modeling these powerful learning capabilities is an important problem for computational visual cognitive science, and models that could replicate them would be of substantial utility in real-world computer vision settings. In this work, we establish benchmarks for both real-time and life-long continual visual learning. Our real-time learning benchmark measures a model's ability to match the rapid visual behavior changes of real humans over the course of minutes and hours, given a stream of visual inputs. Our life-long learning benchmark evaluates the performance of models in a purely online learning curriculum obtained directly from child visual experience over the course of years of development. We evaluate a spectrum of recent deep self-supervised visual learning algorithms on both benchmarks, finding that none of them perfectly match human performance, though some algorithms perform substantially better than others. Interestingly, algorithms embodying recent trends in self-supervised learning -- including BYOL, SwAV and MAE -- are substantially worse on our benchmarks than an earlier generation of self-supervised algorithms such as SimCLR and MoCo-v2. We present analysis indicating that the failure of these newer algorithms is primarily due to their inability to handle the kind of sparse low-diversity datastreams that naturally arise in the real world, and that actively leveraging memory through negative sampling -- a mechanism eschewed by these newer algorithms -- appears useful for facilitating learning in such low-diversity environments. We also illustrate a complementarity between the short and long timescales in the two benchmarks, showing how requiring a single learning algorithm to be locally context-sensitive enough to match real-time learning changes while stable enough to avoid catastrophic forgetting over the long term induces a trade-off that human-like algorithms may have to straddle. Taken together, our benchmarks establish a quantitative way to directly compare learning between neural networks models and human learners, show how choices in the mechanism by which such algorithms handle sample comparison and memory strongly impact their ability to match human learning abilities, and expose an open problem space for identifying more flexible and robust visual self-supervision algorithms. ",,https://openreview.net/forum?id=c0l2YolqD2T,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55704.png?t=1669220319.710745,https://neurips.cc/virtual/2022/poster/55704,Not relevant,Other aspects,"['Unsupervised Learning Algorithms', 'Model Human Real-time', 'Life-long Learning', 'visual inputs', 'timescales', 'real-time learning benchmark', 'life-long learning benchmark', 'deep self-supervised visual learning algorithms', 'BYOL', 'SwAV', 'MAE', 'SimCLR', 'MoCo-v2', 'memory', 'negative sampling', 'context-sensitive', 'catastrophic forgetting', 'human-like algorithms']","['computer vision', 'real-world']",,,,,,
3007,https://neurips.cc/virtual/2022/poster/55773,Enabling Detailed Action Recognition Evaluation Through Video Dataset Augmentation,Poster,NeurIPS,2022,"It is well-known in the video understanding community that human action recognition models suffer from background bias, i.e., over-relying on scene cues in making their predictions. However, it is difficult to quantify this effect using existing evaluation frameworks. We introduce the Human-centric Analysis Toolkit (HAT), which enables evaluation of learned background bias without the need for new manual video annotation. It does so by automatically generating synthetically manipulated videos and leveraging the recent advances in image segmentation and video inpainting. Using HAT we perform an extensive analysis of 74 action recognition models trained on the Kinetics dataset. We confirm that all these models focus more on the scene background than on the human motion; further, we demonstrate that certain model design decisions (such as training with fewer frames per video or using dense as opposed to uniform temporal sampling) appear to worsen the background bias. We open-source HAT to enable the community to design more robust and generalizable human action recognition models.",,https://openreview.net/forum?id=eOnQ2etkxto,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55773.png?t=1669068254.2325068,https://neurips.cc/virtual/2022/poster/55773,Robustness,Defence,"['Human action recognition', 'Background bias', 'Video understanding', 'Human-centric Analysis Toolkit (HAT)', 'Image segmentation', 'Video inpainting', 'Kinetics dataset', 'Model design decisions']",['Video understanding'],,,,,,
3009,https://neurips.cc/virtual/2022/poster/55759,Wild-Time: A Benchmark of in-the-Wild Distribution Shift over Time,Poster,NeurIPS,2022,"Distribution shifts occur when the test distribution differs from the training distribution, and can considerably degrade performance of machine learning models deployed in the real world. While recent works have studied robustness to distribution shifts, distribution shifts arising from the passage of time have the additional structure of timestamp metadata. Real-world examples of such shifts are underexplored, and it is unclear whether existing models can leverage trends in past distribution shifts to reliably extrapolate into the future. To address this gap, we curate Wild-Time, a benchmark of 5 datasets that reflect temporal distribution shifts arising in a variety of real-world applications, including drug discovery, patient prognosis, and news classification. On these datasets, we systematically benchmark 13 approaches with various inductive biases. We evaluate methods in domain-generalization, continual learning, self-supervised learning, and ensemble learning, which leverage timestamps to extract the common structure of the distribution shifts. We extend several domain-generalization methods to the temporal distribution shift setting by treating windows of time as different domains. Finally, we propose two evaluation strategies to evaluate model performance under temporal distribution shifts---evaluation with a fixed time split (Eval-Fix) and evaluation with a data stream (Eval-Stream). Eval-Fix, our primary evaluation strategy, aims to provide a simple evaluation protocol for the broader machine learning community, while Eval-Stream serves as a complementary benchmark for continual learning approaches. Our experiments demonstrate that existing methods are limited in tackling temporal distribution shift: across all settings, we observe an average performance drop of 20% from in-distribution to out-of-distribution data.",,https://openreview.net/forum?id=F9ENmZABB0,https://neurips.cc/virtual/2022/poster/55759,https://neurips.cc/virtual/2022/poster/55759,Robustness,Defence,"['Distribution shifts', 'Temporal distribution shift', 'Real-world applications', 'Drug discovery', 'Patient prognosis', 'News classification', 'Benchmark', 'Inductive biases', 'Domain-generalization', 'Continual learning', 'Self-supervised learning', 'Ensemble learning', 'Evaluation with a fixed time split', 'Evaluation with a data stream']","['Drug discovery', 'Patient prognosis', 'News classification']",,,,,,
3014,https://neurips.cc/virtual/2022/poster/55743,Robustness Analysis of Video-Language Models Against Visual and Language Perturbations,Poster,NeurIPS,2022,"Joint visual and language modeling on large-scale datasets has recently shown good progress in multi-modal tasks when compared to single modal learning. However, robustness of these  approaches against real-world perturbations has not been studied. In this work, we perform the first extensive robustness study of video-language models against various real-world perturbations. We focus on text-to-video retrieval and propose two large-scale benchmark datasets, MSRVTT-P and YouCook2-P, which utilize 90 different visual and 35 different text perturbations. The study reveals some interesting initial findings from the studied models: 1) models are more robust when text is perturbed versus when video is perturbed, 2) models that are pre-trained are more robust than those trained from scratch, 3) models attend more to scene and objects rather than motion and action. We hope this study will serve as a benchmark and guide future research in robust video-language learning. The benchmark introduced in this study along with the code and datasets is available at https://bit.ly/3CNOly4.",,https://openreview.net/forum?id=A79jAS4MeW9,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/93db85ed909c13838ff95ccfa94cebd9.png?t=1667003677.1501272,https://neurips.cc/virtual/2022/poster/55743,Robustness,Defence,"['robustness', 'video-language models', 'visual perturbations', 'language perturbations', 'text-to-video retrieval', 'benchmark', 'dataset']","['Video', 'Language']",,,,,,
3017,https://neurips.cc/virtual/2022/poster/55728,GriddlyJS: A Web IDE for Reinforcement Learning,Poster,NeurIPS,2022,"Progress in reinforcement learning (RL) research is often driven by the design of new, challenging environments---a costly undertaking requiring skills orthogonal to that of a typical machine learning researcher. The complexity of environment development has only increased with the rise of procedural-content generation (PCG) as the prevailing paradigm for producing varied environments capable of testing the robustness and generalization of RL agents. Moreover, existing environments often require complex build processes, making reproducing results difficult. To address these issues, we introduce GriddlyJS, a web-based Integrated Development Environment (IDE) based on the Griddly engine. GriddlyJS allows researchers to easily design and debug arbitrary, complex PCG grid-world environments, as well as visualize, evaluate, and record the performance of trained agent models. By connecting the RL workflow to the advanced functionality enabled by modern web standards, GriddlyJS allows publishing interactive agent-environment demos that reproduce experimental results directly to the web. To demonstrate the versatility of GriddlyJS, we use it to quickly develop a complex compositional puzzle-solving environment alongside arbitrary human-designed environment configurations and their solutions for use in a automatic curriculum learning and offline RL context. The GriddlyJS IDE is open source and freely available at https://griddly.ai.",,https://openreview.net/forum?id=YmacJv0i_UR,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/c8ffe9a587b126f152ed3d89a146b445.png?t=1667643718.7739735,https://neurips.cc/virtual/2022/poster/55728,Not relevant,Defence,"['Reinforcement Learning', 'Procedural-content generation', 'GriddlyJS', 'Web-based Integrated Development Environment', 'grid-world environments', 'agent models', 'RL workflow', 'web standards', 'automatic curriculum learning', 'offline RL']",['RL research'],,,,,,
3021,https://neurips.cc/virtual/2022/poster/55745,Finding Naturally Occurring Physical Backdoors in Image Datasets,Poster,NeurIPS,2022,"Extensive literature on backdoor poison attacks has studied attacks and defenses for backdoors using  “digital trigger patterns.” In contrast, “physical backdoors” use physical objects as triggers, have only recently been identified, and are qualitatively different enough to resist most defenses targeting digital trigger backdoors. Research on physical backdoors is limited by access to large datasets containing real images of physical objects co-located with misclassification targets. Building these datasets is time- and labor-intensive.This work seeks to address the challenge of accessibility for research on physical backdoor attacks. We hypothesize that there may be naturally occurring physically co-located objects already present in popular datasets such as ImageNet. Once identified, a careful relabeling of these data can transform them into training samples for physical backdoor attacks. We propose a method to scalably identify these subsets of potential triggers in existing datasets, along with the specific classes they can poison. We call these naturally occurring trigger-class subsets natural backdoor datasets. Our techniques successfully identify natural backdoors in widely-available datasets, and produce models behaviorally equivalent to those trained on manually curated datasets. We release our code to allow the research community to create their own datasets for research on physical backdoor attacks.",,https://openreview.net/forum?id=v3yM5zVzP4C,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/43ec517d68b6edd3015b3edc9a11367b.png?t=1666283284.777532,https://neurips.cc/virtual/2022/poster/55745,Poisoning,Attack,"['backdoor poison attacks', 'physical backdoors', 'digital trigger patterns', 'ImageNet', 'training samples', 'physical backdoor attacks']",['image datasets'],Other aspects,Poisoning,,,,Arguable
3031,https://neurips.cc/virtual/2022/poster/55691,NAS-Bench-360: Benchmarking Neural Architecture Search on Diverse Tasks,Poster,NeurIPS,2022,"Most existing neural architecture search (NAS) benchmarks and algorithms prioritize well-studied tasks, e.g. image classification on CIFAR or ImageNet. This makes the performance of NAS approaches in more diverse areas poorly understood. In this paper, we present NAS-Bench-360, a benchmark suite to evaluate methods on domains beyond those traditionally studied in architecture search, and use it to address the following question: do state-of-the-art NAS methods perform well on diverse tasks? To construct the benchmark, we curate ten tasks spanning a diverse array of application domains, dataset sizes, problem dimensionalities, and learning objectives. Each task is carefully chosen to interoperate with modern CNN-based search methods while possibly being far-afield from its original development domain. To speed up and reduce the cost of NAS research, for two of the tasks we release the precomputed performance of 15,625 architectures comprising a standard CNN search space. Experimentally, we show the need for more robust NAS evaluation of the kind NAS-Bench-360 enables by showing that several modern NAS procedures perform inconsistently across the ten tasks, with many catastrophically poor results. We also demonstrate how NAS-Bench-360 and its associated precomputed results will enable future scientific discoveries by testing whether several recent hypotheses promoted in the NAS literature hold on diverse tasks. NAS-Bench-360 is hosted at https://nb360.ml.cmu.edu.",,https://openreview.net/forum?id=xUXTbq6gWsB,https://neurips.cc/virtual/2022/poster/55691,https://neurips.cc/virtual/2022/poster/55691,Not relevant,Other aspects,"['Neural architecture search', 'benchmarking', 'diverse tasks', 'CNN-based search methods', 'performance', 'evaluation', 'robustness']",['None'],,,,,,
3052,https://neurips.cc/virtual/2022/poster/55622,SafeBench: A Benchmarking Platform for Safety Evaluation of Autonomous Vehicles,Poster,NeurIPS,2022,"As shown by recent studies, machine intelligence-enabled systems are vulnerable to test cases resulting from either adversarial manipulation or natural distribution shifts. This has raised great concerns about deploying machine learning algorithms for real-world applications, especially in safety-critical domains such as autonomous driving (AD). On the other hand, traditional AD testing on naturalistic scenarios requires hundreds of millions of driving miles due to the high dimensionality and rareness of the safety-critical scenarios in the real world. As a result, several approaches for autonomous driving evaluation have been explored, which are usually, however, based on different simulation platforms, types of safety-critical scenarios, scenario generation algorithms, and driving route variations. Thus, despite a large amount of effort in autonomous driving testing, it is still challenging to compare and understand the effectiveness and efficiency of different testing scenario generation algorithms and testing mechanisms under similar conditions. In this paper, we aim to provide the first unified platform SafeBench to integrate different types of safety-critical testing scenarios, scenario generation algorithms, and other variations such as driving routes and environments. In particular, we consider 8 safety-critical testing scenarios following National Highway Traffic Safety Administration (NHTSA) and develop 4 scenario generation algorithms considering 10 variations for each scenario. Meanwhile, we implement 4 deep reinforcement learning-based AD algorithms with 4 types of input (e.g., bird’s-eye view, camera) to perform fair comparisons on SafeBench. We find our generated testing scenarios are indeed more challenging and observe the trade-off between the performance of AD agents under benign and safety-critical testing scenarios. We believe our unified platform SafeBench for large-scale and effective autonomous driving testing will motivate the development of new testing scenario generation and safe AD algorithms. SafeBench is available at https://safebench.github.io.",,https://openreview.net/forum?id=dwi57JI_-K,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55622.png?t=1669430952.996256,https://neurips.cc/virtual/2022/poster/55622,Robustness,Defence,"['autonomous driving', 'machine learning', 'testing', 'scenario generation', 'deep reinforcement learning', 'benchmarking', 'safety evaluation', 'NHTSA']",['autonomous vehicle'],,,,,,
3054,https://neurips.cc/virtual/2022/poster/55618,"This is the way: designing and compiling LEPISZCZE, a comprehensive NLP benchmark for Polish",Poster,NeurIPS,2022,"The availability of compute and data to train larger and larger language models increases the demand for robust methods of benchmarking the true progress of LM training. Recent years witnessed significant progress in standardized benchmarking for English. Benchmarks such as GLUE, SuperGLUE, or KILT have become a de facto standard tools to compare large language models. Following the trend to replicate GLUE for other languages, the KLEJ benchmark\ (klej is the word for glue in Polish) has been released for Polish. In this paper, we evaluate the progress in benchmarking for low-resourced languages. We note that only a handful of languages have such comprehensive benchmarks. We also note the gap in the number of tasks being evaluated by benchmarks for resource-rich English/Chinese and the rest of the world.In this paper, we introduce LEPISZCZE (lepiszcze is the Polish word for glew, the Middle English predecessor of glue), a new, comprehensive benchmark for Polish NLP with a large variety of tasks and high-quality operationalization of the benchmark.We design LEPISZCZE with flexibility in mind. Including new models, datasets, and tasks is as simple as possible while still offering data versioning and model tracking. In the first run of the benchmark, we test 13 experiments (task and dataset pairs) based on the five most recent LMs for Polish. We use five datasets from the Polish benchmark and add eight novel datasets. As the paper's main contribution, apart from LEPISZCZE, we provide insights and experiences learned while creating the benchmark for Polish as the blueprint to design similar benchmarks for other low-resourced languages.",,https://openreview.net/forum?id=CZAd_6uiUx0,https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55618.png?t=1669324707.07604,https://neurips.cc/virtual/2022/poster/55618,Not relevant,Other aspects,"['NLP', 'benchmarking', 'language models', 'Polish', 'GLUE', 'SuperGLUE', 'KILT', 'KLEJ', 'low-resourced languages', 'datasets', 'tasks', 'flexibility', 'data versioning', 'model tracking']",['Polish'],,,,,,
